{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bartosz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('../../../', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "\n",
    "from tools.data_processor import DataProcessor\n",
    "from tools.frame_processors import SupervisionVertexProcessorWithLandmarkFrontalization\n",
    "from tools.frame_preprocessors import TextureFrontalizationPreprocessor, FaceExtractionPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO= \"smile.mp4\"\n",
    "VIDEO_PATH = os.path.abspath(os.path.join('..', 'data', 'expressions', VIDEO))\n",
    "REFERENCE_POINTS = \"key_points_xyz.npy\"\n",
    "REFERENCE_POINTS_PATH = os.path.abspath(os.path.join('..', 'data', 'reference_points', REFERENCE_POINTS))\n",
    "MODELS_PATH = os.path.join(src_path, \"models\", \"frontalization_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVPWLF_dummy(SupervisionVertexProcessorWithLandmarkFrontalization):\n",
    "    KEYPOINTS_HISTORY = []\n",
    "\n",
    "    def process(self, frame: np.ndarray) -> np.ndarray:\n",
    "        processed_frame = self._model.process(frame)\n",
    "        image_to_frontalize_xyz = self._get_xyz_from_processed_frame(processed_frame)\n",
    "        frontalized_keypoints = self._procrustes_analysis(\n",
    "            image_to_frontalize_xyz[0], self._reference_points[0]\n",
    "        )\n",
    "        frontalized_keypoints = self._get_xy_from_xyz(frontalized_keypoints)\n",
    "        self.KEYPOINTS_HISTORY.append(frontalized_keypoints)\n",
    "\n",
    "        return self._make_face_mesh(frontalized_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 124/321 [00:05<00:08, 22.04it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m f_processor \u001b[38;5;241m=\u001b[39m SVPWLF_dummy(\n\u001b[0;32m      2\u001b[0m     reference_points_path\u001b[38;5;241m=\u001b[39mREFERENCE_POINTS_PATH\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m dp \u001b[38;5;241m=\u001b[39m DataProcessor(\n\u001b[0;32m      6\u001b[0m     frame_processor\u001b[38;5;241m=\u001b[39mf_processor,\n\u001b[0;32m      7\u001b[0m     frame_preprocessors\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     ]\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[43mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVIDEO_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bartosz\\Desktop\\automatic-lie-detection\\src\\tools\\data_processor.py:151\u001b[0m, in \u001b[0;36mDataProcessor.process_video\u001b[1;34m(self, source_path, target_path)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_and_save_video(source_path, target_path)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_and_display_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bartosz\\Desktop\\automatic-lie-detection\\src\\tools\\data_processor.py:125\u001b[0m, in \u001b[0;36mDataProcessor._process_and_display_video\u001b[1;34m(self, source_path)\u001b[0m\n\u001b[0;32m    122\u001b[0m frame_generator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mget_video_frames_generator(source_path)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m tqdm(frame_generator, total\u001b[38;5;241m=\u001b[39mvideo_info\u001b[38;5;241m.\u001b[39mtotal_frames):\n\u001b[1;32m--> 125\u001b[0m     annotated_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m annotated_frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bartosz\\Desktop\\automatic-lie-detection\\src\\tools\\data_processor.py:67\u001b[0m, in \u001b[0;36mDataProcessor._handle_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     65\u001b[0m preprocessed_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_frame(frame)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocessed_frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bartosz\\Desktop\\automatic-lie-detection\\src\\tools\\data_processor.py:53\u001b[0m, in \u001b[0;36mDataProcessor._process_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_frame\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    Process a single frame by detecting facial landmarks and annotating\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    edges.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m        (np.ndarray): The processed frame.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_frame_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mSVPWLF_dummy.process\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m      5\u001b[0m processed_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mprocess(frame)\n\u001b[0;32m      6\u001b[0m image_to_frontalize_xyz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_xyz_from_processed_frame(processed_frame)\n\u001b[0;32m      7\u001b[0m frontalized_keypoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_procrustes_analysis(\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mimage_to_frontalize_xyz\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reference_points[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m frontalized_keypoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_xy_from_xyz(frontalized_keypoints)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKEYPOINTS_HISTORY\u001b[38;5;241m.\u001b[39mappend(frontalized_keypoints)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "f_processor = SVPWLF_dummy(\n",
    "    reference_points_path=REFERENCE_POINTS_PATH\n",
    ")\n",
    "\n",
    "dp = DataProcessor(\n",
    "    frame_processor=f_processor,\n",
    "    frame_preprocessors=[\n",
    "        FaceExtractionPreprocessor(\n",
    "            skip_bad_frames=True,\n",
    "            output_size=(200, 200)\n",
    "        ),\n",
    "        TextureFrontalizationPreprocessor(\n",
    "            models_path=MODELS_PATH,\n",
    "            do_calculate_symmetry=True\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "dp.process_video(VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_processor.KEYPOINTS_HISTORY[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
