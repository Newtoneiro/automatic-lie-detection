{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bartosz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import shap\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.abspath(os.path.join('..', '..', '..', 'data', 'processed', 'ravdess'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".npy\"):\n",
    "            data = np.load(os.path.join(data_path, file), allow_pickle=True)\n",
    "            data = np.array(data, dtype=np.float32)\n",
    "\n",
    "            all_data.append(data)\n",
    "\n",
    "            label = int(file.split(\"-\")[2])\n",
    "            all_labels.append(label)\n",
    "\n",
    "    return np.array(all_data, dtype=object), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, all_labels = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, labels):\n",
    "    tensor_data = [torch.tensor(d, dtype=torch.float32) for d in data]\n",
    "    padded_data = pad_sequence(tensor_data, batch_first=True)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    encoded_labels = torch.tensor(encoded_labels, dtype=torch.float32)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        padded_data, encoded_labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(all_data, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PODEJŚCIE 1 - Dodanie warstwy self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n",
    "\n",
    "Próba dodania warstwy atencji, by model automatycznie decydował które z landmarków są ważne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_landmarks=478):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.num_landmarks = num_landmarks\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Attention layer: Assign weights to each landmark\n",
    "        self.attention = nn.Linear(num_landmarks // 2, num_landmarks // 2)  # Reduce dimensionality\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(input_size=(num_landmarks // 2) * 32, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128 * 2, hidden_size=64, batch_first=True)\n",
    "        \n",
    "        # Fully connected classification layer\n",
    "        self.fc = nn.Linear(64, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, frames, landmarks, coordinates = x.shape\n",
    "        x = x.view(-1, landmarks, coordinates).permute(0, 2, 1)  # Shape: (batch_size*frames, 2, 478)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))  # Shape: (batch_size*frames, 32, 478)\n",
    "        x = self.pool1(x)  # Shape: (batch_size*frames, 32, 239) - Because of pooling\n",
    "        \n",
    "        # Compute landmark importance using attention\n",
    "        x_mean = x.mean(dim=1)  # Average over filters -> (batch_size*frames, 239)\n",
    "        attn_weights = torch.sigmoid(self.attention(x_mean))  # Learnable landmark weights\n",
    "        \n",
    "        x = x * attn_weights.unsqueeze(1)  # Apply learned importance to landmarks\n",
    "        \n",
    "        x = x.view(batch_size, frames, -1)  \n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        \n",
    "        x = self.fc(x[:, -1, :])  \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EmotionClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 130.5693, Train Acc: 0.1173, Val Loss: 28.8773, Val Acc: 0.1299\n",
      "Epoch 2/200, Train Loss: 130.1450, Train Acc: 0.1193, Val Loss: 28.8686, Val Acc: 0.1276\n",
      "Epoch 3/200, Train Loss: 130.1265, Train Acc: 0.1233, Val Loss: 28.8807, Val Acc: 0.1276\n",
      "Epoch 4/200, Train Loss: 130.0376, Train Acc: 0.1332, Val Loss: 28.8561, Val Acc: 0.1253\n",
      "Epoch 5/200, Train Loss: 130.0314, Train Acc: 0.1228, Val Loss: 28.8616, Val Acc: 0.1276\n",
      "Epoch 6/200, Train Loss: 130.0584, Train Acc: 0.1248, Val Loss: 28.8544, Val Acc: 0.1462\n",
      "Epoch 7/200, Train Loss: 130.0230, Train Acc: 0.1342, Val Loss: 28.8600, Val Acc: 0.1276\n",
      "Epoch 8/200, Train Loss: 129.9842, Train Acc: 0.1372, Val Loss: 28.8298, Val Acc: 0.1276\n",
      "Epoch 9/200, Train Loss: 129.7192, Train Acc: 0.1690, Val Loss: 28.7719, Val Acc: 0.1717\n",
      "Epoch 10/200, Train Loss: 128.7858, Train Acc: 0.1784, Val Loss: 28.1453, Val Acc: 0.2065\n",
      "Epoch 11/200, Train Loss: 126.4844, Train Acc: 0.1958, Val Loss: 28.0498, Val Acc: 0.1903\n",
      "Epoch 12/200, Train Loss: 125.0604, Train Acc: 0.1983, Val Loss: 27.7559, Val Acc: 0.1995\n",
      "Epoch 13/200, Train Loss: 123.9409, Train Acc: 0.2083, Val Loss: 27.4591, Val Acc: 0.1995\n",
      "Epoch 14/200, Train Loss: 123.7650, Train Acc: 0.2083, Val Loss: 27.4554, Val Acc: 0.1949\n",
      "Epoch 15/200, Train Loss: 123.1063, Train Acc: 0.2063, Val Loss: 27.3754, Val Acc: 0.1740\n",
      "Epoch 16/200, Train Loss: 122.7889, Train Acc: 0.2132, Val Loss: 27.4064, Val Acc: 0.1833\n",
      "Epoch 17/200, Train Loss: 122.8481, Train Acc: 0.2122, Val Loss: 27.2463, Val Acc: 0.1903\n",
      "Epoch 18/200, Train Loss: 122.6242, Train Acc: 0.2157, Val Loss: 27.2316, Val Acc: 0.1949\n",
      "Epoch 19/200, Train Loss: 122.5064, Train Acc: 0.2187, Val Loss: 27.3339, Val Acc: 0.2042\n",
      "Epoch 20/200, Train Loss: 122.6292, Train Acc: 0.2157, Val Loss: 27.2681, Val Acc: 0.1926\n",
      "Epoch 21/200, Train Loss: 122.6659, Train Acc: 0.2167, Val Loss: 27.2605, Val Acc: 0.1856\n",
      "Epoch 22/200, Train Loss: 122.4345, Train Acc: 0.2107, Val Loss: 27.2402, Val Acc: 0.1972\n",
      "Epoch 23/200, Train Loss: 122.3088, Train Acc: 0.2177, Val Loss: 27.2424, Val Acc: 0.1949\n",
      "Epoch 24/200, Train Loss: 122.4166, Train Acc: 0.2207, Val Loss: 27.3165, Val Acc: 0.1810\n",
      "Epoch 25/200, Train Loss: 122.5206, Train Acc: 0.2132, Val Loss: 27.1463, Val Acc: 0.1972\n",
      "Epoch 26/200, Train Loss: 122.5968, Train Acc: 0.2182, Val Loss: 27.2504, Val Acc: 0.2088\n",
      "Epoch 27/200, Train Loss: 122.1890, Train Acc: 0.2172, Val Loss: 27.2198, Val Acc: 0.1903\n",
      "Epoch 28/200, Train Loss: 122.0437, Train Acc: 0.2202, Val Loss: 27.2487, Val Acc: 0.2019\n",
      "Epoch 29/200, Train Loss: 122.0826, Train Acc: 0.2242, Val Loss: 27.2553, Val Acc: 0.1926\n",
      "Epoch 30/200, Train Loss: 122.1840, Train Acc: 0.2212, Val Loss: 27.1917, Val Acc: 0.1879\n",
      "Epoch 31/200, Train Loss: 122.1921, Train Acc: 0.2122, Val Loss: 27.1946, Val Acc: 0.1926\n",
      "Epoch 32/200, Train Loss: 122.3856, Train Acc: 0.2197, Val Loss: 27.3063, Val Acc: 0.1926\n",
      "Epoch 33/200, Train Loss: 122.0059, Train Acc: 0.2187, Val Loss: 27.2697, Val Acc: 0.1903\n",
      "Epoch 34/200, Train Loss: 121.9565, Train Acc: 0.2207, Val Loss: 27.2096, Val Acc: 0.1856\n",
      "Epoch 35/200, Train Loss: 122.0496, Train Acc: 0.2122, Val Loss: 27.2668, Val Acc: 0.1856\n",
      "Epoch 36/200, Train Loss: 121.8993, Train Acc: 0.2187, Val Loss: 27.2025, Val Acc: 0.2088\n",
      "Epoch 37/200, Train Loss: 121.9370, Train Acc: 0.2083, Val Loss: 27.1857, Val Acc: 0.1833\n",
      "Epoch 38/200, Train Loss: 121.9366, Train Acc: 0.2192, Val Loss: 27.2211, Val Acc: 0.1972\n",
      "Epoch 39/200, Train Loss: 121.8883, Train Acc: 0.2182, Val Loss: 27.1665, Val Acc: 0.1903\n",
      "Epoch 40/200, Train Loss: 121.9337, Train Acc: 0.2202, Val Loss: 27.1698, Val Acc: 0.1972\n",
      "Epoch 41/200, Train Loss: 121.8697, Train Acc: 0.2227, Val Loss: 27.2115, Val Acc: 0.1949\n",
      "Epoch 42/200, Train Loss: 121.8071, Train Acc: 0.2271, Val Loss: 27.1821, Val Acc: 0.1949\n",
      "Epoch 43/200, Train Loss: 121.7570, Train Acc: 0.2222, Val Loss: 27.1420, Val Acc: 0.1949\n",
      "Epoch 44/200, Train Loss: 122.0585, Train Acc: 0.2172, Val Loss: 27.1501, Val Acc: 0.1949\n",
      "Epoch 45/200, Train Loss: 121.7602, Train Acc: 0.2266, Val Loss: 27.1020, Val Acc: 0.1949\n",
      "Epoch 46/200, Train Loss: 121.7209, Train Acc: 0.2202, Val Loss: 27.1740, Val Acc: 0.1903\n",
      "Epoch 47/200, Train Loss: 121.6805, Train Acc: 0.2242, Val Loss: 27.1434, Val Acc: 0.1856\n",
      "Epoch 48/200, Train Loss: 121.6713, Train Acc: 0.2242, Val Loss: 27.2293, Val Acc: 0.1833\n",
      "Epoch 49/200, Train Loss: 121.7147, Train Acc: 0.2192, Val Loss: 27.1393, Val Acc: 0.1879\n",
      "Epoch 50/200, Train Loss: 121.4785, Train Acc: 0.2306, Val Loss: 27.1376, Val Acc: 0.1949\n",
      "Epoch 51/200, Train Loss: 121.7685, Train Acc: 0.2172, Val Loss: 27.1319, Val Acc: 0.1949\n",
      "Epoch 52/200, Train Loss: 121.5404, Train Acc: 0.2256, Val Loss: 27.0796, Val Acc: 0.1833\n",
      "Epoch 53/200, Train Loss: 121.2082, Train Acc: 0.2242, Val Loss: 27.1080, Val Acc: 0.1810\n",
      "Epoch 54/200, Train Loss: 121.1886, Train Acc: 0.2232, Val Loss: 27.0182, Val Acc: 0.1949\n",
      "Epoch 55/200, Train Loss: 121.0446, Train Acc: 0.2301, Val Loss: 27.2715, Val Acc: 0.2042\n",
      "Epoch 56/200, Train Loss: 121.9114, Train Acc: 0.2137, Val Loss: 27.0913, Val Acc: 0.1949\n",
      "Epoch 57/200, Train Loss: 121.5278, Train Acc: 0.2212, Val Loss: 26.9533, Val Acc: 0.1926\n",
      "Epoch 58/200, Train Loss: 121.0307, Train Acc: 0.2157, Val Loss: 27.3472, Val Acc: 0.2158\n",
      "Epoch 59/200, Train Loss: 121.5813, Train Acc: 0.2157, Val Loss: 26.8575, Val Acc: 0.1949\n",
      "Epoch 60/200, Train Loss: 120.7915, Train Acc: 0.2301, Val Loss: 26.9603, Val Acc: 0.2019\n",
      "Epoch 61/200, Train Loss: 121.2118, Train Acc: 0.2202, Val Loss: 27.4389, Val Acc: 0.2181\n",
      "Epoch 62/200, Train Loss: 121.6006, Train Acc: 0.2172, Val Loss: 26.9784, Val Acc: 0.2042\n",
      "Epoch 63/200, Train Loss: 120.3496, Train Acc: 0.2227, Val Loss: 26.8188, Val Acc: 0.1949\n",
      "Epoch 64/200, Train Loss: 120.1506, Train Acc: 0.2207, Val Loss: 26.7146, Val Acc: 0.2158\n",
      "Epoch 65/200, Train Loss: 120.1575, Train Acc: 0.2192, Val Loss: 26.6235, Val Acc: 0.2158\n",
      "Epoch 66/200, Train Loss: 119.2988, Train Acc: 0.2306, Val Loss: 26.4978, Val Acc: 0.2297\n",
      "Epoch 67/200, Train Loss: 119.8614, Train Acc: 0.2301, Val Loss: 26.4972, Val Acc: 0.2297\n",
      "Epoch 68/200, Train Loss: 119.3650, Train Acc: 0.2416, Val Loss: 26.5980, Val Acc: 0.2065\n",
      "Epoch 69/200, Train Loss: 119.0708, Train Acc: 0.2455, Val Loss: 26.5318, Val Acc: 0.2459\n",
      "Epoch 70/200, Train Loss: 117.9926, Train Acc: 0.2445, Val Loss: 26.2484, Val Acc: 0.2204\n",
      "Epoch 71/200, Train Loss: 118.4016, Train Acc: 0.2485, Val Loss: 26.5387, Val Acc: 0.2274\n",
      "Epoch 72/200, Train Loss: 118.4032, Train Acc: 0.2584, Val Loss: 26.5761, Val Acc: 0.2111\n",
      "Epoch 73/200, Train Loss: 118.5998, Train Acc: 0.2291, Val Loss: 26.1853, Val Acc: 0.2204\n",
      "Epoch 74/200, Train Loss: 119.4661, Train Acc: 0.2356, Val Loss: 26.4134, Val Acc: 0.2320\n",
      "Epoch 75/200, Train Loss: 118.1407, Train Acc: 0.2575, Val Loss: 26.4337, Val Acc: 0.2227\n",
      "Epoch 76/200, Train Loss: 116.8002, Train Acc: 0.2624, Val Loss: 26.0578, Val Acc: 0.2599\n",
      "Epoch 77/200, Train Loss: 116.5687, Train Acc: 0.2639, Val Loss: 26.2194, Val Acc: 0.2367\n",
      "Epoch 78/200, Train Loss: 117.0379, Train Acc: 0.2609, Val Loss: 26.0410, Val Acc: 0.2367\n",
      "Epoch 79/200, Train Loss: 116.0035, Train Acc: 0.2734, Val Loss: 25.9598, Val Acc: 0.2529\n",
      "Epoch 80/200, Train Loss: 116.3650, Train Acc: 0.2739, Val Loss: 25.9983, Val Acc: 0.2297\n",
      "Epoch 81/200, Train Loss: 116.5799, Train Acc: 0.2699, Val Loss: 26.0951, Val Acc: 0.2529\n",
      "Epoch 82/200, Train Loss: 116.6219, Train Acc: 0.2560, Val Loss: 25.9072, Val Acc: 0.2552\n",
      "Epoch 83/200, Train Loss: 115.7079, Train Acc: 0.2729, Val Loss: 26.0585, Val Acc: 0.2274\n",
      "Epoch 84/200, Train Loss: 116.3054, Train Acc: 0.2659, Val Loss: 25.9564, Val Acc: 0.2436\n",
      "Epoch 85/200, Train Loss: 115.6631, Train Acc: 0.2724, Val Loss: 25.9353, Val Acc: 0.2575\n",
      "Epoch 86/200, Train Loss: 115.9328, Train Acc: 0.2654, Val Loss: 25.9931, Val Acc: 0.2483\n",
      "Epoch 87/200, Train Loss: 116.3895, Train Acc: 0.2699, Val Loss: 25.8140, Val Acc: 0.2575\n",
      "Epoch 88/200, Train Loss: 115.6204, Train Acc: 0.2709, Val Loss: 26.3691, Val Acc: 0.2274\n",
      "Epoch 89/200, Train Loss: 115.4142, Train Acc: 0.2674, Val Loss: 25.7839, Val Acc: 0.2483\n",
      "Epoch 90/200, Train Loss: 114.6201, Train Acc: 0.2868, Val Loss: 25.7665, Val Acc: 0.2622\n",
      "Epoch 91/200, Train Loss: 115.5466, Train Acc: 0.2694, Val Loss: 26.1552, Val Acc: 0.2320\n",
      "Epoch 92/200, Train Loss: 114.9610, Train Acc: 0.2853, Val Loss: 25.9190, Val Acc: 0.2529\n",
      "Epoch 93/200, Train Loss: 114.1319, Train Acc: 0.2838, Val Loss: 26.1049, Val Acc: 0.2367\n",
      "Epoch 94/200, Train Loss: 115.7446, Train Acc: 0.2719, Val Loss: 25.8949, Val Acc: 0.2575\n",
      "Epoch 95/200, Train Loss: 114.3562, Train Acc: 0.2853, Val Loss: 26.9257, Val Acc: 0.2158\n",
      "Epoch 96/200, Train Loss: 115.5318, Train Acc: 0.2729, Val Loss: 26.2094, Val Acc: 0.2436\n",
      "Epoch 97/200, Train Loss: 115.1595, Train Acc: 0.2714, Val Loss: 26.0892, Val Acc: 0.2436\n",
      "Epoch 98/200, Train Loss: 114.3185, Train Acc: 0.2843, Val Loss: 25.6326, Val Acc: 0.2599\n",
      "Epoch 99/200, Train Loss: 113.9454, Train Acc: 0.2962, Val Loss: 25.5157, Val Acc: 0.2854\n",
      "Epoch 100/200, Train Loss: 114.4035, Train Acc: 0.2729, Val Loss: 25.6848, Val Acc: 0.2668\n",
      "Epoch 101/200, Train Loss: 115.0338, Train Acc: 0.2773, Val Loss: 25.4921, Val Acc: 0.2715\n",
      "Epoch 102/200, Train Loss: 113.2370, Train Acc: 0.2982, Val Loss: 25.6345, Val Acc: 0.2668\n",
      "Epoch 103/200, Train Loss: 113.0206, Train Acc: 0.3042, Val Loss: 25.4138, Val Acc: 0.2807\n",
      "Epoch 104/200, Train Loss: 114.1958, Train Acc: 0.2828, Val Loss: 25.9063, Val Acc: 0.2552\n",
      "Epoch 105/200, Train Loss: 113.6521, Train Acc: 0.2922, Val Loss: 25.5225, Val Acc: 0.2738\n",
      "Epoch 106/200, Train Loss: 112.7590, Train Acc: 0.3007, Val Loss: 25.7614, Val Acc: 0.2506\n",
      "Epoch 107/200, Train Loss: 113.1970, Train Acc: 0.2972, Val Loss: 25.4999, Val Acc: 0.2784\n",
      "Epoch 108/200, Train Loss: 111.5029, Train Acc: 0.3116, Val Loss: 25.4589, Val Acc: 0.2645\n",
      "Epoch 109/200, Train Loss: 111.3498, Train Acc: 0.3141, Val Loss: 25.1292, Val Acc: 0.2923\n",
      "Epoch 110/200, Train Loss: 113.3963, Train Acc: 0.2947, Val Loss: 25.1524, Val Acc: 0.3016\n",
      "Epoch 111/200, Train Loss: 112.4226, Train Acc: 0.2962, Val Loss: 25.1457, Val Acc: 0.3016\n",
      "Epoch 112/200, Train Loss: 110.2241, Train Acc: 0.3236, Val Loss: 24.8326, Val Acc: 0.3132\n",
      "Epoch 113/200, Train Loss: 110.2245, Train Acc: 0.3121, Val Loss: 24.9700, Val Acc: 0.2854\n",
      "Epoch 114/200, Train Loss: 110.0435, Train Acc: 0.3151, Val Loss: 26.1206, Val Acc: 0.2483\n",
      "Epoch 115/200, Train Loss: 111.0428, Train Acc: 0.2977, Val Loss: 26.1653, Val Acc: 0.2738\n",
      "Epoch 116/200, Train Loss: 108.2352, Train Acc: 0.3400, Val Loss: 24.6341, Val Acc: 0.3155\n",
      "Epoch 117/200, Train Loss: 107.9334, Train Acc: 0.3439, Val Loss: 25.6547, Val Acc: 0.2575\n",
      "Epoch 118/200, Train Loss: 106.1149, Train Acc: 0.3459, Val Loss: 26.0451, Val Acc: 0.2668\n",
      "Epoch 119/200, Train Loss: 112.3661, Train Acc: 0.3012, Val Loss: 24.1595, Val Acc: 0.3155\n",
      "Epoch 120/200, Train Loss: 105.7495, Train Acc: 0.3559, Val Loss: 27.6205, Val Acc: 0.2390\n",
      "Epoch 121/200, Train Loss: 111.0862, Train Acc: 0.3201, Val Loss: 23.7411, Val Acc: 0.3411\n",
      "Epoch 122/200, Train Loss: 104.4028, Train Acc: 0.3534, Val Loss: 24.7077, Val Acc: 0.3016\n",
      "Epoch 123/200, Train Loss: 106.8925, Train Acc: 0.3355, Val Loss: 24.9066, Val Acc: 0.2993\n",
      "Epoch 124/200, Train Loss: 107.2602, Train Acc: 0.3400, Val Loss: 23.6189, Val Acc: 0.3248\n",
      "Epoch 125/200, Train Loss: 102.5858, Train Acc: 0.3772, Val Loss: 24.0372, Val Acc: 0.3364\n",
      "Epoch 126/200, Train Loss: 102.6198, Train Acc: 0.3668, Val Loss: 24.7316, Val Acc: 0.2923\n",
      "Epoch 127/200, Train Loss: 102.1912, Train Acc: 0.3762, Val Loss: 22.8420, Val Acc: 0.3503\n",
      "Epoch 128/200, Train Loss: 100.7892, Train Acc: 0.3653, Val Loss: 22.5165, Val Acc: 0.3759\n",
      "Epoch 129/200, Train Loss: 103.4778, Train Acc: 0.3623, Val Loss: 23.5106, Val Acc: 0.3341\n",
      "Epoch 130/200, Train Loss: 102.0608, Train Acc: 0.3787, Val Loss: 23.6616, Val Acc: 0.2854\n",
      "Epoch 131/200, Train Loss: 109.1776, Train Acc: 0.3246, Val Loss: 23.4852, Val Acc: 0.3573\n",
      "Epoch 132/200, Train Loss: 102.1469, Train Acc: 0.3668, Val Loss: 22.5706, Val Acc: 0.3689\n",
      "Epoch 133/200, Train Loss: 100.9661, Train Acc: 0.3718, Val Loss: 22.3898, Val Acc: 0.3805\n",
      "Epoch 134/200, Train Loss: 99.5202, Train Acc: 0.4001, Val Loss: 23.1873, Val Acc: 0.3411\n",
      "Epoch 135/200, Train Loss: 99.6267, Train Acc: 0.3857, Val Loss: 22.6162, Val Acc: 0.3480\n",
      "Epoch 136/200, Train Loss: 103.9620, Train Acc: 0.3534, Val Loss: 22.3005, Val Acc: 0.3643\n",
      "Epoch 137/200, Train Loss: 98.4654, Train Acc: 0.3921, Val Loss: 21.7832, Val Acc: 0.3991\n",
      "Epoch 138/200, Train Loss: 98.1455, Train Acc: 0.3867, Val Loss: 21.8044, Val Acc: 0.3898\n",
      "Epoch 139/200, Train Loss: 98.6494, Train Acc: 0.3872, Val Loss: 21.6972, Val Acc: 0.4130\n",
      "Epoch 140/200, Train Loss: 97.6224, Train Acc: 0.4031, Val Loss: 22.4718, Val Acc: 0.3805\n",
      "Epoch 141/200, Train Loss: 97.6780, Train Acc: 0.3877, Val Loss: 21.5304, Val Acc: 0.4107\n",
      "Epoch 142/200, Train Loss: 98.5767, Train Acc: 0.3857, Val Loss: 23.0663, Val Acc: 0.3527\n",
      "Epoch 143/200, Train Loss: 98.3067, Train Acc: 0.3877, Val Loss: 23.0411, Val Acc: 0.3318\n",
      "Epoch 144/200, Train Loss: 101.5411, Train Acc: 0.3648, Val Loss: 22.8409, Val Acc: 0.3387\n",
      "Epoch 145/200, Train Loss: 98.6386, Train Acc: 0.3802, Val Loss: 23.0649, Val Acc: 0.3619\n",
      "Epoch 146/200, Train Loss: 99.1456, Train Acc: 0.3852, Val Loss: 21.7043, Val Acc: 0.4107\n",
      "Epoch 147/200, Train Loss: 95.3643, Train Acc: 0.4105, Val Loss: 22.3043, Val Acc: 0.3527\n",
      "Epoch 148/200, Train Loss: 97.5000, Train Acc: 0.3852, Val Loss: 22.2224, Val Acc: 0.3689\n",
      "Epoch 149/200, Train Loss: 96.1088, Train Acc: 0.4130, Val Loss: 21.2278, Val Acc: 0.4037\n",
      "Epoch 150/200, Train Loss: 96.7264, Train Acc: 0.4021, Val Loss: 22.4503, Val Acc: 0.3550\n",
      "Epoch 151/200, Train Loss: 97.2383, Train Acc: 0.3971, Val Loss: 21.0370, Val Acc: 0.4176\n",
      "Epoch 152/200, Train Loss: 95.9153, Train Acc: 0.3897, Val Loss: 22.0239, Val Acc: 0.3759\n",
      "Epoch 153/200, Train Loss: 95.4532, Train Acc: 0.3981, Val Loss: 22.0684, Val Acc: 0.3852\n",
      "Epoch 154/200, Train Loss: 95.2501, Train Acc: 0.4026, Val Loss: 21.1786, Val Acc: 0.4130\n",
      "Epoch 155/200, Train Loss: 97.3664, Train Acc: 0.4016, Val Loss: 21.2836, Val Acc: 0.3828\n",
      "Epoch 156/200, Train Loss: 95.1580, Train Acc: 0.4026, Val Loss: 21.2339, Val Acc: 0.4037\n",
      "Epoch 157/200, Train Loss: 94.6790, Train Acc: 0.4165, Val Loss: 22.2542, Val Acc: 0.3503\n",
      "Epoch 158/200, Train Loss: 95.1705, Train Acc: 0.4125, Val Loss: 22.4566, Val Acc: 0.3921\n",
      "Epoch 159/200, Train Loss: 96.3991, Train Acc: 0.4021, Val Loss: 24.2261, Val Acc: 0.3364\n",
      "Epoch 160/200, Train Loss: 95.1919, Train Acc: 0.4115, Val Loss: 21.2628, Val Acc: 0.4060\n",
      "Epoch 161/200, Train Loss: 93.9223, Train Acc: 0.4180, Val Loss: 21.0242, Val Acc: 0.4130\n",
      "Epoch 162/200, Train Loss: 94.8413, Train Acc: 0.4125, Val Loss: 21.8020, Val Acc: 0.3968\n",
      "Epoch 163/200, Train Loss: 97.0772, Train Acc: 0.3971, Val Loss: 22.7552, Val Acc: 0.3782\n",
      "Epoch 164/200, Train Loss: 94.8083, Train Acc: 0.4120, Val Loss: 21.1054, Val Acc: 0.3921\n",
      "Epoch 165/200, Train Loss: 94.2246, Train Acc: 0.4125, Val Loss: 20.9620, Val Acc: 0.4084\n",
      "Epoch 166/200, Train Loss: 93.6161, Train Acc: 0.4190, Val Loss: 20.6828, Val Acc: 0.4084\n",
      "Epoch 167/200, Train Loss: 93.2454, Train Acc: 0.4125, Val Loss: 20.9641, Val Acc: 0.4223\n",
      "Epoch 168/200, Train Loss: 96.2771, Train Acc: 0.4011, Val Loss: 21.0606, Val Acc: 0.4060\n",
      "Epoch 169/200, Train Loss: 95.1525, Train Acc: 0.4031, Val Loss: 21.1593, Val Acc: 0.3828\n",
      "Epoch 170/200, Train Loss: 92.9721, Train Acc: 0.4140, Val Loss: 21.4182, Val Acc: 0.4060\n",
      "Epoch 171/200, Train Loss: 95.5463, Train Acc: 0.4051, Val Loss: 21.9042, Val Acc: 0.3968\n",
      "Epoch 172/200, Train Loss: 93.0577, Train Acc: 0.4170, Val Loss: 20.6317, Val Acc: 0.4107\n",
      "Epoch 173/200, Train Loss: 94.8529, Train Acc: 0.4105, Val Loss: 20.8384, Val Acc: 0.4084\n",
      "Epoch 174/200, Train Loss: 91.6499, Train Acc: 0.4240, Val Loss: 20.8977, Val Acc: 0.3944\n",
      "Epoch 175/200, Train Loss: 96.1899, Train Acc: 0.4006, Val Loss: 20.6888, Val Acc: 0.4107\n",
      "Epoch 176/200, Train Loss: 91.4574, Train Acc: 0.4254, Val Loss: 21.3357, Val Acc: 0.3968\n",
      "Epoch 177/200, Train Loss: 92.4556, Train Acc: 0.4190, Val Loss: 20.9286, Val Acc: 0.4153\n",
      "Epoch 178/200, Train Loss: 91.5913, Train Acc: 0.4314, Val Loss: 20.7572, Val Acc: 0.4316\n",
      "Epoch 179/200, Train Loss: 91.4098, Train Acc: 0.4240, Val Loss: 20.6820, Val Acc: 0.4176\n",
      "Epoch 180/200, Train Loss: 91.9905, Train Acc: 0.4175, Val Loss: 20.5050, Val Acc: 0.4176\n",
      "Epoch 181/200, Train Loss: 91.3603, Train Acc: 0.4250, Val Loss: 20.8133, Val Acc: 0.4014\n",
      "Epoch 182/200, Train Loss: 91.9080, Train Acc: 0.4200, Val Loss: 20.3405, Val Acc: 0.4223\n",
      "Epoch 183/200, Train Loss: 92.4479, Train Acc: 0.4314, Val Loss: 20.9941, Val Acc: 0.4130\n",
      "Epoch 184/200, Train Loss: 91.1964, Train Acc: 0.4294, Val Loss: 21.2065, Val Acc: 0.3944\n",
      "Epoch 185/200, Train Loss: 90.8168, Train Acc: 0.4220, Val Loss: 20.1428, Val Acc: 0.4316\n",
      "Epoch 186/200, Train Loss: 92.8565, Train Acc: 0.4180, Val Loss: 20.3427, Val Acc: 0.4130\n",
      "Epoch 187/200, Train Loss: 90.5318, Train Acc: 0.4369, Val Loss: 20.2403, Val Acc: 0.4130\n",
      "Epoch 188/200, Train Loss: 90.8014, Train Acc: 0.4225, Val Loss: 21.9203, Val Acc: 0.3503\n",
      "Epoch 189/200, Train Loss: 91.3573, Train Acc: 0.4294, Val Loss: 20.3495, Val Acc: 0.4200\n",
      "Epoch 190/200, Train Loss: 90.6018, Train Acc: 0.4369, Val Loss: 20.6935, Val Acc: 0.4084\n",
      "Epoch 191/200, Train Loss: 91.0591, Train Acc: 0.4200, Val Loss: 20.5105, Val Acc: 0.4153\n",
      "Epoch 192/200, Train Loss: 91.1873, Train Acc: 0.4299, Val Loss: 20.2747, Val Acc: 0.4292\n",
      "Epoch 193/200, Train Loss: 89.4078, Train Acc: 0.4374, Val Loss: 20.0575, Val Acc: 0.4385\n",
      "Epoch 194/200, Train Loss: 90.5429, Train Acc: 0.4423, Val Loss: 20.1165, Val Acc: 0.4385\n",
      "Epoch 195/200, Train Loss: 90.7797, Train Acc: 0.4394, Val Loss: 20.5576, Val Acc: 0.4292\n",
      "Epoch 196/200, Train Loss: 92.6064, Train Acc: 0.4220, Val Loss: 20.5248, Val Acc: 0.4153\n",
      "Epoch 197/200, Train Loss: 88.5872, Train Acc: 0.4523, Val Loss: 20.7650, Val Acc: 0.4200\n",
      "Epoch 198/200, Train Loss: 87.6509, Train Acc: 0.4558, Val Loss: 20.0907, Val Acc: 0.4223\n",
      "Epoch 199/200, Train Loss: 87.9464, Train Acc: 0.4463, Val Loss: 19.9011, Val Acc: 0.4385\n",
      "Epoch 200/200, Train Loss: 88.4451, Train Acc: 0.4518, Val Loss: 19.8686, Val Acc: 0.4408\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"runs/torch-lstm/feature_selection\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_batch = y_batch.argmax(dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = y_batch.argmax(dim=1)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Train\", train_acc, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Validation\", val_acc, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGradientExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\shap\\explainers\\_gradient.py:85\u001b[0m, in \u001b[0;36mGradientExplainer.__init__\u001b[1;34m(self, model, data, session, batch_size, local_smoothing)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m _TFGradient(model, data, session, batch_size, local_smoothing)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m \u001b[43m_PyTorchGradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\shap\\explainers\\_gradient.py:464\u001b[0m, in \u001b[0;36m_PyTorchGradient.__init__\u001b[1;34m(self, model, data, batch_size, local_smoothing)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    463\u001b[0m multi_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    466\u001b[0m     multi_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m, in \u001b[0;36mEmotionClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m batch_size, frames, landmarks, coordinates \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, landmarks, coordinates)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size*frames, 2, 478)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Shape: (batch_size*frames, 32, 478)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x)  \u001b[38;5;66;03m# Shape: (batch_size*frames, 32, 239) - Because of pooling\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Compute landmark importance using attention\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "explainer = shap.GradientExplainer(model, X_train.to('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PODEJŚCIE 2 - Trening autoencodera.\n",
    "\n",
    "- Train a 1D convolutional autoencoder with your raw time-series data.\n",
    "- Extract the most informative latent features instead of all landmarks.\n",
    "- Use only these features in your final classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
