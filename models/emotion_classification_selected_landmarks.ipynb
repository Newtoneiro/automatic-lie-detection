{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "**********\n",
      "_CUDA version: \n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Oct_30_01:18:48_Pacific_Daylight_Time_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n",
      "**********\n",
      "CUDNN version: 90100\n",
      "Available GPU devices: 1\n",
      "Device Name: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print('*'*10)\n",
    "print(f'_CUDA version: ')\n",
    "!nvcc --version\n",
    "print('*'*10)\n",
    "print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
    "print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
    "print(f'Device Name: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zdobycie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.abspath(os.path.join('..', 'data', 'processed', 'ravdess'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDMARK_INDEXES = [\n",
    "    76, 306,  # mouth corners\n",
    "    74, 73, 72, 11, 302, 303, 304, # upper lip\n",
    "    90, 180, 85, 16, 315, 404, 320, # lower lip\n",
    "    33, 161, 159, 157, 133, 154, 145, 163,  # left eye\n",
    "    70, 63, 105, 66, 107,  # left eyebrow\n",
    "    362, 384, 386, 388, 263, 390, 374, 381,  # right eye\n",
    "    300, 293, 334, 296, 336,  # right eyebrow\n",
    "    1, 5, 197, 168  # nose\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja wybranych punkt√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAIjCAYAAADRKhuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABKKklEQVR4nO3deXhU1eH/8c8kkIQtYQ8BIpFFQUDAgCkoIhiNSxGKKCVWFgWtUhWptVCUiAtxRVChKCr4fJWlQLQWKYgINQgtCgRxYQ+rgOCSsElgcn5/zC8jQyaQSWbmTCbv1/Pkiblz7twzl3Hmc892HcYYIwAAAEsibFcAAABUboQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEcBHQ4YMUVJSku1qlInD4dDjjz9uuxoekpKSNGTIEJ/327lzpxwOh2bOnOn3OgXS448/LofDYbsaxVx99dVq166d7WqgkiKMIOxt3LhR/fv3V7NmzRQTE6MmTZro2muv1SuvvGK7al7NmjVLkyZNsnb8oi95bz+/+c1vrNWrrFasWCGHw6H58+fbrgqAElSxXQEgkFatWqWePXvqggsu0PDhw9WoUSPt2bNH//3vfzV58mTdf//9tqtYzKxZs/TVV19p5MiRVusxcOBA3XjjjR7bGjRo4PfjbN68WRERXBcBlRlhBGHt6aefVlxcnD7//HPVrl3b47Hvv//eTqUqiMsuu0x/+MMfAn6c6OjogB8DJTt27Jhq1Khhuxqo5LgcQVjbvn272rZtWyyISFLDhg2LbXvnnXeUnJysatWqqW7duvr973+vPXv2nPc4hYWFmjRpktq2bauYmBjFx8frnnvu0U8//VSs7L///W/16NFDtWrVUmxsrLp06aJZs2ZJcvXbf/jhh9q1a5e7a+TM8SknT55URkaGWrZsqejoaCUmJuqRRx7RyZMnPY5x8uRJPfTQQ2rQoIFq1aqlm2++WXv37j3v6yiNgoICjRs3TsnJyYqLi1ONGjXUvXt3LV++3Ot5mTx5stq3b6+YmBg1aNBA119/vb744gt3mbPHjPz44496+OGH1b59e9WsWVOxsbG64YYbtGHDBr/UvyQvvPCCunXrpnr16qlatWpKTk722rXjcDj0pz/9Se+//77atWun6OhotW3bVosXLy5WduXKlerSpYtiYmLUokULvfbaa16PXfSc8+bN0yWXXKJq1aqpa9eu2rhxoyTptddeU8uWLRUTE6Orr75aO3fu9Ng/Oztbt956qy644AL3++Khhx7SiRMnPMoNGTJENWvW1Pbt23XjjTeqVq1auv3220s8Jx999JGqV6+ugQMH6vTp05KkpUuX6sorr1Tt2rVVs2ZNXXzxxfrb3/52znMLnA8tIwhrzZo10+rVq/XVV1+dd3De008/rccee0y33Xabhg0bpkOHDumVV17RVVddpfXr13sNNEXuuecezZw5U0OHDtUDDzyg3Nxcvfrqq1q/fr0+++wzVa1aVZI0c+ZM3XnnnWrbtq3GjBmj2rVra/369Vq8eLHS09M1duxY5eXlae/evXrppZckSTVr1pTk+mK/+eabtXLlSt19991q06aNNm7cqJdeeklbtmzR+++/767PsGHD9M477yg9PV3dunXTJ598optuusmnc3f8+HEdPnzYY1tcXJzy8/P1xhtvaODAgRo+fLiOHDmiN998U2lpaVqzZo06duzoLn/XXXdp5syZuuGGGzRs2DCdPn1a2dnZ+u9//6vOnTt7Pe6OHTv0/vvv69Zbb9WFF16ogwcP6rXXXlOPHj30zTffqHHjxj69jtKaPHmybr75Zt1+++0qKCjQnDlzdOutt2rhwoXFzt3KlSuVlZWl++67T7Vq1dLLL7+sW265Rbt371a9evUkucYqXXfddWrQoIEef/xxnT59WhkZGYqPj/d6/OzsbH3wwQcaMWKEJCkzM1O//e1v9cgjj2jq1Km677779NNPP+m5557TnXfeqU8++cS977x583T8+HHde++9qlevntasWaNXXnlFe/fu1bx58zyOc/r0aaWlpenKK6/UCy+8oOrVq3utz8KFC9W/f38NGDBAb731liIjI/X111/rt7/9rS699FI98cQTio6O1rZt2/TZZ5+V+bwDkiQDhLGPPvrIREZGmsjISNO1a1fzyCOPmCVLlpiCggKPcjt37jSRkZHm6aef9ti+ceNGU6VKFY/tgwcPNs2aNXP/nZ2dbSSZd99912PfxYsXe2z/+eefTa1atUxKSoo5ceKER9nCwkL3f990000ez1/k//7v/0xERITJzs722D5t2jQjyXz22WfGGGNycnKMJHPfffd5lEtPTzeSTEZGhpcz9avc3FwjyevP8uXLzenTp83Jkyc99vnpp59MfHy8ufPOO93bPvnkEyPJPPDAA8WOcebrbdasmRk8eLD7719++cU4nc5idYqOjjZPPPFEsXrOmDHjnK9n+fLlRpKZN2/eOcsdP37c4++CggLTrl0706tXL4/tkkxUVJTZtm2be9uGDRuMJPPKK6+4t/Xt29fExMSYXbt2ubd98803JjIy0pz90SvJREdHm9zcXPe21157zUgyjRo1Mvn5+e7tY8aMMZI8yp5dd2OMyczMNA6Hw+P4gwcPNpLM6NGji5Xv0aOHadu2rTHGmAULFpiqVaua4cOHe/xbvPTSS0aSOXToULH9gfKgmwZh7dprr9Xq1at18803a8OGDXruueeUlpamJk2a6IMPPnCXy8rKUmFhoW677TYdPnzY/dOoUSO1atXKaxdEkXnz5ikuLk7XXnutx77JycmqWbOme9+lS5fqyJEjGj16tGJiYjyeozRTPefNm6c2bdqodevWHsfp1auXJLmPs2jRIknSAw884LG/rwNi7777bi1dutTjp0OHDoqMjFRUVJQkV2vNjz/+qNOnT6tz585at26de/8FCxbI4XAoIyOj2HOf6/VGR0e7B7Q6nU798MMP7u6AM5/f36pVq+b+759++kl5eXnq3r2712OmpqaqRYsW7r8vvfRSxcbGaseOHe56L1myRH379tUFF1zgLtemTRulpaV5Pf4111zj0SWXkpIiSbrllltUq1atYtuLjnV23Y8dO6bDhw+rW7duMsZo/fr1xY517733ej8JkmbPnq0BAwbonnvu0WuvveYxuLiodfCf//ynCgsLS3wOwFd00yDsdenSRVlZWSooKNCGDRv03nvv6aWXXlL//v2Vk5OjSy65RFu3bpUxRq1atfL6HEXdLN5s3bpVeXl5XsegSL8OlN2+fbsklXkth61bt+rbb78tcUZL0XF27dqliIgIjy9LSbr44ot9Ol6rVq2Umprq9bG3335bL774ojZt2qRTp065t1944YXu/96+fbsaN26sunXr+nTconEmU6dOVW5urpxOp/uxoi6QQFi4cKGeeuop5eTkeIzB8RaczgwYRerUqeMeI3To0CGdOHHC6/vp4osvdgfGcz1nXFycJCkxMdHr9jPHI+3evVvjxo3TBx98UGycUl5ensffVapUUdOmTYsdX5Jyc3P1hz/8QbfeeqvXqe8DBgzQG2+8oWHDhmn06NG65ppr1K9fP/Xv358ZUSgXwggqjaioKHXp0kVdunTRRRddpKFDh2revHnKyMhQYWGhHA6H/v3vfysyMrLYvkXjNrwpLCxUw4YN9e6773p93F/TYQsLC9W+fXtNnDjR6+Nnf2kFyjvvvKMhQ4aob9+++stf/qKGDRsqMjJSmZmZ7sBVHhMmTNBjjz2mO++8U08++aTq1q2riIgIjRw5MmBX49nZ2br55pt11VVXaerUqUpISFDVqlU1Y8YM9+DiM3l7j0iSMabMdSjpOc93LKfTqWuvvVY//vij/vrXv6p169aqUaOG9u3bpyFDhhQ7Z2e2PJ0tISFBCQkJWrRokb744oti43qqVaumTz/9VMuXL9eHH36oxYsXa+7cuerVq5c++uijEusKnA9hBJVS0Yfs/v37JUktWrSQMUYXXnihLrroIp+eq0WLFvr44491xRVXeDSXeysnSV999ZVatmxZYrmSujBatGihDRs26JprrjlnN0ezZs1UWFio7du3e7SGbN68+XwvpVTmz5+v5s2bKysry6MeZ3fHtGjRQkuWLNGPP/7oU+vI/Pnz1bNnT7355pse23/++WfVr1+/fJUvwYIFCxQTE6MlS5Z4TDWeMWNGmZ6vQYMGqlatmrZu3VrsMX/9OxTZuHGjtmzZorfffluDBg1yb1+6dKnPzxUTE6OFCxeqV69euv766/Wf//xHbdu29SgTERGha665Rtdcc40mTpyoCRMmaOzYsVq+fHmJLWnA+dCuhrC2fPlyr1erRc3kRV/W/fr1U2RkpMaPH1+svDFGP/zwQ4nHuO222+R0OvXkk08We+z06dP6+eefJUnXXXedatWqpczMTP3yyy/FjlGkRo0axZrWi46zb98+TZ8+vdhjJ06c0LFjxyRJN9xwgyTp5Zdf9ijjr1Vdi65+z6zz//73P61evdqj3C233CJjjMaPH1/sOc7VghAZGVns8Xnz5mnfvn3lqfY5RUZGyuFweHQJ7dy502OGkq/Pl5aWpvfff1+7d+92b//222+1ZMmS8la32LEkz3NqjNHkyZPL9HxxcXFasmSJGjZsqGuvvdajtevHH38sVr5o9tTZ08sBX9AygrB2//336/jx4/rd736n1q1bq6CgQKtWrdLcuXOVlJSkoUOHSnJdxT/11FMaM2aMdu7cqb59+6pWrVrKzc3Ve++9p7vvvlsPP/yw12P06NFD99xzjzIzM5WTk6PrrrtOVatW1datWzVv3jxNnjxZ/fv3V2xsrF566SUNGzZMXbp0UXp6uurUqaMNGzbo+PHjevvttyVJycnJmjt3rkaNGqUuXbqoZs2a6t27t+644w794x//0B//+EctX75cV1xxhZxOpzZt2qR//OMfWrJkiTp37qyOHTtq4MCBmjp1qvLy8tStWzctW7ZM27Zt88s5/e1vf6usrCz97ne/00033aTc3FxNmzZNl1xyiY4ePeou17NnT91xxx16+eWXtXXrVl1//fUqLCxUdna2evbsqT/96U8lPv8TTzyhoUOHqlu3btq4caPeffddNW/evFz1XrBggTZt2lRs++DBg3XTTTdp4sSJuv7665Wenq7vv/9eU6ZMUcuWLfXll1+W6Xjjx4/X4sWL1b17d9133306ffq0XnnlFbVt27bMz+lN69at1aJFCz388MPat2+fYmNjtWDBAq9r3JRW/fr13euJpKamauXKlWrSpImeeOIJffrpp7rpppvUrFkzff/995o6daqaNm2qK6+80m+vCZWQhRk8QND8+9//Nnfeeadp3bq1qVmzpomKijItW7Y0999/vzl48GCx8gsWLDBXXnmlqVGjhqlRo4Zp3bq1GTFihNm8ebO7zNlTe4u8/vrrJjk52VSrVs3UqlXLtG/f3jzyyCPmu+++8yj3wQcfmG7duplq1aqZ2NhYc/nll5vZs2e7Hz969KhJT083tWvXNpI8jlVQUGCeffZZ07ZtWxMdHW3q1KljkpOTzfjx401eXp673IkTJ8wDDzxg6tWrZ2rUqGF69+5t9uzZ49PU3ueff97r44WFhWbChAmmWbNmJjo62nTq1MksXLjQ63k5ffq0ef75503r1q1NVFSUadCggbnhhhvM2rVr3WW8Te3985//bBISEky1atXMFVdcYVavXm169OhhevToUayepZ3aW9JP0VTpN99807Rq1cpER0eb1q1bmxkzZpiMjAyv03BHjBhR7Dhnvw5jjPnPf/5jkpOTTVRUlGnevLmZNm1aqZ+zpH8Hb1OVv/nmG5Oammpq1qxp6tevb4YPH+6ebnzm+Rk8eLCpUaOG1/N05tTeItu2bTMJCQmmTZs25tChQ2bZsmWmT58+pnHjxiYqKso0btzYDBw40GzZssXrcwKl5TCmHCOuAAAAyokxIwAAwCrCCAAAsIowAgAArAqJMDJlyhQlJSUpJiZGKSkpWrNmzTnLz5s3T61bt1ZMTIzat2/vdTVDAABQMVgPI0VTGDMyMrRu3Tp16NBBaWlp7qWtz7Zq1SoNHDhQd911l9avX6++ffuqb9+++uqrr4JccwAA4A/WZ9OkpKSoS5cuevXVVyW5lrxOTEzU/fffr9GjRxcrP2DAAB07dkwLFy50b/vNb36jjh07atq0aUGrNwAA8A+ri54VFBRo7dq1GjNmjHtbRESEUlNTi63mWGT16tUaNWqUx7ailQ69OXnypMfKgEV3Ga1Xr16p7pQKAABcjDE6cuSIGjdu7NebI1oNI4cPH5bT6VR8fLzH9vj4eK8rJUrSgQMHvJY/cOCA1/KZmZlel6MGAABls2fPnhLv/lwWYb8c/JgxYzxaUvLy8nTBBRdoz549io2NtVgzAAAqlvz8fCUmJqpWrVp+fV6rYaR+/fqKjIzUwYMHPbYfPHhQjRo18rpPo0aNfCofHR3tcRfOIrGxsYQRAADKwN/DHKzOpomKilJycrKWLVvm3lZYWKhly5apa9euXvfp2rWrR3nJdavsksoDAIDQZr2bZtSoURo8eLA6d+6syy+/XJMmTdKxY8fcd1MdNGiQmjRposzMTEnSgw8+qB49eujFF1/UTTfdpDlz5uiLL77Q66+/bvNlAACAMrIeRgYMGKBDhw5p3LhxOnDggDp27KjFixe7B6nu3r3bY8Rut27dNGvWLD366KP629/+platWun9999Xu3btbL0EAABQDtbXGQm2/Px8xcXFKS8vjzEjAM7L6XTq1KlTtqsBBE3VqlUVGRnp9bFAfYdabxkBgFB19OhR7d27V5Xsmg2VnMPhUNOmTVWzZs2gHZMwAgBeOJ1O7d27V9WrV1eDBg1YJBGVgjFGhw4d0t69e9WqVasSW0j8jTACAF6cOnVKxhg1aNBA1apVs10dIGgaNGignTt36tSpU0ELI9ZvlAcAoYwWEVQ2Nt7zhBEAAGAVYQQAAFhFGAEAeHj88cfVsWNH29XwKlTrtnPnTjkcDuXk5Fg5/syZM1W7dm0rx/YHwggAhJFDhw7p3nvv1QUXXKDo6Gg1atRIaWlp+uyzz6zVKZgBwnYoQNkwmwYAAsnplLKzpf37pYQEqXt3KYAzFG655RYVFBTo7bffVvPmzXXw4EEtW7ZMP/zwQ8COCbvCYVE+WkYAIFCysqSkJKlnTyk93fU7Kcm1PQB+/vlnZWdn69lnn1XPnj3VrFkzXX755RozZoxuvvlmj3LDhg1TgwYNFBsbq169emnDhg3nfO433nhDbdq0UUxMjFq3bq2pU6d6PL53714NHDhQdevWVY0aNdS5c2f973//08yZMzV+/Hht2LBBDodDDodDM2fOLHU9nnnmGcXHx6tWrVq666679Msvv5TrHG3fvl19+vRRfHy8atasqS5duujjjz/2KJOUlKQJEybozjvvVK1atXTBBRcUu//ZmjVr1KlTJ8XExKhz585av369x+MrVqyQw+HQkiVL1KlTJ1WrVk29evXS999/r3//+99q06aNYmNjlZ6eruPHj7v3W7x4sa688krVrl1b9erV029/+1tt377d/XhRy8/cuXPVo0cPxcTE6N133y32Og8dOqTOnTvrd7/7nU6ePKmffvpJt99+u3uqeqtWrTRjxoxynUu/MpVMXl6ekWTy8vJsVwVACDtx4oT55ptvzIkTJ8r2BAsWGONwGCN5/jgcrp8FC/xbYWPMqVOnTM2aNc3IkSPNL7/8UmK51NRU07t3b/P555+bLVu2mD//+c+mXr165ocffjDGGJORkWE6dOjgLv/OO++YhIQEs2DBArNjxw6zYMECU7duXTNz5kxjjDFHjhwxzZs3N927dzfZ2dlm69atZu7cuWbVqlXm+PHj5s9//rNp27at2b9/v9m/f785fvx4qeoxd+5cEx0dbd544w2zadMmM3bsWFOrVi2Pup0tNzfXSDLr16/3+nhOTo6ZNm2a2bhxo9myZYt59NFHTUxMjNm1a5e7TLNmzUzdunXNlClTzNatW01mZqaJiIgwmzZtcr/eBg0amPT0dPPVV1+Zf/3rX6Z58+Yex12+fLmRZH7zm9+YlStXmnXr1pmWLVuaHj16mOuuu86sW7fOfPrpp6ZevXrmmWeecR97/vz5ZsGCBWbr1q1m/fr1pnfv3qZ9+/bG6XR6vL6kpCT3v8d3331nZsyYYeLi4owxxuzevdtcfPHFZvDgweb06dPGGGNGjBhhOnbsaD7//HOTm5trli5daj744AOv5+hc7/1AfYcSRgDAi3KFkdOnjWnatHgQOTOQJCa6yvnZ/PnzTZ06dUxMTIzp1q2bGTNmjNmwYYP78ezsbBMbG1ssrLRo0cK89tprxpjiYaRFixZm1qxZHuWffPJJ07VrV2OMMa+99pqpVauWO0Sc7eznK209unbtau677z6Px1NSUsoVRrxp27ateeWVV9x/N2vWzPzhD39w/11YWGgaNmxo/v73vxtjXK+3Xr16Hu+Nv//9717DyMcff+wuk5mZaSSZ7du3u7fdc889Ji0trcS6HTp0yEgyGzdu9Hh9kyZN8ihXFEY2bdpkEhMTzQMPPGAKCwvdj/fu3dsMHTq0VOfDRhihmwYA/C07W9q7t+THjZH27HGV87NbbrlF3333nT744ANdf/31WrFihS677DJ318iGDRt09OhR1atXTzVr1nT/5ObmenQHFDl27Ji2b9+uu+66y6P8U0895S6fk5OjTp06qW7duqWuZ2nq8e233yolJcVjv65du5bxzLgcPXpUDz/8sNq0aaPatWurZs2a+vbbb7V7926Pcpdeeqn7vx0Ohxo1aqTvv//eXa9LL71UMTEx563Xmc8THx+v6tWrq3nz5h7bip5XkrZu3aqBAweqefPmio2NVVJSkiQVq1/nzp2LHevEiRPq3r27+vXrp8mTJ3ssXnbvvfdqzpw56tixox555BGtWrWqxHNkAwNYAcDf9u/3bzkfxcTE6Nprr9W1116rxx57TMOGDVNGRoaGDBmio0ePKiEhQStWrCi2n7epoUePHpUkTZ8+vVgwKFoqvCzL5ftaD395+OGHtXTpUr3wwgtq2bKlqlWrpv79+6ugoMCjXNWqVT3+djgcKiws9Pl4Zz6Pw+E47/P27t1bzZo10/Tp09W4cWMVFhaqXbt2xepXo0aNYseKjo5WamqqFi5cqL/85S9q0qSJ+7EbbrhBu3bt0qJFi7R06VJdc801GjFihF544QWfX1Mg0DICAP6WkODfcuV0ySWX6NixY5Kkyy67TAcOHFCVKlXUsmVLj5/69esX2zc+Pl6NGzfWjh07ipW/8MILJbmu/nNycvTjjz96PX5UVJScTqfHttLUo02bNvrf//7nsd9///vfcp2Lzz77TEOGDNHvfvc7tW/fXo0aNdLOnTt9eo42bdroyy+/9BhMW956SdIPP/ygzZs369FHH9U111yjNm3a6Keffir1/hEREfq///s/JScnq2fPnvruu+88Hm/QoIEGDx6sd955R5MmTSo2KNcmwggA+Fv37lLTplJJ9/hwOKTERFc5P/rhhx/Uq1cvvfPOO/ryyy+Vm5urefPm6bnnnlOfPn0kSampqeratav69u2rjz76SDt37tSqVas0duxYffHFF16fd/z48crMzNTLL7+sLVu2aOPGjZoxY4YmTpwoSRo4cKAaNWqkvn376rPPPtOOHTu0YMECrV69WpJrdkpubq5ycnJ0+PBhnTx5slT1ePDBB/XWW29pxowZ2rJlizIyMvT111+X6lxs3rxZOTk5Hj+nTp1Sq1atlJWVpZycHG3YsEHp6ek+t3ikp6fL4XBo+PDh+uabb7Ro0SK/tDDUqVNH9erV0+uvv65t27bpk08+0ahRo3x6jsjISL377rvq0KGDevXqpQMHDkiSxo0bp3/+85/atm2bvv76ay1cuFBt2rQpd539hTACAP4WGSlNnuz677MDSdHfkyb5fb2RmjVrKiUlRS+99JKuuuoqtWvXTo899piGDx+uV1999f8f3qFFixbpqquu0tChQ3XRRRfp97//vXbt2qX4+Hivzzts2DC98cYbmjFjhtq3b68ePXpo5syZ7paRqKgoffTRR2rYsKFuvPFGtW/fXs8884y7G+eWW27R9ddfr549e6pBgwaaPXt2qeoxYMAAPfbYY3rkkUeUnJysXbt26d577y3Vufj973+vTp06efwcPHhQEydOVJ06ddStWzf17t1baWlpuuyyy3w+z//617+0ceNGderUSWPHjtWzzz7r03N4ExERoTlz5mjt2rVq166dHnroIT3//PM+P0+VKlU0e/ZstW3b1j2dOCoqSmPGjNGll16qq666SpGRkZozZ0656+wvDmOMsV2JYMrPz1dcXJzy8vIUGxtruzoAQtQvv/yi3NxcXXjhhR4DFX2SlSU9+KDnYNbERFcQ6dfPL/UE/O1c7/1AfYcygBUAAqVfP6lPn6CuwApURIQRAAikyEjp6qtt1wIIaYwZAQAAVhFGAACAVYQRADiHSjbGH7DynieMAIAXRdNSz175Egh3Re/5yCAOtGYAKwB4UaVKFVWvXl2HDh1S1apVFRHBtRvCX2FhoQ4dOqTq1aurSpXgRQTCCAB44XA4lJCQoNzcXO3atct2dYCgiYiI0AUXXOBxo71AI4wAQAmioqLUqlUrumpQqURFRQW9JZAwAgDnEBERUfYVWAGUCp2gAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMCqkAgjU6ZMUVJSkmJiYpSSkqI1a9aUWHb69Onq3r276tSpozp16ig1NfWc5QEAQGizHkbmzp2rUaNGKSMjQ+vWrVOHDh2Ulpam77//3mv5FStWaODAgVq+fLlWr16txMREXXfdddq3b1+Qaw4AAPzBYYwxNiuQkpKiLl266NVXX5UkFRYWKjExUffff79Gjx593v2dTqfq1KmjV199VYMGDTpv+fz8fMXFxSkvL0+xsbHlrj8AAJVFoL5DrbaMFBQUaO3atUpNTXVvi4iIUGpqqlavXl2q5zh+/LhOnTqlunXren385MmTys/P9/gBAAChw2oYOXz4sJxOp+Lj4z22x8fH68CBA6V6jr/+9a9q3LixR6A5U2ZmpuLi4tw/iYmJ5a43AADwH+tjRsrjmWee0Zw5c/Tee+8pJibGa5kxY8YoLy/P/bNnz54g1xIAAJxLFZsHr1+/viIjI3Xw4EGP7QcPHlSjRo3Oue8LL7ygZ555Rh9//LEuvfTSEstFR0crOjraL/UFAAD+Z7VlJCoqSsnJyVq2bJl7W2FhoZYtW6auXbuWuN9zzz2nJ598UosXL1bnzp2DUVUAABAgVltGJGnUqFEaPHiwOnfurMsvv1yTJk3SsWPHNHToUEnSoEGD1KRJE2VmZkqSnn32WY0bN06zZs1SUlKSe2xJzZo1VbNmTWuvAwAAlI31MDJgwAAdOnRI48aN04EDB9SxY0ctXrzYPah19+7dioj4tQHn73//uwoKCtS/f3+P58nIyNDjjz8ezKoDAAA/sL7OSLCxzggAAGUTluuMAAAAEEYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWFXFdgUAAJWI0yllZ0v790sJCVL37lJkpO1awTLCCABUJjbDQFaW9OCD0t69v25r2lSaPFnq1y84dUBIIowgPIXS1Vco1QWVm80wkJUl9e8vGeO5fd8+1/b58wkklRhjRhB+srKkpCSpZ08pPd31OynJtb0y1wWVW1EYODOISL+GgUC+J51OVwg6O4hIv24bOdJVDpUSYQThxeYHbijXBZWb7TCQnV38/4Oz67Bnj6scKiXCCMKH7Q/cUK3LuTid0ooV0uzZrt+26xNOQunc2g4D+/f7txzCDmEE4cP2B26o1qUkdCEFTqidW9thICHBv+UQdggjCB+2P3DLcgxbV4J0IQVOKJ5b22Gge3fXQFmHw/vjDoeUmOgqVx6h1BoFnxBGED5sf+CW5Rg2rgQrShdSRRSq5zZYYaAkkZGuGTtFxzr72JI0aVL5ZpmFWmsUfEIYQXAF8srF9gduqNblbBWhC6miCtVzG4wwcD79+rmm7zZp4rm9adPyT+sNxdYo+IQwguAJ9JVLKHzghmJdzhbqXUgVWSif20CGAV/qsHOntHy5NGuW63dubvmOHaqtUfAJYQTBEawrl1D4wA3FupwplLuQKrpQP7eBCAO+ioyUrr5aGjjQ9bu8gTxUW6PgE4cx3uJk+MrPz1dcXJzy8vIUGxtruzqVg9PpagEp6QPD4XB9Qefm+q+lIJRWPQ2luhTVJynJFQS9/e8fiH+PyoJzG3yzZ7taWs9n1ixXAEK5BOo7lOXgEXi+XLlcfbV/jll09RUKQqku0q9dSP37u74cz/zStN2FVNFxboMv1FujUCp00yDwQrkfvbIK1S6kcMC5Da5QHiyOUqNlBIHHlUto6tdP6tMntLqQwgXnNnhojQoLjBlB4NGPDiDQvN2RODHRFURojfIbxoyg4uLKBUCg0RpVoRFGEBxF/ehnX7k0bcqVCwD/CLXB4ig1wgiChysXAKEm1KbeV1KEEQQXVy4AQoW3cSZNm7q6lWmtDSqm9sI33BUTQDjgfjYhhTCC0uOumADCAfezCTmEEZQOVxEAwgX3swk5hBGcH1cRAMIJq0KHHMIIzo+rCADhhFWhQw5hBOfHVQSAcML9bEIOYQTnx1UEgHBStCq0VDyQsCq0FYQRnB9XEQDCDXdXDikseobz494yAMIRq0KHDMIISod7ywAIR6wKHRIIIyg9riIAAAFAGIFvuIoAAPgZA1gBAIBVhBEAAGAVYQQAAFhFGAEAAFYxgBUA4LrRJTPlYAlhBCgLPrgRTrKyvK8hNHkyawghKOimAXyVlSUlJUk9e0rp6a7fSUmu7UBFk5XlWl357Dtz79vn2s77GkFAGAF8wQc3wonT6WoROfMWD0WKto0c6SoHBBBhBCgtPrgRbrKziwfrMxkj7dnjKgcEEGEEKC0+uBFu9u/3bzmgjBjACpRWKHxwM3AW/pSQ4N9y3vCeRSkQRoDSCsYH97kw44EvNn/r3t31Htq3z3v3o8Pherx797I9P+9ZlBLdNEBpFX1wOxzeH3c4pMTEsn9wnwsDZyvHLCanU1qxQpo92/U70OOPIiNdwUAq/r4u+nvSpLIFPt6z8AFhBCitQH5wnwsDZyvHF5utsNWvnzR/vtSkief2pk1d28vSgsF7Fj5yGOPt3RK+8vPzFRcXp7y8PMXGxtquDgIpUE363pqeExNdQSQQTc8rVri+mM5n+XLp6qv9f3zbnE7Xl3JJg4eLuhJycytul01R2Dr747go5JY1FPjCn/+/VPb3bBgL1HcoY0YQngLZV92vn9SnT/DGLoTCwFmbfJnFVBG/2M7XiuBwuFoR+vQJbNiKjPTf+fPlPcs4IIgwgnBU0lVmUZO+P64y/fnBfT62B87aFu5hLBzDVmnfi1u3Fm/1YoBrpRQSY0amTJmipKQkxcTEKCUlRWvWrCnVfnPmzJHD4VDfvn0DW0FUHOHYV21z4GwoCPcwFo5hqzTv2Xr1pMcfD+9xQCg162Fk7ty5GjVqlDIyMrRu3Tp16NBBaWlp+v7778+5386dO/Xwww+re7h+AKNswnFhMlsDZ0NFuIexcAxb53vPFl0YhNNFA8rFehiZOHGihg8frqFDh+qSSy7RtGnTVL16db311lsl7uN0OnX77bdr/Pjxat68eRBri5AXjleZUmBmPFQU4R7GwjVsnes9O3689MMPJe9bES8aUC5Ww0hBQYHWrl2r1NRU97aIiAilpqZq9erVJe73xBNPqGHDhrrrrrvOe4yTJ08qPz/f4wdhLByvMov06yft3OmagTBrlut3bm54B5Ei4RzGwjlslfSebdWqdPtXtIsGlJnVAayHDx+W0+lUfHy8x/b4+Hht2rTJ6z4rV67Um2++qZycnFIdIzMzU+PHjy9vVRGqzh6J361bYFeUtC2YA2dDTbBnMQVTUdjyNgMsUFPGg8XbezacLxpQJhVqNs2RI0d0xx13aPr06apfv36p9hkzZoxGjRrl/js/P1+JiYmBqiKCqaTpuwMHSi+84Nk3LVX8q0yEdxgL57B1tkAvQ48Kx2oYqV+/viIjI3Xw4EGP7QcPHlSjRo2Kld++fbt27typ3r17u7cVFhZKkqpUqaLNmzerRYsWHvtER0crOjo6ALWHVeeavvvCC9LDD7uW1A63q8zyYk2H0BbOYetMRV1T/ftz0QBJlsNIVFSUkpOTtWzZMvf03MLCQi1btkx/+tOfipVv3bq1Nm7c6LHt0Ucf1ZEjRzR58mRaPCqL0iwSNWeOtH27tGoVX7xFuGlZcFT2wFfa1x/OXVPwmfVumlGjRmnw4MHq3LmzLr/8ck2aNEnHjh3T0KFDJUmDBg1SkyZNlJmZqZiYGLVr185j/9q1a0tSse0IY6WdvrtqVfhfZZb2gz8YC8GBwOfr669MXVM4J+thZMCAATp06JDGjRunAwcOqGPHjlq8eLF7UOvu3bsVEWF9BjJCSbhO3/VVaT/4Q2W58XBXnsAXDq0pZX39laVrCufEjfJQ8XATLt9urMb5Crzy3MwvHFpTKsPNDCEpcN+hNDmg4gnXRaJKy9cl72lJCryyrvxbFCor+pLo4bjyMYKKMIKKp7yLRDmdrtaC2bNdvyvaktO+fvCzpkPglSXw2bqPUiDe/wRelBNhBBVTWVfkzMpyNSf37Cmlp7t+JyVVnCtQyfcP/srekhQMZQl8NloTAvX+J/CinAgjqLh8XR49WE3igW558fWDP5yXGw8VZQl8wW5NCOT7n8CL8jKVTF5enpFk8vLybFcFwXT6tDFNmxrjut4s/uNwGJOY6CpXHgsWFD9O06au7f5S9FocDt9ei7e6JSb6t26V2YIFrnN/9r9L0bazz/Py5SW/H8/8Wb68/HULxvvf19ePCilQ36G0jKByCEaTeLBaXsra0lGZb7QXDL52HQazNSEY7/9wvpkhAs76OiNAUAS6STzYa3mUdfVK1nQILF8W8QrmkujB6hJiETOUEWEElUOgB9j5cuXprzDAB39o8iXwBWtJ9GAOMCXwogwII6gcAn2XUFtTG/ngr/iCESq5Sy5CHGNGUDkEekYJUxtRHkWhcuBA129/t24xowohjjCCyiOQA+yY2ohQxwBThDDuTYPKJ1A3JSuaTSN5H4zIBz5CQTjclA/WBOo7lDAC+JO3m54lJvp3MCIAWBKo71AGsAL+xAwXAPAZYQTwN2a4AIBPGMAKAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKuY2gugYmEFUSDsEEYAVBzeVrht2tR1EzhWuAUqLLppAFQMRff+OTOISNK+fa7tWVl26gWg3AgjAEKf0+lqEfF2K62ibSNHusoBqHAIIwBCX3Z28RaRMxkj7dnjKgegwiGMAAh9+/f7txyAkEIYARD6EhL8Ww5ASCGMAAh93bu7Zs04HN4fdzikxERXOQAVDmEEQOiLjHRN35WKB5KivydNYr0RoIIijACoGPr1k+bPl5o08dzetKlrO+uMABUWi54BqDj69ZP69GEFViDMEEYAVCyRkdLVV9uuBQA/opsGAABYRRgBAABWlTqMfPfdd4GsBwAAqKRKHUbatm2rWbNmBbIuAACgEip1GHn66ad1zz336NZbb9WPP/4YyDoBAIBKpNRh5L777tOXX36pH374QZdccon+9a9/BbJeAACgkvBpau+FF16oTz75RK+++qr69eunNm3aqEoVz6dYt26dXysIAADCm8/rjOzatUtZWVmqU6eO+vTpUyyMAAAA+MKnJDF9+nT9+c9/Vmpqqr7++ms1aNAgUPUCAACVRKnDyPXXX681a9bo1Vdf1aBBgwJZJwAAUImUOow4nU59+eWXatq0aSDrAwAAKplSh5GlS5cGsh4AAKCSYjl4AABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgVanv2gsAIcPplLKzpf37pYQEqXt3KTLSdq0AlBFhBEDFkpUlPfigtHfvr9uaNpUmT5b69bNXLwBlRjcNgIojK0vq398ziEjSvn2u7VlZduoFoFwIIwAqBqfT1SJiTPHHiraNHOkqB6BCIYwAqBiys4u3iJzJGGnPHlc5ABUKYQRAxbB/v3/LAQgZhBEAFUNCgn/LAQgZhBEAFUP37q5ZMw6H98cdDikx0VUOQIVCGAFQMURGuqbvSsUDSdHfkyax3ghQARFGAFQc/fpJ8+dLTZp4bm/a1LWddUaAColFzwBULP36SX36sAIrEEYIIwAqnshI6eqrbdcCgJ/QTQMAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAq62FkypQpSkpKUkxMjFJSUrRmzZpzlv/55581YsQIJSQkKDo6WhdddJEWLVoUpNoCAAB/s3qjvLlz52rUqFGaNm2aUlJSNGnSJKWlpWnz5s1q2LBhsfIFBQW69tpr1bBhQ82fP19NmjTRrl27VLt27eBXHgAA+IXDGGNsHTwlJUVdunTRq6++KkkqLCxUYmKi7r//fo0ePbpY+WnTpun555/Xpk2bVLVq1TIdMz8/X3FxccrLy1NsbGy56g8AQGUSqO9Qa900BQUFWrt2rVJTU3+tTESEUlNTtXr1aq/7fPDBB+ratatGjBih+Ph4tWvXThMmTJDT6SzxOCdPnlR+fr7HDwAACB3Wwsjhw4fldDoVHx/vsT0+Pl4HDhzwus+OHTs0f/58OZ1OLVq0SI899phefPFFPfXUUyUeJzMzU3Fxce6fxMREv74OAABQPtYHsPqisLBQDRs21Ouvv67k5GQNGDBAY8eO1bRp00rcZ8yYMcrLy3P/7NmzJ4g1BgAA52NtAGv9+vUVGRmpgwcPemw/ePCgGjVq5HWfhIQEVa1aVZGRke5tbdq00YEDB1RQUKCoqKhi+0RHRys6Otq/lQcAAH5jrWUkKipKycnJWrZsmXtbYWGhli1bpq5du3rd54orrtC2bdtUWFjo3rZlyxYlJCR4DSIAACD0We2mGTVqlKZPn663335b3377re69914dO3ZMQ4cOlSQNGjRIY8aMcZe/99579eOPP+rBBx/Uli1b9OGHH2rChAkaMWKErZcAAADKyeo6IwMGDNChQ4c0btw4HThwQB07dtTixYvdg1p3796tiIhf81JiYqKWLFmihx56SJdeeqmaNGmiBx98UH/9619tvQQAAFBOVtcZsYF1RgAAKJuwW2cEAABAIowAAADLCCMAAMAqwggAALCKMAIAAKyyOrUXAMrM6ZSys6X9+6WEBKl7d+mM1ZkBVByEEQAVT1aW9OCD0t69v25r2lSaPFnq189evQCUCd00ACqWrCypf3/PICJJ+/a5tmdl2akXgDIjjACoOJxOV4uIt7Uai7aNHOkqB6DCIIwAqDiys4u3iJzJGGnPHlc5ABUGYQRAxbF/v3/LAQgJhBEAFUdCgn/LAQgJhBEAFUf37q5ZMw6H98cdDikx0VUOQIVBGAFQcURGuqbvSsUDSdHfkyax3ghQwRBGAFQs/fpJ8+dLTZp4bm/a1LWddUaACodFzwBUPP36SX36sAIrECYIIwAqpshI6eqrbdcCgB/QTQMAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKyqYrsCABBwTqeUnS3t3y8lJEjdu0uRkbZrBeD/I4wA8J9Q/NLPypIefFDau/fXbU2bSpMnS/362asXADe6aQD4R1aWlJQk9ewppae7ficlubaXhdMprVghzZ7t+u10lq1O/ft7BhFJ2rfPtb2sdQPgVw5jjLFdiWDKz89XXFyc8vLyFBsba7s6QHgo+tI/++PE4XD9nj/ft1YIf7RmOJ2uMHR2EDmzbk2bSrm5pWu9CcVWHyDIAvUdSssIgPJxOl3Bwdt1TdG2kSNL37Lhr9aM7OySg0hR3fbscZUrTZ382eoDwANhBIBLWbtF/Pml789gs3//+cuUphxdPUDAEUYAlO/K319f+pJ/g01CQunqda5y/m71AeAVYQSo7Mp75e+PL/0i/gw23bu7xoQUjVs5m8MhJSa6ypXEn+EIQIkII0Bl5o8rf3986RfxZ7CJjHQNeC2qw9l1kqRJk849CLU84cgfs4GASoIwAlRm/rjy98eXfhF/BhvJNfNm/nypSRPP7U2blm6GT1nDEQNeAZ8QRoDKzF/dIuX90i/iz2BzZt127pSWL5dmzXL9zs0tXZ3KEo4Y8Ar4jHVGgHBVmnUxVqxwXbWfz/Ll0tVX++eYpeFtnZHERFcQCfaqqUXhQvLszvK2hoq/1zYBQkygvkMJI0A4Ku2iYUVfnvv2eR83YvPLM5QWGSttOPJ3uANCTKC+Q7k3DRBuSloNtaib4Mwr+aJukf79XcHD25W/r90i/hIZGTpf2P36SX36nD8c+XM2EFCJMGYECCdlmR3jr/Ee4a4oHA0c6PrtLaD5czYQUIlYDyNTpkxRUlKSYmJilJKSojVr1pyz/KRJk3TxxRerWrVqSkxM1EMPPaRffvklSLUFQlxZZ8eUZ5AnfuXv2UBAJWG1m2bu3LkaNWqUpk2bppSUFE2aNElpaWnavHmzGjZsWKz8rFmzNHr0aL311lvq1q2btmzZoiFDhsjhcGjixIkWXgEQYsrTTRBK3SIVVSh3ewEhzGrLyMSJEzV8+HANHTpUl1xyiaZNm6bq1avrrbfe8lp+1apVuuKKK5Senq6kpCRdd911Gjhw4HlbU4BKg24C++j2AnxmLYwUFBRo7dq1Sk1N/bUyERFKTU3V6tWrve7TrVs3rV271h0+duzYoUWLFunGG28s8TgnT55Ufn6+xw8QtugmCA3+6PZiBVdUIta6aQ4fPiyn06n4+HiP7fHx8dq0aZPXfdLT03X48GFdeeWVMsbo9OnT+uMf/6i//e1vJR4nMzNT48eP92vdgZBFN0HoKE+3V2mnZgNhwvoAVl+sWLFCEyZM0NSpU7Vu3TplZWXpww8/1JNPPlniPmPGjFFeXp77Z8+ePUGsMWAB3QQVGyu4ohKytuhZQUGBqlevrvnz56tv377u7YMHD9bPP/+sf/7zn8X26d69u37zm9/o+eefd2975513dPfdd+vo0aOKiDh/tmLRM1QaobRoGEqHFVwR4gL1HWqtZSQqKkrJyclatmyZe1thYaGWLVumrl27et3n+PHjxQJH5P//H7KSLSSLyqC8YwZKsy4GQos/blwIVEBWp/aOGjVKgwcPVufOnXX55Zdr0qRJOnbsmIYOHSpJGjRokJo0aaLMzExJUu/evTVx4kR16tRJKSkp2rZtmx577DH17t3bHUqAsMCYgfDga+sUK7iikrIaRgYMGKBDhw5p3LhxOnDggDp27KjFixe7B7Xu3r3boyXk0UcflcPh0KOPPqp9+/apQYMG6t27t55++mlbLwHwP1+Wc0foKkugZGo2KilulAeEEsYMhIeSAqW3O/2eKZRvXAgoDMeMAPDCn2MGWKfCjrLcH6hI0dRsqfhaMUzNRhgjjAChxF9jBrKyXFfYPXtK6emu30lJTAsNhvIGSqZmoxKyOmYEwFn8MWaAMSeBU5oBqf4IlP36SX36MDUblQZhBAglRcu5n2/MQEnLuZ+vi8DhcHUR9OnDF5uvSjsg1V+DULlxISoRummAUFLeMQPl7SJgnIl3vqyKyv2BAJ8RRoBQU54xA+XpImCciXe+DkhlECrgM8IIEIrKetfXsnYRcD+UkpWltYlBqIBPWGcECCdlWaeiMq1tUpb79cye7WopOp9Zs1xL75f3eEAIC9R3KANYgXBS1EXQv78rRJwZSErqIvDlyr80AypD9Qu4rEvsl2dAKoNQgVKhmwYIN752EfjzfiiBGHfij0G15emGYkAqEHCEESAc+TLmxF9TUQMx7sQf4aY8K6JKDEgFgoAxI0Bl54/7oQRi3ElZ7+9ythUrXCHmfJYvP3eXirdunsREVxBhQCoqCe5NAyAw/HHl78976kjlb804k7+6oco6wwnAeRFGAJR/Kqo/x51I/g03/uqGkn4dkDpwoOs3XTOAXzCbBoBLee6H4s8vfMm/4aa8S+wDCDjCCIBflXUqqr+/8P3dmuHrdGcAQUU3DYDy8/eME39Pp2VFVCCkEUYA+Ic/v/ADMZ2WAahAyGJqLwD/8ucKrEynBUJKoL5DCSMAQluoLi8PVELcmwZA5cT9XYCwx5gRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgldUw8umnn6p3795q3LixHA6H3n///fPus2LFCl122WWKjo5Wy5YtNXPmzIDXEwAABI7VMHLs2DF16NBBU6ZMKVX53Nxc3XTTTerZs6dycnI0cuRIDRs2TEuWLAlwTQEAQKBUsXnwG264QTfccEOpy0+bNk0XXnihXnzxRUlSmzZttHLlSr300ktKS0sLVDUBAEAAWQ0jvlq9erVSU1M9tqWlpWnkyJEl7nPy5EmdPHnS/XdeXp4kKT8/PyB1BAAgXBV9dxpj/Pq8FSqMHDhwQPHx8R7b4uPjlZ+frxMnTqhatWrF9snMzNT48eOLbU9MTAxYPQEACGc//PCD4uLi/PZ8FSqMlMWYMWM0atQo998///yzmjVrpt27d/v1RKJk+fn5SkxM1J49exQbG2u7OpUC5zz4OOfBxzkPvry8PF1wwQWqW7euX5+3QoWRRo0a6eDBgx7bDh48qNjYWK+tIpIUHR2t6OjoYtvj4uJ48wZZbGws5zzIOOfBxzkPPs558EVE+Hf+S4VaZ6Rr165atmyZx7alS5eqa9eulmoEAADKy2oYOXr0qHJycpSTkyPJNXU3JydHu3fvluTqYhk0aJC7/B//+Eft2LFDjzzyiDZt2qSpU6fqH//4hx566CEb1QcAAH5gNYx88cUX6tSpkzp16iRJGjVqlDp16qRx48ZJkvbv3+8OJpJ04YUX6sMPP9TSpUvVoUMHvfjii3rjjTd8mtYbHR2tjIwMr103CAzOefBxzoOPcx58nPPgC9Q5dxh/z88BAADwQYUaMwIAAMIPYQQAAFhFGAEAAFYRRgAAgFVhGUamTJmipKQkxcTEKCUlRWvWrDln+Xnz5ql169aKiYlR+/bttWjRoiDVNHz4cs6nT5+u7t27q06dOqpTp45SU1PP+2+E4nx9nxeZM2eOHA6H+vbtG9gKhiFfz/nPP/+sESNGKCEhQdHR0brooov4fPGRr+d80qRJuvjii1WtWjUlJibqoYce0i+//BKk2lZ8n376qXr37q3GjRvL4XDo/fffP+8+K1as0GWXXabo6Gi1bNlSM2fO9P3AJszMmTPHREVFmbfeest8/fXXZvjw4aZ27drm4MGDXst/9tlnJjIy0jz33HPmm2++MY8++qipWrWq2bhxY5BrXnH5es7T09PNlClTzPr16823335rhgwZYuLi4szevXuDXPOKy9dzXiQ3N9c0adLEdO/e3fTp0yc4lQ0Tvp7zkydPms6dO5sbb7zRrFy50uTm5poVK1aYnJycINe84vL1nL/77rsmOjravPvuuyY3N9csWbLEJCQkmIceeijINa+4Fi1aZMaOHWuysrKMJPPee++ds/yOHTtM9erVzahRo8w333xjXnnlFRMZGWkWL17s03HDLoxcfvnlZsSIEe6/nU6nady4scnMzPRa/rbbbjM33XSTx7aUlBRzzz33BLSe4cTXc36206dPm1q1apm33347UFUMO2U556dPnzbdunUzb7zxhhk8eDBhxEe+nvO///3vpnnz5qagoCBYVQw7vp7zESNGmF69enlsGzVqlLniiisCWs9wVZow8sgjj5i2bdt6bBswYIBJS0vz6Vhh1U1TUFCgtWvXKjU11b0tIiJCqampWr16tdd9Vq9e7VFektLS0kosD09lOednO378uE6dOuX3Gy+Fq7Ke8yeeeEINGzbUXXfdFYxqhpWynPMPPvhAXbt21YgRIxQfH6927dppwoQJcjqdwap2hVaWc96tWzetXbvW3ZWzY8cOLVq0SDfeeGNQ6lwZ+es7tELdKO98Dh8+LKfTqfj4eI/t8fHx2rRpk9d9Dhw44LX8gQMHAlbPcFKWc362v/71r2rcuHGxNzS8K8s5X7lypd588033rRfgm7Kc8x07duiTTz7R7bffrkWLFmnbtm267777dOrUKWVkZASj2hVaWc55enq6Dh8+rCuvvFLGGJ0+fVp//OMf9be//S0YVa6USvoOzc/P14kTJ0q8ie3ZwqplBBXPM888ozlz5ui9995TTEyM7eqEpSNHjuiOO+7Q9OnTVb9+fdvVqTQKCwvVsGFDvf7660pOTtaAAQM0duxYTZs2zXbVwtaKFSs0YcIETZ06VevWrVNWVpY+/PBDPfnkk7arhvMIq5aR+vXrKzIyUgcPHvTYfvDgQTVq1MjrPo0aNfKpPDyV5ZwXeeGFF/TMM8/o448/1qWXXhrIaoYVX8/59u3btXPnTvXu3du9rbCwUJJUpUoVbd68WS1atAhspSu4srzPExISVLVqVUVGRrq3tWnTRgcOHFBBQYGioqICWueKrizn/LHHHtMdd9yhYcOGSZLat2+vY8eO6e6779bYsWP9ftt7lPwdGhsbW+pWESnMWkaioqKUnJysZcuWubcVFhZq2bJl6tq1q9d9unbt6lFekpYuXVpieXgqyzmXpOeee05PPvmkFi9erM6dOwejqmHD13PeunVrbdy40X2H7JycHN18883q2bOncnJylJiYGMzqV0hleZ9fccUV2rZtmzv4SdKWLVuUkJBAECmFspzz48ePFwscRWHQcBu2gPDbd6hvY2tD35w5c0x0dLSZOXOm+eabb8zdd99tateubQ4cOGCMMeaOO+4wo0ePdpf/7LPPTJUqVcwLL7xgvv32W5ORkcHUXh/5es6feeYZExUVZebPn2/279/v/jly5Iitl1Dh+HrOz8ZsGt/5es53795tatWqZf70pz+ZzZs3m4ULF5qGDRuap556ytZLqHB8PecZGRmmVq1aZvbs2WbHjh3mo48+Mi1atDC33XabrZdQ4Rw5csSsX7/erF+/3kgyEydONOvXrze7du0yxhgzevRoc8cdd7jLF03t/ctf/mK+/fZbM2XKFKb2FnnllVfMBRdcYKKioszll19u/vvf/7of69Gjhxk8eLBH+X/84x/moosuMlFRUaZt27bmww8/DHKNKz5fznmzZs2MpGI/GRkZwa94Bebr+/xMhJGy8fWcr1q1yqSkpJjo6GjTvHlz8/TTT5vTp08HudYVmy/n/NSpU+bxxx83LVq0MDExMSYxMdHcd9995qeffgp+xSuo5cuXe/18LjrPgwcPNj169Ci2T8eOHU1UVJRp3ry5mTFjhs/HdRhD2xUAALAnrMaMAACAiocwAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMALAKqfTqW7duqlfv34e2/Py8pSYmKixY8daqhmAYGE5eADWbdmyRR07dtT06dN1++23S5IGDRqkDRs26PPPP+cut0CYI4wACAkvv/yyHn/8cX399ddas2aNbr31Vn3++efq0KGD7aoBCDDCCICQYIxRr169FBkZqY0bN+r+++/Xo48+artaAIKAMAIgZGzatElt2rRR+/bttW7dOlWpUsV2lQAEAQNYAYSMt956S9WrV1dubq727t1ruzoAgoSWEQAhYdWqVerRo4c++ugjPfXUU5Kkjz/+WA6Hw3LNAAQaLSMArDt+/LiGDBmie++9Vz179tSbb76pNWvWaNq0abarBiAIaBkBYN2DDz6oRYsWacOGDapevbok6bXXXtPDDz+sjRs3KikpyW4FAQQUYQSAVf/5z390zTXXaMWKFbryyis9HktLS9Pp06fprgHCHGEEAABYxZgRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVv0/YReY965O+dAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_key_points = np.load(os.path.join('..', 'data', 'reference_points', 'key_points_xyz.npy'))\n",
    "selected_key_points = loaded_key_points[:, LANDMARK_INDEXES, :]\n",
    "\n",
    "# Plot the key points\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(selected_key_points[:, :, 0], selected_key_points[:, :, 1], c='r', marker='o', label='Selected Landmarks')\n",
    "\n",
    "# Flip Y-axis if needed (often in image coordinates)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Selected Facial Landmarks\")\n",
    "plt.legend()\n",
    "OFFSET = 0.3\n",
    "plt.xlim(0 - OFFSET, 1 + OFFSET)\n",
    "plt.ylim(1 + OFFSET, 0 - OFFSET)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".npy\"):\n",
    "            data = np.load(os.path.join(data_path, file), allow_pickle=True)\n",
    "            data = np.array(data, dtype=np.float32)\n",
    "            selected_data = data[:, LANDMARK_INDEXES, :]\n",
    "\n",
    "            all_data.append(selected_data)\n",
    "\n",
    "            label = int(file.split(\"-\")[2])\n",
    "            all_labels.append(label)\n",
    "\n",
    "    return np.array(all_data, dtype=object), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, all_labels = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, labels):\n",
    "    tensor_data = [torch.tensor(d, dtype=torch.float32) for d in data]\n",
    "    padded_data = pad_sequence(tensor_data, batch_first=True)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    encoded_labels = torch.tensor(encoded_labels, dtype=torch.float32)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        padded_data, encoded_labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(all_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2012, 157, 46, 2])\n",
      "torch.Size([2012, 8])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W podej≈õciu wykorzystane zostanƒÖ 2 modele - pierwszy z nich bƒôdzie sieciƒÖ konwolucyjnƒÖ 2d, kt√≥ra bƒôdzie mia≈Ça za zadanie nauczyƒá siƒô rozpoznawaƒá cechy charakterystyczne dla wybranej klatki (zbioru wsp√≥≈Çrzƒôdnych pkt charakterystycznych). Do klasyfikacji szeregu czasowego zostanie wykorzystana sekwencyjna sieƒá neuronowa LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbudowanie modelu ekstrakcji cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        # Spatial feature extraction using Conv1D\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # LSTM layers for temporal feature extraction\n",
    "        self.lstm1 = nn.LSTM(input_size=736, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128 * 2, hidden_size=64, batch_first=True)\n",
    "        \n",
    "        # Fully connected classification layer\n",
    "        self.fc = nn.Linear(64, 8)  # 8 emotion classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, frames, landmarks, coordinates)\n",
    "        batch_size, frames, landmarks, coordinates = x.shape\n",
    "        \n",
    "        # Reshape for Conv1D: (batch_size * frames, landmarks, coordinates)\n",
    "        x = x.view(-1, landmarks, coordinates).permute(0, 2, 1)\n",
    "        \n",
    "        # Spatial feature extraction\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Flatten spatial features\n",
    "        x = x.view(batch_size, frames, -1)  # (batch_size, frames, features)\n",
    "        \n",
    "        # Temporal feature extraction\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.fc(x[:, -1, :])  # Take the last timestep's output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EmotionClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Train Loss: 130.7564, Train Acc: 0.1203, Val Loss: 28.8912, Val Acc: 0.1346\n",
      "Epoch 2/300, Train Loss: 130.1031, Train Acc: 0.1347, Val Loss: 28.8578, Val Acc: 0.1462\n",
      "Epoch 3/300, Train Loss: 130.0316, Train Acc: 0.1317, Val Loss: 28.8631, Val Acc: 0.1276\n",
      "Epoch 4/300, Train Loss: 130.0780, Train Acc: 0.1352, Val Loss: 28.8676, Val Acc: 0.1369\n",
      "Epoch 5/300, Train Loss: 130.0165, Train Acc: 0.1322, Val Loss: 28.8523, Val Acc: 0.1462\n",
      "Epoch 6/300, Train Loss: 130.0488, Train Acc: 0.1257, Val Loss: 28.8584, Val Acc: 0.1276\n",
      "Epoch 7/300, Train Loss: 130.0137, Train Acc: 0.1292, Val Loss: 28.8626, Val Acc: 0.1276\n",
      "Epoch 8/300, Train Loss: 130.0287, Train Acc: 0.1342, Val Loss: 28.8606, Val Acc: 0.1276\n",
      "Epoch 9/300, Train Loss: 130.0249, Train Acc: 0.1387, Val Loss: 28.8605, Val Acc: 0.1276\n",
      "Epoch 10/300, Train Loss: 130.0356, Train Acc: 0.1252, Val Loss: 28.8675, Val Acc: 0.1276\n",
      "Epoch 11/300, Train Loss: 130.0088, Train Acc: 0.1387, Val Loss: 28.8618, Val Acc: 0.1276\n",
      "Epoch 12/300, Train Loss: 130.0353, Train Acc: 0.1297, Val Loss: 28.8577, Val Acc: 0.1276\n",
      "Epoch 13/300, Train Loss: 130.0638, Train Acc: 0.1272, Val Loss: 28.8628, Val Acc: 0.1369\n",
      "Epoch 14/300, Train Loss: 130.0077, Train Acc: 0.1377, Val Loss: 28.8595, Val Acc: 0.1276\n",
      "Epoch 15/300, Train Loss: 129.9790, Train Acc: 0.1387, Val Loss: 28.8599, Val Acc: 0.1276\n",
      "Epoch 16/300, Train Loss: 130.0212, Train Acc: 0.1282, Val Loss: 28.8575, Val Acc: 0.1276\n",
      "Epoch 17/300, Train Loss: 130.0193, Train Acc: 0.1198, Val Loss: 28.8586, Val Acc: 0.1369\n",
      "Epoch 18/300, Train Loss: 130.0061, Train Acc: 0.1357, Val Loss: 28.8577, Val Acc: 0.1276\n",
      "Epoch 19/300, Train Loss: 129.9093, Train Acc: 0.1501, Val Loss: 28.7782, Val Acc: 0.1763\n",
      "Epoch 20/300, Train Loss: 127.8166, Train Acc: 0.1819, Val Loss: 27.7932, Val Acc: 0.1879\n",
      "Epoch 21/300, Train Loss: 124.6315, Train Acc: 0.2078, Val Loss: 27.8864, Val Acc: 0.1810\n",
      "Epoch 22/300, Train Loss: 124.2168, Train Acc: 0.2083, Val Loss: 27.3505, Val Acc: 0.1972\n",
      "Epoch 23/300, Train Loss: 122.9515, Train Acc: 0.2112, Val Loss: 27.2356, Val Acc: 0.1972\n",
      "Epoch 24/300, Train Loss: 122.8902, Train Acc: 0.2152, Val Loss: 27.3015, Val Acc: 0.2065\n",
      "Epoch 25/300, Train Loss: 122.3875, Train Acc: 0.2167, Val Loss: 27.4334, Val Acc: 0.1856\n",
      "Epoch 26/300, Train Loss: 122.6004, Train Acc: 0.2107, Val Loss: 27.1909, Val Acc: 0.1949\n",
      "Epoch 27/300, Train Loss: 122.3176, Train Acc: 0.2197, Val Loss: 27.1888, Val Acc: 0.1995\n",
      "Epoch 28/300, Train Loss: 122.4296, Train Acc: 0.2162, Val Loss: 27.1556, Val Acc: 0.2111\n",
      "Epoch 29/300, Train Loss: 122.3687, Train Acc: 0.2286, Val Loss: 27.4603, Val Acc: 0.2111\n",
      "Epoch 30/300, Train Loss: 122.4450, Train Acc: 0.2172, Val Loss: 27.1698, Val Acc: 0.1903\n",
      "Epoch 31/300, Train Loss: 122.4470, Train Acc: 0.2137, Val Loss: 27.2178, Val Acc: 0.1717\n",
      "Epoch 32/300, Train Loss: 122.2364, Train Acc: 0.2172, Val Loss: 27.4533, Val Acc: 0.1717\n",
      "Epoch 33/300, Train Loss: 122.0914, Train Acc: 0.2187, Val Loss: 27.1443, Val Acc: 0.2065\n",
      "Epoch 34/300, Train Loss: 122.1113, Train Acc: 0.2222, Val Loss: 27.2002, Val Acc: 0.1856\n",
      "Epoch 35/300, Train Loss: 121.8540, Train Acc: 0.2212, Val Loss: 27.1366, Val Acc: 0.1949\n",
      "Epoch 36/300, Train Loss: 121.8930, Train Acc: 0.2266, Val Loss: 27.1996, Val Acc: 0.2111\n",
      "Epoch 37/300, Train Loss: 121.9617, Train Acc: 0.2207, Val Loss: 27.1794, Val Acc: 0.1995\n",
      "Epoch 38/300, Train Loss: 121.8411, Train Acc: 0.2217, Val Loss: 27.1447, Val Acc: 0.1995\n",
      "Epoch 39/300, Train Loss: 121.7536, Train Acc: 0.2261, Val Loss: 27.0850, Val Acc: 0.1949\n",
      "Epoch 40/300, Train Loss: 121.6979, Train Acc: 0.2306, Val Loss: 27.1363, Val Acc: 0.1995\n",
      "Epoch 41/300, Train Loss: 121.7976, Train Acc: 0.2227, Val Loss: 27.1200, Val Acc: 0.1995\n",
      "Epoch 42/300, Train Loss: 121.5978, Train Acc: 0.2162, Val Loss: 27.1219, Val Acc: 0.2111\n",
      "Epoch 43/300, Train Loss: 121.6132, Train Acc: 0.2261, Val Loss: 27.1040, Val Acc: 0.2019\n",
      "Epoch 44/300, Train Loss: 121.7163, Train Acc: 0.2251, Val Loss: 27.2230, Val Acc: 0.1926\n",
      "Epoch 45/300, Train Loss: 121.7821, Train Acc: 0.2222, Val Loss: 27.1304, Val Acc: 0.1810\n",
      "Epoch 46/300, Train Loss: 121.6407, Train Acc: 0.2256, Val Loss: 27.1737, Val Acc: 0.2065\n",
      "Epoch 47/300, Train Loss: 121.5902, Train Acc: 0.2251, Val Loss: 27.1932, Val Acc: 0.2019\n",
      "Epoch 48/300, Train Loss: 121.6115, Train Acc: 0.2152, Val Loss: 27.1608, Val Acc: 0.1856\n",
      "Epoch 49/300, Train Loss: 121.6769, Train Acc: 0.2217, Val Loss: 27.1457, Val Acc: 0.1972\n",
      "Epoch 50/300, Train Loss: 121.5049, Train Acc: 0.2271, Val Loss: 27.0718, Val Acc: 0.1972\n",
      "Epoch 51/300, Train Loss: 121.5285, Train Acc: 0.2281, Val Loss: 27.0861, Val Acc: 0.1903\n",
      "Epoch 52/300, Train Loss: 121.5354, Train Acc: 0.2242, Val Loss: 27.1566, Val Acc: 0.1903\n",
      "Epoch 53/300, Train Loss: 121.5220, Train Acc: 0.2276, Val Loss: 26.9948, Val Acc: 0.2042\n",
      "Epoch 54/300, Train Loss: 121.4075, Train Acc: 0.2261, Val Loss: 27.1082, Val Acc: 0.2042\n",
      "Epoch 55/300, Train Loss: 121.5638, Train Acc: 0.2162, Val Loss: 27.0582, Val Acc: 0.2065\n",
      "Epoch 56/300, Train Loss: 121.3683, Train Acc: 0.2276, Val Loss: 27.0235, Val Acc: 0.2042\n",
      "Epoch 57/300, Train Loss: 121.6462, Train Acc: 0.2137, Val Loss: 27.0916, Val Acc: 0.2135\n",
      "Epoch 58/300, Train Loss: 121.3755, Train Acc: 0.2276, Val Loss: 27.0572, Val Acc: 0.1972\n",
      "Epoch 59/300, Train Loss: 121.3320, Train Acc: 0.2311, Val Loss: 27.1413, Val Acc: 0.2088\n",
      "Epoch 60/300, Train Loss: 121.5723, Train Acc: 0.2237, Val Loss: 27.0680, Val Acc: 0.2042\n",
      "Epoch 61/300, Train Loss: 121.3631, Train Acc: 0.2202, Val Loss: 27.0877, Val Acc: 0.1972\n",
      "Epoch 62/300, Train Loss: 121.3631, Train Acc: 0.2197, Val Loss: 27.0092, Val Acc: 0.2042\n",
      "Epoch 63/300, Train Loss: 121.3564, Train Acc: 0.2207, Val Loss: 27.0153, Val Acc: 0.1995\n",
      "Epoch 64/300, Train Loss: 121.4085, Train Acc: 0.2182, Val Loss: 27.0293, Val Acc: 0.2111\n",
      "Epoch 65/300, Train Loss: 121.5534, Train Acc: 0.2137, Val Loss: 27.0924, Val Acc: 0.2019\n",
      "Epoch 66/300, Train Loss: 121.0403, Train Acc: 0.2256, Val Loss: 27.0005, Val Acc: 0.1972\n",
      "Epoch 67/300, Train Loss: 121.3517, Train Acc: 0.2247, Val Loss: 27.0126, Val Acc: 0.2088\n",
      "Epoch 68/300, Train Loss: 121.1637, Train Acc: 0.2281, Val Loss: 27.0159, Val Acc: 0.1972\n",
      "Epoch 69/300, Train Loss: 121.0879, Train Acc: 0.2197, Val Loss: 26.9310, Val Acc: 0.2065\n",
      "Epoch 70/300, Train Loss: 120.8479, Train Acc: 0.2207, Val Loss: 26.9486, Val Acc: 0.2135\n",
      "Epoch 71/300, Train Loss: 121.1020, Train Acc: 0.2266, Val Loss: 27.1125, Val Acc: 0.1972\n",
      "Epoch 72/300, Train Loss: 121.4213, Train Acc: 0.2207, Val Loss: 27.0151, Val Acc: 0.1995\n",
      "Epoch 73/300, Train Loss: 120.8076, Train Acc: 0.2157, Val Loss: 27.2357, Val Acc: 0.2042\n",
      "Epoch 74/300, Train Loss: 121.1952, Train Acc: 0.2242, Val Loss: 27.0271, Val Acc: 0.2111\n",
      "Epoch 75/300, Train Loss: 120.7629, Train Acc: 0.2266, Val Loss: 26.9736, Val Acc: 0.1972\n",
      "Epoch 76/300, Train Loss: 121.1990, Train Acc: 0.2242, Val Loss: 27.0584, Val Acc: 0.1972\n",
      "Epoch 77/300, Train Loss: 121.3349, Train Acc: 0.2177, Val Loss: 27.0799, Val Acc: 0.1903\n",
      "Epoch 78/300, Train Loss: 120.6851, Train Acc: 0.2237, Val Loss: 26.8534, Val Acc: 0.2088\n",
      "Epoch 79/300, Train Loss: 120.6130, Train Acc: 0.2336, Val Loss: 26.9551, Val Acc: 0.2065\n",
      "Epoch 80/300, Train Loss: 120.7676, Train Acc: 0.2266, Val Loss: 26.9715, Val Acc: 0.1856\n",
      "Epoch 81/300, Train Loss: 120.4375, Train Acc: 0.2286, Val Loss: 26.9318, Val Acc: 0.1926\n",
      "Epoch 82/300, Train Loss: 120.0711, Train Acc: 0.2286, Val Loss: 26.6066, Val Acc: 0.2065\n",
      "Epoch 83/300, Train Loss: 120.0932, Train Acc: 0.2336, Val Loss: 26.9129, Val Acc: 0.2019\n",
      "Epoch 84/300, Train Loss: 120.9484, Train Acc: 0.2311, Val Loss: 26.8278, Val Acc: 0.2204\n",
      "Epoch 85/300, Train Loss: 120.0497, Train Acc: 0.2401, Val Loss: 26.6815, Val Acc: 0.2135\n",
      "Epoch 86/300, Train Loss: 119.2164, Train Acc: 0.2361, Val Loss: 26.4526, Val Acc: 0.2019\n",
      "Epoch 87/300, Train Loss: 120.4227, Train Acc: 0.2316, Val Loss: 26.7181, Val Acc: 0.2042\n",
      "Epoch 88/300, Train Loss: 119.4765, Train Acc: 0.2406, Val Loss: 26.5543, Val Acc: 0.2111\n",
      "Epoch 89/300, Train Loss: 120.6014, Train Acc: 0.2266, Val Loss: 26.6218, Val Acc: 0.2042\n",
      "Epoch 90/300, Train Loss: 118.9565, Train Acc: 0.2425, Val Loss: 27.3040, Val Acc: 0.2042\n",
      "Epoch 91/300, Train Loss: 120.1519, Train Acc: 0.2256, Val Loss: 27.0692, Val Acc: 0.2065\n",
      "Epoch 92/300, Train Loss: 120.2314, Train Acc: 0.2435, Val Loss: 26.8670, Val Acc: 0.2158\n",
      "Epoch 93/300, Train Loss: 119.2274, Train Acc: 0.2455, Val Loss: 26.3602, Val Acc: 0.2251\n",
      "Epoch 94/300, Train Loss: 118.8076, Train Acc: 0.2445, Val Loss: 27.0001, Val Acc: 0.2088\n",
      "Epoch 95/300, Train Loss: 118.4182, Train Acc: 0.2540, Val Loss: 26.3107, Val Acc: 0.2274\n",
      "Epoch 96/300, Train Loss: 118.6986, Train Acc: 0.2505, Val Loss: 27.1288, Val Acc: 0.2111\n",
      "Epoch 97/300, Train Loss: 118.5609, Train Acc: 0.2515, Val Loss: 26.2769, Val Acc: 0.2158\n",
      "Epoch 98/300, Train Loss: 117.9896, Train Acc: 0.2550, Val Loss: 26.4157, Val Acc: 0.2158\n",
      "Epoch 99/300, Train Loss: 118.2456, Train Acc: 0.2465, Val Loss: 27.0820, Val Acc: 0.2111\n",
      "Epoch 100/300, Train Loss: 118.3128, Train Acc: 0.2589, Val Loss: 27.1270, Val Acc: 0.2181\n",
      "Epoch 101/300, Train Loss: 119.0662, Train Acc: 0.2470, Val Loss: 26.8903, Val Acc: 0.2181\n",
      "Epoch 102/300, Train Loss: 117.6003, Train Acc: 0.2639, Val Loss: 26.2842, Val Acc: 0.2135\n",
      "Epoch 103/300, Train Loss: 117.4131, Train Acc: 0.2599, Val Loss: 26.3137, Val Acc: 0.2111\n",
      "Epoch 104/300, Train Loss: 117.3272, Train Acc: 0.2530, Val Loss: 26.6505, Val Acc: 0.2204\n",
      "Epoch 105/300, Train Loss: 117.5771, Train Acc: 0.2440, Val Loss: 26.2726, Val Acc: 0.2181\n",
      "Epoch 106/300, Train Loss: 118.2960, Train Acc: 0.2485, Val Loss: 26.5848, Val Acc: 0.2227\n",
      "Epoch 107/300, Train Loss: 116.8459, Train Acc: 0.2674, Val Loss: 26.3004, Val Acc: 0.2320\n",
      "Epoch 108/300, Train Loss: 117.2278, Train Acc: 0.2575, Val Loss: 27.0989, Val Acc: 0.2019\n",
      "Epoch 109/300, Train Loss: 118.5261, Train Acc: 0.2460, Val Loss: 26.3628, Val Acc: 0.2204\n",
      "Epoch 110/300, Train Loss: 117.4310, Train Acc: 0.2510, Val Loss: 26.2860, Val Acc: 0.2135\n",
      "Epoch 111/300, Train Loss: 116.2701, Train Acc: 0.2694, Val Loss: 26.1774, Val Acc: 0.2436\n",
      "Epoch 112/300, Train Loss: 117.0386, Train Acc: 0.2669, Val Loss: 26.5189, Val Acc: 0.2367\n",
      "Epoch 113/300, Train Loss: 116.3604, Train Acc: 0.2629, Val Loss: 26.2397, Val Acc: 0.2297\n",
      "Epoch 114/300, Train Loss: 118.4422, Train Acc: 0.2515, Val Loss: 26.2984, Val Acc: 0.2320\n",
      "Epoch 115/300, Train Loss: 115.9232, Train Acc: 0.2709, Val Loss: 26.1759, Val Acc: 0.2274\n",
      "Epoch 116/300, Train Loss: 116.4406, Train Acc: 0.2639, Val Loss: 26.8897, Val Acc: 0.2204\n",
      "Epoch 117/300, Train Loss: 115.7652, Train Acc: 0.2729, Val Loss: 26.6375, Val Acc: 0.2204\n",
      "Epoch 118/300, Train Loss: 116.5828, Train Acc: 0.2594, Val Loss: 26.1230, Val Acc: 0.2204\n",
      "Epoch 119/300, Train Loss: 116.3380, Train Acc: 0.2624, Val Loss: 26.4734, Val Acc: 0.2251\n",
      "Epoch 120/300, Train Loss: 115.9760, Train Acc: 0.2694, Val Loss: 26.0362, Val Acc: 0.2181\n",
      "Epoch 121/300, Train Loss: 114.9742, Train Acc: 0.2694, Val Loss: 26.2134, Val Acc: 0.2204\n",
      "Epoch 122/300, Train Loss: 115.2219, Train Acc: 0.2768, Val Loss: 25.9354, Val Acc: 0.2483\n",
      "Epoch 123/300, Train Loss: 115.4246, Train Acc: 0.2679, Val Loss: 26.1559, Val Acc: 0.2483\n",
      "Epoch 124/300, Train Loss: 114.9599, Train Acc: 0.2704, Val Loss: 26.0246, Val Acc: 0.2274\n",
      "Epoch 125/300, Train Loss: 116.1060, Train Acc: 0.2724, Val Loss: 26.6427, Val Acc: 0.2367\n",
      "Epoch 126/300, Train Loss: 115.9593, Train Acc: 0.2654, Val Loss: 25.9226, Val Acc: 0.2320\n",
      "Epoch 127/300, Train Loss: 114.2526, Train Acc: 0.2729, Val Loss: 25.8237, Val Acc: 0.2343\n",
      "Epoch 128/300, Train Loss: 114.1750, Train Acc: 0.2843, Val Loss: 25.7579, Val Acc: 0.2436\n",
      "Epoch 129/300, Train Loss: 114.7437, Train Acc: 0.2629, Val Loss: 26.0479, Val Acc: 0.2367\n",
      "Epoch 130/300, Train Loss: 114.1945, Train Acc: 0.2758, Val Loss: 26.0205, Val Acc: 0.2111\n",
      "Epoch 131/300, Train Loss: 114.1683, Train Acc: 0.2758, Val Loss: 25.9191, Val Acc: 0.2506\n",
      "Epoch 132/300, Train Loss: 115.4137, Train Acc: 0.2883, Val Loss: 25.7895, Val Acc: 0.2158\n",
      "Epoch 133/300, Train Loss: 113.5606, Train Acc: 0.2868, Val Loss: 25.7387, Val Acc: 0.2251\n",
      "Epoch 134/300, Train Loss: 113.1657, Train Acc: 0.2878, Val Loss: 25.6543, Val Acc: 0.2575\n",
      "Epoch 135/300, Train Loss: 113.8439, Train Acc: 0.2803, Val Loss: 26.0189, Val Acc: 0.2390\n",
      "Epoch 136/300, Train Loss: 115.5122, Train Acc: 0.2724, Val Loss: 25.9891, Val Acc: 0.2343\n",
      "Epoch 137/300, Train Loss: 116.0404, Train Acc: 0.2739, Val Loss: 25.9369, Val Acc: 0.2320\n",
      "Epoch 138/300, Train Loss: 113.9888, Train Acc: 0.2749, Val Loss: 25.5127, Val Acc: 0.2320\n",
      "Epoch 139/300, Train Loss: 113.9387, Train Acc: 0.2853, Val Loss: 25.8460, Val Acc: 0.2297\n",
      "Epoch 140/300, Train Loss: 113.4789, Train Acc: 0.2709, Val Loss: 25.8024, Val Acc: 0.2320\n",
      "Epoch 141/300, Train Loss: 115.1114, Train Acc: 0.2739, Val Loss: 25.9753, Val Acc: 0.2297\n",
      "Epoch 142/300, Train Loss: 113.2293, Train Acc: 0.2863, Val Loss: 25.9491, Val Acc: 0.2691\n",
      "Epoch 143/300, Train Loss: 112.7522, Train Acc: 0.2977, Val Loss: 25.6249, Val Acc: 0.2506\n",
      "Epoch 144/300, Train Loss: 114.0359, Train Acc: 0.2932, Val Loss: 25.7427, Val Acc: 0.2738\n",
      "Epoch 145/300, Train Loss: 112.9158, Train Acc: 0.2952, Val Loss: 25.7501, Val Acc: 0.2459\n",
      "Epoch 146/300, Train Loss: 112.9543, Train Acc: 0.2873, Val Loss: 25.4180, Val Acc: 0.2506\n",
      "Epoch 147/300, Train Loss: 111.9107, Train Acc: 0.3082, Val Loss: 25.4198, Val Acc: 0.2506\n",
      "Epoch 148/300, Train Loss: 113.1779, Train Acc: 0.2932, Val Loss: 25.4313, Val Acc: 0.2506\n",
      "Epoch 149/300, Train Loss: 114.5726, Train Acc: 0.2793, Val Loss: 26.1427, Val Acc: 0.2274\n",
      "Epoch 150/300, Train Loss: 112.4183, Train Acc: 0.2962, Val Loss: 25.7826, Val Acc: 0.2552\n",
      "Epoch 151/300, Train Loss: 114.7143, Train Acc: 0.2793, Val Loss: 26.1919, Val Acc: 0.2204\n",
      "Epoch 152/300, Train Loss: 114.4516, Train Acc: 0.2803, Val Loss: 25.3673, Val Acc: 0.2645\n",
      "Epoch 153/300, Train Loss: 112.9207, Train Acc: 0.2883, Val Loss: 25.6082, Val Acc: 0.2529\n",
      "Epoch 154/300, Train Loss: 112.7656, Train Acc: 0.2917, Val Loss: 25.8297, Val Acc: 0.2274\n",
      "Epoch 155/300, Train Loss: 112.8776, Train Acc: 0.2942, Val Loss: 25.4104, Val Acc: 0.2274\n",
      "Epoch 156/300, Train Loss: 111.6169, Train Acc: 0.2982, Val Loss: 25.1629, Val Acc: 0.2599\n",
      "Epoch 157/300, Train Loss: 112.3385, Train Acc: 0.2977, Val Loss: 25.3138, Val Acc: 0.2691\n",
      "Epoch 158/300, Train Loss: 111.6818, Train Acc: 0.3106, Val Loss: 25.0803, Val Acc: 0.2900\n",
      "Epoch 159/300, Train Loss: 112.6720, Train Acc: 0.2952, Val Loss: 25.8360, Val Acc: 0.2320\n",
      "Epoch 160/300, Train Loss: 111.7768, Train Acc: 0.2967, Val Loss: 25.1248, Val Acc: 0.2483\n",
      "Epoch 161/300, Train Loss: 112.0827, Train Acc: 0.2898, Val Loss: 25.2551, Val Acc: 0.2645\n",
      "Epoch 162/300, Train Loss: 111.4245, Train Acc: 0.2987, Val Loss: 25.1874, Val Acc: 0.2622\n",
      "Epoch 163/300, Train Loss: 111.7653, Train Acc: 0.3012, Val Loss: 25.1251, Val Acc: 0.2413\n",
      "Epoch 164/300, Train Loss: 110.3138, Train Acc: 0.3106, Val Loss: 25.0300, Val Acc: 0.2645\n",
      "Epoch 165/300, Train Loss: 110.9984, Train Acc: 0.3082, Val Loss: 25.6655, Val Acc: 0.2367\n",
      "Epoch 166/300, Train Loss: 112.4430, Train Acc: 0.3091, Val Loss: 25.3395, Val Acc: 0.2900\n",
      "Epoch 167/300, Train Loss: 111.4790, Train Acc: 0.3072, Val Loss: 25.2465, Val Acc: 0.2552\n",
      "Epoch 168/300, Train Loss: 111.7544, Train Acc: 0.2982, Val Loss: 25.4768, Val Acc: 0.2668\n",
      "Epoch 169/300, Train Loss: 110.3239, Train Acc: 0.3037, Val Loss: 24.9933, Val Acc: 0.2691\n",
      "Epoch 170/300, Train Loss: 112.3879, Train Acc: 0.2957, Val Loss: 25.0934, Val Acc: 0.2738\n",
      "Epoch 171/300, Train Loss: 110.6000, Train Acc: 0.3196, Val Loss: 25.0155, Val Acc: 0.2622\n",
      "Epoch 172/300, Train Loss: 110.9045, Train Acc: 0.3057, Val Loss: 25.4200, Val Acc: 0.2715\n",
      "Epoch 173/300, Train Loss: 110.9250, Train Acc: 0.3007, Val Loss: 25.3074, Val Acc: 0.2529\n",
      "Epoch 174/300, Train Loss: 110.5698, Train Acc: 0.3072, Val Loss: 24.8935, Val Acc: 0.2993\n",
      "Epoch 175/300, Train Loss: 111.1800, Train Acc: 0.3072, Val Loss: 26.0472, Val Acc: 0.2413\n",
      "Epoch 176/300, Train Loss: 112.1626, Train Acc: 0.3052, Val Loss: 25.2384, Val Acc: 0.2529\n",
      "Epoch 177/300, Train Loss: 112.5297, Train Acc: 0.2883, Val Loss: 25.1787, Val Acc: 0.2483\n",
      "Epoch 178/300, Train Loss: 112.1840, Train Acc: 0.2997, Val Loss: 24.9916, Val Acc: 0.2529\n",
      "Epoch 179/300, Train Loss: 110.1425, Train Acc: 0.3096, Val Loss: 24.9653, Val Acc: 0.2645\n",
      "Epoch 180/300, Train Loss: 111.2439, Train Acc: 0.3057, Val Loss: 26.1310, Val Acc: 0.2413\n",
      "Epoch 181/300, Train Loss: 109.0031, Train Acc: 0.3171, Val Loss: 24.5638, Val Acc: 0.2668\n",
      "Epoch 182/300, Train Loss: 108.7839, Train Acc: 0.3320, Val Loss: 24.8371, Val Acc: 0.2761\n",
      "Epoch 183/300, Train Loss: 109.8505, Train Acc: 0.3236, Val Loss: 25.0281, Val Acc: 0.2645\n",
      "Epoch 184/300, Train Loss: 108.6429, Train Acc: 0.3196, Val Loss: 24.3038, Val Acc: 0.2854\n",
      "Epoch 185/300, Train Loss: 108.4314, Train Acc: 0.3221, Val Loss: 25.1180, Val Acc: 0.2645\n",
      "Epoch 186/300, Train Loss: 108.1174, Train Acc: 0.3226, Val Loss: 24.4770, Val Acc: 0.2715\n",
      "Epoch 187/300, Train Loss: 107.2526, Train Acc: 0.3310, Val Loss: 24.1155, Val Acc: 0.2831\n",
      "Epoch 188/300, Train Loss: 108.1676, Train Acc: 0.3265, Val Loss: 25.6359, Val Acc: 0.2854\n",
      "Epoch 189/300, Train Loss: 111.5390, Train Acc: 0.3017, Val Loss: 25.7136, Val Acc: 0.2483\n",
      "Epoch 190/300, Train Loss: 106.9994, Train Acc: 0.3335, Val Loss: 24.4807, Val Acc: 0.3086\n",
      "Epoch 191/300, Train Loss: 106.8452, Train Acc: 0.3325, Val Loss: 25.1871, Val Acc: 0.2691\n",
      "Epoch 192/300, Train Loss: 105.4434, Train Acc: 0.3494, Val Loss: 23.6308, Val Acc: 0.3202\n",
      "Epoch 193/300, Train Loss: 104.1088, Train Acc: 0.3434, Val Loss: 24.2823, Val Acc: 0.2738\n",
      "Epoch 194/300, Train Loss: 104.5971, Train Acc: 0.3564, Val Loss: 23.7503, Val Acc: 0.3155\n",
      "Epoch 195/300, Train Loss: 103.9277, Train Acc: 0.3569, Val Loss: 23.5451, Val Acc: 0.2947\n",
      "Epoch 196/300, Train Loss: 104.0679, Train Acc: 0.3613, Val Loss: 23.2599, Val Acc: 0.3039\n",
      "Epoch 197/300, Train Loss: 102.0842, Train Acc: 0.3673, Val Loss: 23.3848, Val Acc: 0.3434\n",
      "Epoch 198/300, Train Loss: 106.8172, Train Acc: 0.3355, Val Loss: 23.4118, Val Acc: 0.3318\n",
      "Epoch 199/300, Train Loss: 104.6061, Train Acc: 0.3484, Val Loss: 24.0630, Val Acc: 0.3155\n",
      "Epoch 200/300, Train Loss: 101.1818, Train Acc: 0.3767, Val Loss: 23.6766, Val Acc: 0.3248\n",
      "Epoch 201/300, Train Loss: 100.9360, Train Acc: 0.3723, Val Loss: 24.5095, Val Acc: 0.3086\n",
      "Epoch 202/300, Train Loss: 100.9873, Train Acc: 0.3733, Val Loss: 23.4228, Val Acc: 0.3039\n",
      "Epoch 203/300, Train Loss: 102.0349, Train Acc: 0.3738, Val Loss: 24.6965, Val Acc: 0.3063\n",
      "Epoch 204/300, Train Loss: 100.8237, Train Acc: 0.3802, Val Loss: 22.5811, Val Acc: 0.3225\n",
      "Epoch 205/300, Train Loss: 100.6638, Train Acc: 0.3673, Val Loss: 23.6177, Val Acc: 0.3341\n",
      "Epoch 206/300, Train Loss: 102.6015, Train Acc: 0.3638, Val Loss: 23.1620, Val Acc: 0.2970\n",
      "Epoch 207/300, Train Loss: 99.6534, Train Acc: 0.3842, Val Loss: 23.2073, Val Acc: 0.3109\n",
      "Epoch 208/300, Train Loss: 97.6709, Train Acc: 0.4081, Val Loss: 22.5797, Val Acc: 0.3225\n",
      "Epoch 209/300, Train Loss: 97.6225, Train Acc: 0.3902, Val Loss: 23.7192, Val Acc: 0.3341\n",
      "Epoch 210/300, Train Loss: 98.3457, Train Acc: 0.3976, Val Loss: 22.2197, Val Acc: 0.3944\n",
      "Epoch 211/300, Train Loss: 96.3607, Train Acc: 0.4026, Val Loss: 22.1560, Val Acc: 0.3480\n",
      "Epoch 212/300, Train Loss: 96.4108, Train Acc: 0.4031, Val Loss: 22.1587, Val Acc: 0.3503\n",
      "Epoch 213/300, Train Loss: 97.1835, Train Acc: 0.4011, Val Loss: 22.2403, Val Acc: 0.3689\n",
      "Epoch 214/300, Train Loss: 94.9699, Train Acc: 0.4085, Val Loss: 21.7799, Val Acc: 0.3689\n",
      "Epoch 215/300, Train Loss: 92.9786, Train Acc: 0.4423, Val Loss: 20.8764, Val Acc: 0.3759\n",
      "Epoch 216/300, Train Loss: 94.9579, Train Acc: 0.4190, Val Loss: 22.8679, Val Acc: 0.3295\n",
      "Epoch 217/300, Train Loss: 94.0060, Train Acc: 0.4150, Val Loss: 21.1178, Val Acc: 0.3759\n",
      "Epoch 218/300, Train Loss: 96.6779, Train Acc: 0.4105, Val Loss: 23.5223, Val Acc: 0.3225\n",
      "Epoch 219/300, Train Loss: 97.9181, Train Acc: 0.4046, Val Loss: 23.9691, Val Acc: 0.3016\n",
      "Epoch 220/300, Train Loss: 96.0816, Train Acc: 0.4066, Val Loss: 21.4719, Val Acc: 0.3968\n",
      "Epoch 221/300, Train Loss: 95.0778, Train Acc: 0.4056, Val Loss: 22.5917, Val Acc: 0.3318\n",
      "Epoch 222/300, Train Loss: 96.0823, Train Acc: 0.4155, Val Loss: 21.7522, Val Acc: 0.3689\n",
      "Epoch 223/300, Train Loss: 91.9359, Train Acc: 0.4299, Val Loss: 20.5705, Val Acc: 0.4176\n",
      "Epoch 224/300, Train Loss: 89.5143, Train Acc: 0.4602, Val Loss: 20.3079, Val Acc: 0.4176\n",
      "Epoch 225/300, Train Loss: 88.8729, Train Acc: 0.4587, Val Loss: 21.4131, Val Acc: 0.3550\n",
      "Epoch 226/300, Train Loss: 91.3911, Train Acc: 0.4458, Val Loss: 21.1295, Val Acc: 0.3944\n",
      "Epoch 227/300, Train Loss: 93.9305, Train Acc: 0.4279, Val Loss: 22.2724, Val Acc: 0.3527\n",
      "Epoch 228/300, Train Loss: 92.2455, Train Acc: 0.4384, Val Loss: 20.5702, Val Acc: 0.4130\n",
      "Epoch 229/300, Train Loss: 89.2375, Train Acc: 0.4483, Val Loss: 20.8989, Val Acc: 0.4014\n",
      "Epoch 230/300, Train Loss: 92.2318, Train Acc: 0.4369, Val Loss: 23.1990, Val Acc: 0.3341\n",
      "Epoch 231/300, Train Loss: 91.0272, Train Acc: 0.4319, Val Loss: 21.3780, Val Acc: 0.3573\n",
      "Epoch 232/300, Train Loss: 90.4778, Train Acc: 0.4478, Val Loss: 20.0961, Val Acc: 0.3828\n",
      "Epoch 233/300, Train Loss: 88.8675, Train Acc: 0.4632, Val Loss: 20.3254, Val Acc: 0.4060\n",
      "Epoch 234/300, Train Loss: 88.0217, Train Acc: 0.4632, Val Loss: 19.5527, Val Acc: 0.4339\n",
      "Epoch 235/300, Train Loss: 88.5203, Train Acc: 0.4468, Val Loss: 19.9713, Val Acc: 0.4060\n",
      "Epoch 236/300, Train Loss: 92.0450, Train Acc: 0.4165, Val Loss: 20.2569, Val Acc: 0.3921\n",
      "Epoch 237/300, Train Loss: 85.7163, Train Acc: 0.4687, Val Loss: 19.8520, Val Acc: 0.4107\n",
      "Epoch 238/300, Train Loss: 89.4196, Train Acc: 0.4354, Val Loss: 19.7582, Val Acc: 0.4014\n",
      "Epoch 239/300, Train Loss: 88.9487, Train Acc: 0.4568, Val Loss: 21.7963, Val Acc: 0.3596\n",
      "Epoch 240/300, Train Loss: 89.8238, Train Acc: 0.4473, Val Loss: 20.8230, Val Acc: 0.3643\n",
      "Epoch 241/300, Train Loss: 87.2496, Train Acc: 0.4632, Val Loss: 19.3741, Val Acc: 0.4246\n",
      "Epoch 242/300, Train Loss: 84.2396, Train Acc: 0.4886, Val Loss: 21.7625, Val Acc: 0.3828\n",
      "Epoch 243/300, Train Loss: 86.7295, Train Acc: 0.4583, Val Loss: 20.0547, Val Acc: 0.4037\n",
      "Epoch 244/300, Train Loss: 84.4357, Train Acc: 0.4901, Val Loss: 19.2305, Val Acc: 0.4037\n",
      "Epoch 245/300, Train Loss: 85.8548, Train Acc: 0.4647, Val Loss: 20.2453, Val Acc: 0.3898\n",
      "Epoch 246/300, Train Loss: 84.8128, Train Acc: 0.4816, Val Loss: 18.9670, Val Acc: 0.4269\n",
      "Epoch 247/300, Train Loss: 83.1039, Train Acc: 0.4906, Val Loss: 20.9660, Val Acc: 0.4084\n",
      "Epoch 248/300, Train Loss: 85.1468, Train Acc: 0.4642, Val Loss: 19.2641, Val Acc: 0.4107\n",
      "Epoch 249/300, Train Loss: 84.9592, Train Acc: 0.4727, Val Loss: 20.2316, Val Acc: 0.4246\n",
      "Epoch 250/300, Train Loss: 83.7943, Train Acc: 0.4871, Val Loss: 19.6015, Val Acc: 0.4223\n",
      "Epoch 251/300, Train Loss: 82.5573, Train Acc: 0.4816, Val Loss: 19.3591, Val Acc: 0.4316\n",
      "Epoch 252/300, Train Loss: 82.0178, Train Acc: 0.4985, Val Loss: 18.5582, Val Acc: 0.4571\n",
      "Epoch 253/300, Train Loss: 82.3525, Train Acc: 0.4935, Val Loss: 19.4582, Val Acc: 0.4316\n",
      "Epoch 254/300, Train Loss: 82.2182, Train Acc: 0.4920, Val Loss: 19.9265, Val Acc: 0.4292\n",
      "Epoch 255/300, Train Loss: 80.5910, Train Acc: 0.4995, Val Loss: 19.2280, Val Acc: 0.4432\n",
      "Epoch 256/300, Train Loss: 81.3702, Train Acc: 0.4925, Val Loss: 18.9679, Val Acc: 0.4316\n",
      "Epoch 257/300, Train Loss: 81.9731, Train Acc: 0.4925, Val Loss: 21.2580, Val Acc: 0.4060\n",
      "Epoch 258/300, Train Loss: 88.5803, Train Acc: 0.4493, Val Loss: 18.9362, Val Acc: 0.4362\n",
      "Epoch 259/300, Train Loss: 84.0243, Train Acc: 0.4811, Val Loss: 19.8478, Val Acc: 0.4292\n",
      "Epoch 260/300, Train Loss: 81.6578, Train Acc: 0.4950, Val Loss: 19.1345, Val Acc: 0.4385\n",
      "Epoch 261/300, Train Loss: 80.6352, Train Acc: 0.4975, Val Loss: 18.3448, Val Acc: 0.4826\n",
      "Epoch 262/300, Train Loss: 80.1761, Train Acc: 0.5060, Val Loss: 18.5705, Val Acc: 0.4571\n",
      "Epoch 263/300, Train Loss: 82.4096, Train Acc: 0.4791, Val Loss: 20.6099, Val Acc: 0.3875\n",
      "Epoch 264/300, Train Loss: 82.2736, Train Acc: 0.5060, Val Loss: 19.5273, Val Acc: 0.4408\n",
      "Epoch 265/300, Train Loss: 85.3730, Train Acc: 0.4776, Val Loss: 18.3716, Val Acc: 0.4942\n",
      "Epoch 266/300, Train Loss: 78.4513, Train Acc: 0.5258, Val Loss: 19.2825, Val Acc: 0.4571\n",
      "Epoch 267/300, Train Loss: 84.8664, Train Acc: 0.4761, Val Loss: 19.1656, Val Acc: 0.4640\n",
      "Epoch 268/300, Train Loss: 82.8604, Train Acc: 0.4836, Val Loss: 19.9479, Val Acc: 0.4014\n",
      "Epoch 269/300, Train Loss: 81.1948, Train Acc: 0.4960, Val Loss: 18.6258, Val Acc: 0.4478\n",
      "Epoch 270/300, Train Loss: 79.0434, Train Acc: 0.5114, Val Loss: 18.2647, Val Acc: 0.4664\n",
      "Epoch 271/300, Train Loss: 81.2253, Train Acc: 0.4995, Val Loss: 18.5286, Val Acc: 0.4687\n",
      "Epoch 272/300, Train Loss: 79.3654, Train Acc: 0.5219, Val Loss: 18.8261, Val Acc: 0.4756\n",
      "Epoch 273/300, Train Loss: 81.4373, Train Acc: 0.5040, Val Loss: 18.7953, Val Acc: 0.4617\n",
      "Epoch 274/300, Train Loss: 80.8359, Train Acc: 0.5040, Val Loss: 18.2955, Val Acc: 0.4919\n",
      "Epoch 275/300, Train Loss: 80.1703, Train Acc: 0.5144, Val Loss: 19.6448, Val Acc: 0.4524\n",
      "Epoch 276/300, Train Loss: 85.5562, Train Acc: 0.4781, Val Loss: 19.8843, Val Acc: 0.4548\n",
      "Epoch 277/300, Train Loss: 79.0632, Train Acc: 0.5040, Val Loss: 18.0491, Val Acc: 0.4826\n",
      "Epoch 278/300, Train Loss: 76.8400, Train Acc: 0.5303, Val Loss: 18.0870, Val Acc: 0.4780\n",
      "Epoch 279/300, Train Loss: 83.2455, Train Acc: 0.5030, Val Loss: 20.1670, Val Acc: 0.4339\n",
      "Epoch 280/300, Train Loss: 85.1044, Train Acc: 0.4737, Val Loss: 19.4495, Val Acc: 0.4316\n",
      "Epoch 281/300, Train Loss: 81.8716, Train Acc: 0.4990, Val Loss: 17.8934, Val Acc: 0.4455\n",
      "Epoch 282/300, Train Loss: 77.4512, Train Acc: 0.5149, Val Loss: 17.7259, Val Acc: 0.4919\n",
      "Epoch 283/300, Train Loss: 78.8701, Train Acc: 0.5214, Val Loss: 17.8413, Val Acc: 0.4872\n",
      "Epoch 284/300, Train Loss: 75.5802, Train Acc: 0.5378, Val Loss: 17.5882, Val Acc: 0.5128\n",
      "Epoch 285/300, Train Loss: 80.5926, Train Acc: 0.5060, Val Loss: 17.8260, Val Acc: 0.5058\n",
      "Epoch 286/300, Train Loss: 78.5002, Train Acc: 0.5199, Val Loss: 18.4190, Val Acc: 0.4687\n",
      "Epoch 287/300, Train Loss: 77.6850, Train Acc: 0.5263, Val Loss: 21.2491, Val Acc: 0.3991\n",
      "Epoch 288/300, Train Loss: 78.0289, Train Acc: 0.5189, Val Loss: 20.9166, Val Acc: 0.4432\n",
      "Epoch 289/300, Train Loss: 81.0708, Train Acc: 0.5055, Val Loss: 17.3879, Val Acc: 0.5151\n",
      "Epoch 290/300, Train Loss: 74.3036, Train Acc: 0.5437, Val Loss: 17.1514, Val Acc: 0.5290\n",
      "Epoch 291/300, Train Loss: 72.9234, Train Acc: 0.5626, Val Loss: 18.0895, Val Acc: 0.4988\n",
      "Epoch 292/300, Train Loss: 74.6220, Train Acc: 0.5403, Val Loss: 17.7836, Val Acc: 0.5012\n",
      "Epoch 293/300, Train Loss: 79.2973, Train Acc: 0.5169, Val Loss: 18.1003, Val Acc: 0.4849\n",
      "Epoch 294/300, Train Loss: 75.8758, Train Acc: 0.5427, Val Loss: 18.6922, Val Acc: 0.4664\n",
      "Epoch 295/300, Train Loss: 79.4429, Train Acc: 0.5184, Val Loss: 17.1246, Val Acc: 0.5568\n",
      "Epoch 296/300, Train Loss: 76.5908, Train Acc: 0.5413, Val Loss: 19.3060, Val Acc: 0.4803\n",
      "Epoch 297/300, Train Loss: 79.5044, Train Acc: 0.5099, Val Loss: 17.1678, Val Acc: 0.5081\n",
      "Epoch 298/300, Train Loss: 86.3191, Train Acc: 0.4697, Val Loss: 17.5939, Val Acc: 0.4942\n",
      "Epoch 299/300, Train Loss: 74.4948, Train Acc: 0.5353, Val Loss: 17.3803, Val Acc: 0.5081\n",
      "Epoch 300/300, Train Loss: 72.9225, Train Acc: 0.5611, Val Loss: 18.0484, Val Acc: 0.5104\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"runs/torch-lstm/emotion_classifier_selected_landmarks\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_batch = y_batch.argmax(dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = y_batch.argmax(dim=1)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Train\", train_acc, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Validation\", val_acc, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0418, Test Accuracy: 0.5278\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        y_batch = y_batch.argmax(dim=1)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "test_loss /= len(y_test)\n",
    "test_acc = correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
