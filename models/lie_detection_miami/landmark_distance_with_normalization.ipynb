{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "**********\n",
      "_CUDA version: \n",
      "CUDA version:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Oct_30_01:18:48_Pacific_Daylight_Time_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n",
      "\n",
      "**********\n",
      "CUDNN version: 90100\n",
      "Available GPU devices: 1\n",
      "Device Name: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "from utils import get_cuda_info\n",
    "\n",
    "get_cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybór optymalnych punktów charakterystycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "LANDMARK_INDEXES = np.load(os.path.join('..', '..', 'data', 'landmarks', 'combined_selected_points.npy'))\n",
    "REFERENCE_POINT_IDX = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja wybranych punktów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import visualize_landmarks\n",
    "\n",
    "visualize_landmarks(LANDMARK_INDEXES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zdobycie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, convert_landmarks_to_distances\n",
    "\n",
    "all_data, all_labels = load_data('miami_deception')\n",
    "all_data = convert_landmarks_to_distances(all_data, LANDMARK_INDEXES, REFERENCE_POINT_IDX, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess_data\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(all_data, all_labels, binarize_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([224, 1679, 154])\n",
      "torch.Size([224])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W podejściu wykorzystane zostaną 2 modele - pierwszy z nich będzie siecią konwolucyjną 2d, która będzie miała za zadanie nauczyć się rozpoznawać cechy charakterystyczne dla wybranej klatki (zbioru współrzędnych pkt charakterystycznych). Do klasyfikacji szeregu czasowego zostanie wykorzystana sekwencyjna sieć neuronowa LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbudowanie modelu ekstrakcji cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LieClassifier(nn.Module):\n",
    "    def __init__(self, input_distances):\n",
    "        super(LieClassifier, self).__init__()\n",
    "        \n",
    "        # Spatial feature extraction (changed in_channels to 1)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Calculate LSTM input dimension\n",
    "        self.conv_output_size = 64 * (input_distances // 4)  # After two poolings\n",
    "        \n",
    "        # Temporal feature extraction\n",
    "        self.lstm = nn.LSTM(input_size=self.conv_output_size, hidden_size=128,\n",
    "                          batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc1 = nn.Linear(256, 64)  # 128*2 for bidirectional\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, frames, distances)\n",
    "        batch_size, frames, distances = x.shape\n",
    "        \n",
    "        # Reshape for Conv1D: (batch*frames, 1, distances)\n",
    "        x = x.view(-1, 1, distances)\n",
    "        \n",
    "        # Spatial features\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        x = x.view(batch_size, frames, -1)\n",
    "        \n",
    "        # Temporal features\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = x[:, -1, :]  # Last timestep\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = LieClassifier(input_distances=len(LANDMARK_INDEXES))\n",
    "pos_weight = torch.tensor([2.0]).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostyka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Debug Mode ===\n",
      "Input shape: torch.Size([32, 1679, 154])\n",
      "Label distribution: 0.56 (1s)\n",
      "\n",
      "Step 0:\n",
      "Loss: 0.9612\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6574 0.6504 0.6455 0.6534 0.6537]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Param conv1.weight: shape (32, 1, 3) | grad norm: 0.000000\n",
      "Param conv1.bias: shape (32,) | grad norm: 0.021902\n",
      "Param conv2.weight: shape (64, 32, 3) | grad norm: 0.028837\n",
      "Param conv2.bias: shape (64,) | grad norm: 0.029633\n",
      "Param lstm.weight_ih_l0: shape (512, 2432) | grad norm: 0.174707\n",
      "Param lstm.weight_hh_l0: shape (512, 128) | grad norm: 0.047296\n",
      "Param lstm.bias_ih_l0: shape (512,) | grad norm: 0.023743\n",
      "Param lstm.bias_hh_l0: shape (512,) | grad norm: 0.023743\n",
      "Param lstm.weight_ih_l0_reverse: shape (512, 2432) | grad norm: 0.082762\n",
      "Param lstm.weight_hh_l0_reverse: shape (512, 128) | grad norm: 0.000000\n",
      "Param lstm.bias_ih_l0_reverse: shape (512,) | grad norm: 0.011247\n",
      "Param lstm.bias_hh_l0_reverse: shape (512,) | grad norm: 0.011247\n",
      "Param fc1.weight: shape (64, 256) | grad norm: 0.098542\n",
      "Param fc1.bias: shape (64,) | grad norm: 0.045717\n",
      "Param fc2.weight: shape (1, 64) | grad norm: 0.062332\n",
      "Param fc2.bias: shape (1,) | grad norm: 0.034644\n",
      "Total gradient norm: 0.6963\n",
      "\n",
      "Step 2:\n",
      "Loss: 0.9729\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6065 0.6143 0.6688 0.633  0.6459]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 2.0669\n",
      "\n",
      "Step 4:\n",
      "Loss: 0.9649\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.5985 0.6586 0.6424 0.6256 0.6464]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 2.9997\n",
      "\n",
      "Step 6:\n",
      "Loss: 0.9609\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6491 0.6302 0.6305 0.6402 0.6491]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 4.2470\n",
      "\n",
      "Step 8:\n",
      "Loss: 0.9543\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6162 0.6515 0.6216 0.6549 0.6339]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 5.4964\n",
      "\n",
      "Step 10:\n",
      "Loss: 0.9598\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6637 0.6563 0.6116 0.6374 0.59  ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 6.5387\n",
      "\n",
      "Step 12:\n",
      "Loss: 0.9672\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6467 0.6122 0.6165 0.6428 0.6295]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 7.7555\n",
      "\n",
      "Step 14:\n",
      "Loss: 0.9434\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6248 0.613  0.6276 0.6049 0.6151]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 8.8482\n",
      "\n",
      "Step 16:\n",
      "Loss: 0.9495\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6235 0.6437 0.6324 0.6348 0.627 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 9.6916\n",
      "\n",
      "Step 18:\n",
      "Loss: 0.9685\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6351 0.6045 0.6172 0.6311 0.6363]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 11.3221\n",
      "\n",
      "Step 20:\n",
      "Loss: 0.9630\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6371 0.6707 0.63   0.6511 0.6158]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 12.1444\n",
      "\n",
      "Step 22:\n",
      "Loss: 0.9571\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6489 0.6356 0.6353 0.5952 0.6274]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 13.5553\n",
      "\n",
      "Step 24:\n",
      "Loss: 0.9689\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.665  0.6231 0.6173 0.637  0.6307]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 14.7752\n",
      "\n",
      "Step 26:\n",
      "Loss: 0.9720\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6478 0.6496 0.6447 0.6331 0.6263]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 15.9586\n",
      "\n",
      "Step 28:\n",
      "Loss: 0.9558\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6071 0.6402 0.6167 0.6064 0.6257]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 17.3598\n",
      "\n",
      "Step 30:\n",
      "Loss: 0.9380\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6295 0.6146 0.6402 0.6289 0.6747]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 18.4238\n",
      "\n",
      "Step 32:\n",
      "Loss: 0.9597\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6439 0.6199 0.5983 0.6342 0.6293]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 19.5933\n",
      "\n",
      "Step 34:\n",
      "Loss: 0.9527\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6394 0.6375 0.5992 0.6222 0.6448]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 20.8165\n",
      "\n",
      "Step 36:\n",
      "Loss: 0.9334\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6367 0.6296 0.636  0.6079 0.6345]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 21.9244\n",
      "\n",
      "Step 38:\n",
      "Loss: 0.9362\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6408 0.6136 0.6268 0.6437 0.6567]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 23.0297\n",
      "\n",
      "Step 40:\n",
      "Loss: 0.9663\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6269 0.6578 0.624  0.6027 0.6579]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 24.3397\n",
      "\n",
      "Step 42:\n",
      "Loss: 0.9444\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6444 0.6427 0.6496 0.5905 0.607 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 25.5607\n",
      "\n",
      "Step 44:\n",
      "Loss: 0.9547\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6049 0.6385 0.595  0.6262 0.6498]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 26.6771\n",
      "\n",
      "Step 46:\n",
      "Loss: 0.9653\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6093 0.6137 0.6206 0.6151 0.5977]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 27.6564\n",
      "\n",
      "Step 48:\n",
      "Loss: 0.9442\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.616  0.6359 0.6205 0.6404 0.6283]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 28.5495\n",
      "\n",
      "Step 50:\n",
      "Loss: 0.9691\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6212 0.6259 0.6329 0.6259 0.6188]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 29.9332\n",
      "\n",
      "Step 52:\n",
      "Loss: 0.9578\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6103 0.6333 0.6371 0.6322 0.6246]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 31.3212\n",
      "\n",
      "Step 54:\n",
      "Loss: 0.9621\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6006 0.5974 0.6321 0.6366 0.6502]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 32.6236\n",
      "\n",
      "Step 56:\n",
      "Loss: 0.9688\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6501 0.6289 0.6437 0.6129 0.6204]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 33.4355\n",
      "\n",
      "Step 58:\n",
      "Loss: 0.9547\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6465 0.6252 0.618  0.6386 0.6368]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 34.9184\n",
      "\n",
      "Step 60:\n",
      "Loss: 0.9487\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6168 0.6097 0.6185 0.633  0.6326]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 36.0833\n",
      "\n",
      "Step 62:\n",
      "Loss: 0.9635\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6227 0.6232 0.6457 0.6289 0.638 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 36.9123\n",
      "\n",
      "Step 64:\n",
      "Loss: 0.9525\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6076 0.6367 0.6264 0.6432 0.6118]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 38.0160\n",
      "\n",
      "Step 66:\n",
      "Loss: 0.9504\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6393 0.6082 0.6127 0.602  0.6323]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 38.9659\n",
      "\n",
      "Step 68:\n",
      "Loss: 0.9444\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6126 0.6151 0.6423 0.6233 0.6479]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 39.8755\n",
      "\n",
      "Step 70:\n",
      "Loss: 0.9371\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6533 0.6308 0.6164 0.6288 0.6408]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 41.2906\n",
      "\n",
      "Step 72:\n",
      "Loss: 0.9593\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6238 0.6142 0.6431 0.6419 0.615 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 42.4088\n",
      "\n",
      "Step 74:\n",
      "Loss: 0.9487\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6297 0.6296 0.6248 0.6361 0.6411]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 43.1260\n",
      "\n",
      "Step 76:\n",
      "Loss: 0.9400\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6341 0.6228 0.6125 0.6508 0.5978]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 44.1266\n",
      "\n",
      "Step 78:\n",
      "Loss: 0.9592\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.646  0.6349 0.6296 0.6502 0.6428]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 45.1877\n",
      "\n",
      "Step 80:\n",
      "Loss: 0.9824\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6176 0.6433 0.6543 0.64   0.6215]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 46.5368\n",
      "\n",
      "Step 82:\n",
      "Loss: 0.9451\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6448 0.632  0.6249 0.6126 0.6261]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 47.5747\n",
      "\n",
      "Step 84:\n",
      "Loss: 0.9637\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6539 0.6489 0.6194 0.6287 0.6076]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 48.7067\n",
      "\n",
      "Step 86:\n",
      "Loss: 0.9709\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6576 0.6555 0.6517 0.6321 0.6039]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 50.2168\n",
      "\n",
      "Step 88:\n",
      "Loss: 0.9517\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6202 0.606  0.6221 0.637  0.6165]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 51.4056\n",
      "\n",
      "Step 90:\n",
      "Loss: 0.9553\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6266 0.5966 0.6464 0.5982 0.6389]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 52.4070\n",
      "\n",
      "Step 92:\n",
      "Loss: 0.9508\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6215 0.622  0.616  0.6371 0.6171]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 53.6730\n",
      "\n",
      "Step 94:\n",
      "Loss: 0.9604\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6317 0.6215 0.6232 0.5934 0.6047]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 54.7202\n",
      "\n",
      "Step 96:\n",
      "Loss: 0.9532\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6393 0.6077 0.6409 0.6053 0.6392]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 56.0632\n",
      "\n",
      "Step 98:\n",
      "Loss: 0.9711\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6348 0.6376 0.5961 0.6083 0.6135]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 57.2926\n",
      "\n",
      "Step 100:\n",
      "Loss: 0.9481\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6379 0.6074 0.6041 0.6556 0.6207]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 58.3204\n",
      "\n",
      "Step 102:\n",
      "Loss: 0.9599\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.5999 0.6415 0.6445 0.6224 0.6167]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 59.3632\n",
      "\n",
      "Step 104:\n",
      "Loss: 0.9579\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6297 0.6323 0.6158 0.6128 0.6426]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 60.3245\n",
      "\n",
      "Step 106:\n",
      "Loss: 0.9448\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6449 0.6211 0.6158 0.6172 0.661 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 61.7282\n",
      "\n",
      "Step 108:\n",
      "Loss: 0.9573\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6052 0.6262 0.6056 0.5972 0.6475]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 62.8834\n",
      "\n",
      "Step 110:\n",
      "Loss: 0.9507\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.5913 0.5932 0.6336 0.6248 0.6131]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 64.4558\n",
      "\n",
      "Step 112:\n",
      "Loss: 0.9595\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6495 0.65   0.5971 0.6537 0.6362]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 66.0556\n",
      "\n",
      "Step 114:\n",
      "Loss: 0.9637\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6249 0.6526 0.625  0.6423 0.6239]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 67.1455\n",
      "\n",
      "Step 116:\n",
      "Loss: 0.9565\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6254 0.6418 0.625  0.6445 0.6278]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 68.2538\n",
      "\n",
      "Step 118:\n",
      "Loss: 0.9691\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.627  0.6239 0.6221 0.599  0.6242]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 69.5201\n",
      "\n",
      "Step 120:\n",
      "Loss: 0.9627\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6478 0.642  0.633  0.6315 0.6158]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 70.7347\n",
      "\n",
      "Step 122:\n",
      "Loss: 0.9531\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6382 0.662  0.63   0.6559 0.604 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 71.8101\n",
      "\n",
      "Step 124:\n",
      "Loss: 0.9545\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.63   0.6423 0.6262 0.6309 0.6507]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 72.9944\n",
      "\n",
      "Step 126:\n",
      "Loss: 0.9502\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6146 0.6033 0.6413 0.6439 0.6259]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 74.3087\n",
      "\n",
      "Step 128:\n",
      "Loss: 0.9576\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6184 0.6373 0.6137 0.6278 0.6494]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 75.6468\n",
      "\n",
      "Step 130:\n",
      "Loss: 0.9792\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.602  0.6372 0.6403 0.6353 0.6279]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 76.9348\n",
      "\n",
      "Step 132:\n",
      "Loss: 0.9529\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6335 0.6278 0.627  0.6174 0.604 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 77.8665\n",
      "\n",
      "Step 134:\n",
      "Loss: 0.9634\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6547 0.6211 0.6455 0.6326 0.6159]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 78.8771\n",
      "\n",
      "Step 136:\n",
      "Loss: 0.9484\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6221 0.5985 0.6233 0.6141 0.6308]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 80.0902\n",
      "\n",
      "Step 138:\n",
      "Loss: 0.9458\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6551 0.6312 0.6346 0.6119 0.6078]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 80.9753\n",
      "\n",
      "Step 140:\n",
      "Loss: 0.9612\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6288 0.6641 0.6192 0.6323 0.6181]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 81.9813\n",
      "\n",
      "Step 142:\n",
      "Loss: 0.9407\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.662  0.6083 0.608  0.6403 0.655 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 82.9804\n",
      "\n",
      "Step 144:\n",
      "Loss: 0.9608\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6032 0.5944 0.6653 0.6098 0.6246]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 84.2842\n",
      "\n",
      "Step 146:\n",
      "Loss: 0.9641\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6369 0.5972 0.6359 0.6448 0.631 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 85.5799\n",
      "\n",
      "Step 148:\n",
      "Loss: 0.9565\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6542 0.6057 0.6184 0.6087 0.6414]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 86.9823\n",
      "\n",
      "Step 150:\n",
      "Loss: 0.9535\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6536 0.6079 0.6416 0.5974 0.6351]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 88.5513\n",
      "\n",
      "Step 152:\n",
      "Loss: 0.9588\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.5998 0.6228 0.6095 0.6389 0.6265]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 89.7815\n",
      "\n",
      "Step 154:\n",
      "Loss: 0.9571\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.5875 0.5976 0.6208 0.6255 0.645 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 91.2325\n",
      "\n",
      "Step 156:\n",
      "Loss: 0.9748\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6198 0.6366 0.6407 0.6479 0.6196]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 92.3419\n",
      "\n",
      "Step 158:\n",
      "Loss: 0.9716\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6028 0.626  0.6679 0.6123 0.6371]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 93.4365\n",
      "\n",
      "Step 160:\n",
      "Loss: 0.9578\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6249 0.6331 0.6129 0.6254 0.5995]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 94.6400\n",
      "\n",
      "Step 162:\n",
      "Loss: 0.9593\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6278 0.6318 0.6421 0.6274 0.5996]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 95.7924\n",
      "\n",
      "Step 164:\n",
      "Loss: 0.9695\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.623  0.6434 0.643  0.5988 0.6232]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 96.9994\n",
      "\n",
      "Step 166:\n",
      "Loss: 0.9613\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6077 0.655  0.6508 0.6518 0.6264]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 98.0026\n",
      "\n",
      "Step 168:\n",
      "Loss: 0.9507\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6424 0.6062 0.622  0.6181 0.6173]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 99.3116\n",
      "\n",
      "Step 170:\n",
      "Loss: 0.9561\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6151 0.6337 0.6351 0.623  0.6333]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 100.4493\n",
      "\n",
      "Step 172:\n",
      "Loss: 0.9441\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.635  0.6154 0.6224 0.6264 0.6307]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 101.6590\n",
      "\n",
      "Step 174:\n",
      "Loss: 0.9456\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6212 0.6331 0.6346 0.6547 0.6277]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 102.5926\n",
      "\n",
      "Step 176:\n",
      "Loss: 0.9458\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6011 0.6015 0.6319 0.6396 0.6676]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 103.6862\n",
      "\n",
      "Step 178:\n",
      "Loss: 0.9541\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6394 0.6351 0.6514 0.6228 0.6402]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 104.9308\n",
      "\n",
      "Step 180:\n",
      "Loss: 0.9605\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.635  0.6462 0.6539 0.6125 0.6345]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 106.0893\n",
      "\n",
      "Step 182:\n",
      "Loss: 0.9526\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6405 0.6505 0.6043 0.6329 0.6262]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 107.5665\n",
      "\n",
      "Step 184:\n",
      "Loss: 0.9463\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6112 0.6136 0.6215 0.597  0.6309]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 108.5444\n",
      "\n",
      "Step 186:\n",
      "Loss: 0.9786\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.595  0.6295 0.6629 0.6162 0.5832]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 110.0080\n",
      "\n",
      "Step 188:\n",
      "Loss: 0.9600\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6536 0.6528 0.6547 0.6141 0.6383]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 111.2964\n",
      "\n",
      "Step 190:\n",
      "Loss: 0.9547\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6469 0.613  0.6231 0.6558 0.6409]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 112.2809\n",
      "\n",
      "Step 192:\n",
      "Loss: 0.9654\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6494 0.6454 0.6424 0.6348 0.6119]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 113.3890\n",
      "\n",
      "Step 194:\n",
      "Loss: 0.9768\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6104 0.6179 0.628  0.6344 0.6471]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 114.6192\n",
      "\n",
      "Step 196:\n",
      "Loss: 0.9596\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6256 0.6454 0.613  0.6258 0.6407]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 115.7397\n",
      "\n",
      "Step 198:\n",
      "Loss: 0.9611\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6524 0.5978 0.6049 0.6201 0.6511]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 116.9404\n",
      "\n",
      "Step 200:\n",
      "Loss: 0.9603\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6237 0.6409 0.621  0.6142 0.6466]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 118.0227\n",
      "\n",
      "Step 202:\n",
      "Loss: 0.9552\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6245 0.6473 0.6405 0.6291 0.6461]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 119.1229\n",
      "\n",
      "Step 204:\n",
      "Loss: 0.9440\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6556 0.6227 0.6316 0.6497 0.6628]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 120.3154\n",
      "\n",
      "Step 206:\n",
      "Loss: 0.9554\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6348 0.6472 0.6359 0.621  0.6001]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 121.7895\n",
      "\n",
      "Step 208:\n",
      "Loss: 0.9590\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6352 0.6355 0.6161 0.6318 0.6446]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 122.7259\n",
      "\n",
      "Step 210:\n",
      "Loss: 0.9581\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6056 0.63   0.6383 0.6105 0.6081]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 123.7896\n",
      "\n",
      "Step 212:\n",
      "Loss: 0.9691\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6313 0.6561 0.6601 0.6107 0.6321]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 124.9568\n",
      "\n",
      "Step 214:\n",
      "Loss: 0.9497\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6121 0.6387 0.6392 0.6218 0.659 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 126.0461\n",
      "\n",
      "Step 216:\n",
      "Loss: 0.9546\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6212 0.6183 0.6299 0.6362 0.6299]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 127.2432\n",
      "\n",
      "Step 218:\n",
      "Loss: 0.9486\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.591  0.6298 0.648  0.6039 0.6412]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 127.9828\n",
      "\n",
      "Step 220:\n",
      "Loss: 0.9402\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6397 0.5981 0.6299 0.6153 0.648 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 128.9040\n",
      "\n",
      "Step 222:\n",
      "Loss: 0.9608\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.636  0.6216 0.604  0.6473 0.6448]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 129.8616\n",
      "\n",
      "Step 224:\n",
      "Loss: 0.9475\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.624  0.642  0.6204 0.6134 0.652 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 130.9396\n",
      "\n",
      "Step 226:\n",
      "Loss: 0.9779\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.5997 0.614  0.6327 0.6654 0.632 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 132.4130\n",
      "\n",
      "Step 228:\n",
      "Loss: 0.9620\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6285 0.6309 0.6501 0.6258 0.6241]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 133.3240\n",
      "\n",
      "Step 230:\n",
      "Loss: 0.9541\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6588 0.6223 0.616  0.6531 0.6064]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 134.3952\n",
      "\n",
      "Step 232:\n",
      "Loss: 0.9539\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6259 0.6316 0.6248 0.641  0.6489]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 135.3861\n",
      "\n",
      "Step 234:\n",
      "Loss: 0.9805\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.5917 0.6379 0.6592 0.6436 0.6303]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 136.5562\n",
      "\n",
      "Step 236:\n",
      "Loss: 0.9497\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6286 0.6337 0.6024 0.6195 0.599 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 137.7432\n",
      "\n",
      "Step 238:\n",
      "Loss: 0.9455\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6534 0.6419 0.6066 0.6105 0.6511]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 138.8956\n",
      "\n",
      "Step 240:\n",
      "Loss: 0.9557\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6174 0.6119 0.6232 0.6349 0.6404]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 140.0513\n",
      "\n",
      "Step 242:\n",
      "Loss: 0.9409\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6023 0.5936 0.646  0.6429 0.6237]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 141.2079\n",
      "\n",
      "Step 244:\n",
      "Loss: 0.9518\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6268 0.6265 0.5879 0.6098 0.6426]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 142.3191\n",
      "\n",
      "Step 246:\n",
      "Loss: 0.9535\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6555 0.6321 0.6457 0.6424 0.6093]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 143.3570\n",
      "\n",
      "Step 248:\n",
      "Loss: 0.9540\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6525 0.6271 0.638  0.6415 0.6262]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 144.4455\n",
      "\n",
      "Step 250:\n",
      "Loss: 0.9705\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6499 0.6298 0.655  0.6315 0.6253]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 145.5087\n",
      "\n",
      "Step 252:\n",
      "Loss: 0.9575\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6141 0.612  0.6202 0.6185 0.5969]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 146.6411\n",
      "\n",
      "Step 254:\n",
      "Loss: 0.9607\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6029 0.6391 0.6082 0.6424 0.6506]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 147.6170\n",
      "\n",
      "Step 256:\n",
      "Loss: 0.9491\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6397 0.6084 0.6524 0.6545 0.6492]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 148.6518\n",
      "\n",
      "Step 258:\n",
      "Loss: 0.9516\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6462 0.5971 0.6371 0.6248 0.6182]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 149.9962\n",
      "\n",
      "Step 260:\n",
      "Loss: 0.9639\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6333 0.6212 0.6385 0.6469 0.6582]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 151.2232\n",
      "\n",
      "Step 262:\n",
      "Loss: 0.9616\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6081 0.6519 0.6364 0.6026 0.6143]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 152.8522\n",
      "\n",
      "Step 264:\n",
      "Loss: 0.9740\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6559 0.6225 0.6172 0.6526 0.6175]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 154.2140\n",
      "\n",
      "Step 266:\n",
      "Loss: 0.9518\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6359 0.6274 0.645  0.6237 0.6227]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 155.2095\n",
      "\n",
      "Step 268:\n",
      "Loss: 0.9681\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6198 0.6451 0.6538 0.6058 0.6124]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 156.6880\n",
      "\n",
      "Step 270:\n",
      "Loss: 0.9518\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6309 0.6212 0.6365 0.6314 0.6215]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 157.8416\n",
      "\n",
      "Step 272:\n",
      "Loss: 0.9478\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6576 0.6116 0.6478 0.6127 0.627 ]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 159.1129\n",
      "\n",
      "Step 274:\n",
      "Loss: 0.9611\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6307 0.6471 0.6417 0.6453 0.6246]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 159.6977\n",
      "\n",
      "Step 276:\n",
      "Loss: 0.9688\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6393 0.6438 0.652  0.6252 0.6275]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 160.8081\n",
      "\n",
      "Step 278:\n",
      "Loss: 0.9566\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6226 0.627  0.657  0.6251 0.6296]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 162.2268\n",
      "\n",
      "Step 280:\n",
      "Loss: 0.9539\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6348 0.6512 0.6572 0.6366 0.6181]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 163.3328\n",
      "\n",
      "Step 282:\n",
      "Loss: 0.9714\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.649  0.6096 0.6315 0.6167 0.6371]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 164.8276\n",
      "\n",
      "Step 284:\n",
      "Loss: 0.9545\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6372 0.6013 0.6505 0.6212 0.6523]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 166.0365\n",
      "\n",
      "Step 286:\n",
      "Loss: 0.9626\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6561 0.6097 0.6438 0.6412 0.6484]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 166.9724\n",
      "\n",
      "Step 288:\n",
      "Loss: 0.9625\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6164 0.624  0.6149 0.6445 0.6472]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 167.7821\n",
      "\n",
      "Step 290:\n",
      "Loss: 0.9534\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.638  0.6148 0.6594 0.6514 0.6359]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 169.2310\n",
      "\n",
      "Step 292:\n",
      "Loss: 0.9476\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6251 0.6209 0.5956 0.6231 0.6527]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 170.3001\n",
      "\n",
      "Step 294:\n",
      "Loss: 0.9456\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6653 0.6104 0.6271 0.6165 0.6664]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 171.3510\n",
      "\n",
      "Step 296:\n",
      "Loss: 0.9616\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6425 0.6524 0.6379 0.6519 0.6479]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 172.4475\n",
      "\n",
      "Step 298:\n",
      "Loss: 0.9615\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6322 0.6502 0.626  0.6124 0.6323]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 173.6571\n",
      "\n",
      "Step 300:\n",
      "Loss: 0.9510\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.6306 0.6276 0.6473 0.6544 0.6403]\n",
      "Labels (5 samples): [1. 0. 0. 0. 1.]\n",
      "Total gradient norm: 174.8679\n",
      "\n",
      "Step 302:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m criterion_diag \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(pos_weight\u001b[38;5;241m=\u001b[39mpos_weight_diag)\n\u001b[0;32m      6\u001b[0m optimizer_diag \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43moverfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_diag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_diag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_diag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_treshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bartosz\\Desktop\\automatic-lie-detection\\models\\utils\\model_functions\\train_model.py:182\u001b[0m, in \u001b[0;36moverfit_model\u001b[1;34m(model, criterion, optimizer, X_train, y_train, prediction_treshold)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m999\u001b[39m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions (5 samples): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobs[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.model_functions import overfit_model\n",
    "\n",
    "model_diag = LieClassifier(input_distances=len(LANDMARK_INDEXES))\n",
    "pos_weight_diag = torch.tensor([2.0]).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "criterion_diag = nn.BCEWithLogitsLoss(pos_weight=pos_weight_diag)\n",
    "optimizer_diag = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "overfit_model(model_diag, criterion_diag, optimizer_diag, X_train, y_train, prediction_treshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "RUNS_FOLDER_PATH = os.path.abspath('runs')\n",
    "writer_path = os.path.join('runs', 'torch_lstm', 'lie_classifier_landmark_distance_normalized')\n",
    "writer = SummaryWriter(writer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction stats: Min=0.599, Max=0.655, Mean=0.629\n",
      "Prediction stats: Min=0.601, Max=0.665, Mean=0.636\n",
      "Prediction stats: Min=0.598, Max=0.662, Mean=0.639\n",
      "Prediction stats: Min=0.617, Max=0.675, Mean=0.644\n",
      "Prediction stats: Min=0.607, Max=0.666, Mean=0.645\n",
      "Prediction stats: Min=0.613, Max=0.681, Mean=0.647\n",
      "Prediction stats: Min=0.637, Max=0.681, Mean=0.659\n",
      "Epoch 1/300, Train Loss: 6.67501765, Train Acc: 0.47321429, Val Loss: 1.96382093, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.627, Max=0.695, Mean=0.659\n",
      "Prediction stats: Min=0.623, Max=0.689, Mean=0.662\n",
      "Prediction stats: Min=0.618, Max=0.694, Mean=0.664\n",
      "Prediction stats: Min=0.625, Max=0.687, Mean=0.661\n",
      "Prediction stats: Min=0.643, Max=0.697, Mean=0.665\n",
      "Prediction stats: Min=0.626, Max=0.698, Mean=0.666\n",
      "Prediction stats: Min=0.631, Max=0.708, Mean=0.671\n",
      "Epoch 2/300, Train Loss: 6.60921568, Train Acc: 0.47767857, Val Loss: 1.98519129, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.647, Max=0.696, Mean=0.674\n",
      "Prediction stats: Min=0.647, Max=0.700, Mean=0.678\n",
      "Prediction stats: Min=0.650, Max=0.705, Mean=0.683\n",
      "Prediction stats: Min=0.629, Max=0.708, Mean=0.679\n",
      "Prediction stats: Min=0.646, Max=0.708, Mean=0.684\n",
      "Prediction stats: Min=0.660, Max=0.714, Mean=0.686\n",
      "Prediction stats: Min=0.642, Max=0.713, Mean=0.688\n",
      "Epoch 3/300, Train Loss: 6.63737857, Train Acc: 0.50446429, Val Loss: 2.00533038, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.641, Max=0.712, Mean=0.688\n",
      "Prediction stats: Min=0.658, Max=0.713, Mean=0.688\n",
      "Prediction stats: Min=0.634, Max=0.717, Mean=0.687\n",
      "Prediction stats: Min=0.646, Max=0.713, Mean=0.687\n",
      "Prediction stats: Min=0.651, Max=0.719, Mean=0.686\n",
      "Prediction stats: Min=0.634, Max=0.718, Mean=0.688\n",
      "Prediction stats: Min=0.659, Max=0.716, Mean=0.692\n",
      "Epoch 4/300, Train Loss: 6.61613154, Train Acc: 0.46428571, Val Loss: 2.01308632, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.670, Max=0.713, Mean=0.693\n",
      "Prediction stats: Min=0.632, Max=0.711, Mean=0.686\n",
      "Prediction stats: Min=0.640, Max=0.713, Mean=0.691\n",
      "Prediction stats: Min=0.654, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.636, Max=0.714, Mean=0.686\n",
      "Prediction stats: Min=0.648, Max=0.716, Mean=0.690\n",
      "Prediction stats: Min=0.645, Max=0.716, Mean=0.694\n",
      "Epoch 5/300, Train Loss: 6.62762266, Train Acc: 0.46428571, Val Loss: 2.01428765, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.653, Max=0.719, Mean=0.697\n",
      "Prediction stats: Min=0.638, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.642, Max=0.720, Mean=0.692\n",
      "Prediction stats: Min=0.658, Max=0.713, Mean=0.688\n",
      "Prediction stats: Min=0.664, Max=0.720, Mean=0.696\n",
      "Prediction stats: Min=0.651, Max=0.720, Mean=0.691\n",
      "Prediction stats: Min=0.646, Max=0.713, Mean=0.691\n",
      "Epoch 6/300, Train Loss: 6.57732785, Train Acc: 0.53125000, Val Loss: 2.01860553, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.657, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.670, Max=0.716, Mean=0.696\n",
      "Prediction stats: Min=0.641, Max=0.719, Mean=0.692\n",
      "Prediction stats: Min=0.621, Max=0.720, Mean=0.689\n",
      "Prediction stats: Min=0.648, Max=0.722, Mean=0.695\n",
      "Prediction stats: Min=0.657, Max=0.709, Mean=0.685\n",
      "Prediction stats: Min=0.644, Max=0.721, Mean=0.689\n",
      "Epoch 7/300, Train Loss: 6.56635463, Train Acc: 0.56250000, Val Loss: 2.01831782, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.648, Max=0.717, Mean=0.692\n",
      "Prediction stats: Min=0.658, Max=0.723, Mean=0.698\n",
      "Prediction stats: Min=0.641, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.645, Max=0.720, Mean=0.694\n",
      "Prediction stats: Min=0.623, Max=0.716, Mean=0.694\n",
      "Prediction stats: Min=0.616, Max=0.719, Mean=0.693\n",
      "Prediction stats: Min=0.656, Max=0.719, Mean=0.696\n",
      "Epoch 8/300, Train Loss: 6.66224873, Train Acc: 0.45535714, Val Loss: 2.02056503, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.627, Max=0.721, Mean=0.697\n",
      "Prediction stats: Min=0.658, Max=0.722, Mean=0.696\n",
      "Prediction stats: Min=0.662, Max=0.720, Mean=0.697\n",
      "Prediction stats: Min=0.656, Max=0.721, Mean=0.699\n",
      "Prediction stats: Min=0.663, Max=0.721, Mean=0.695\n",
      "Prediction stats: Min=0.646, Max=0.718, Mean=0.690\n",
      "Prediction stats: Min=0.622, Max=0.715, Mean=0.694\n",
      "Epoch 9/300, Train Loss: 6.65522069, Train Acc: 0.46428571, Val Loss: 2.01963598, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.630, Max=0.721, Mean=0.692\n",
      "Prediction stats: Min=0.665, Max=0.721, Mean=0.697\n",
      "Prediction stats: Min=0.643, Max=0.721, Mean=0.695\n",
      "Prediction stats: Min=0.631, Max=0.720, Mean=0.692\n",
      "Prediction stats: Min=0.646, Max=0.721, Mean=0.695\n",
      "Prediction stats: Min=0.639, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.614, Max=0.722, Mean=0.695\n",
      "Epoch 10/300, Train Loss: 6.61678320, Train Acc: 0.49553571, Val Loss: 2.02324665, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.651, Max=0.721, Mean=0.698\n",
      "Prediction stats: Min=0.655, Max=0.718, Mean=0.693\n",
      "Prediction stats: Min=0.652, Max=0.719, Mean=0.696\n",
      "Prediction stats: Min=0.659, Max=0.719, Mean=0.696\n",
      "Prediction stats: Min=0.626, Max=0.718, Mean=0.685\n",
      "Prediction stats: Min=0.651, Max=0.722, Mean=0.694\n",
      "Prediction stats: Min=0.652, Max=0.717, Mean=0.699\n",
      "Epoch 11/300, Train Loss: 6.55907506, Train Acc: 0.57589286, Val Loss: 2.02010912, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.660, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.655, Max=0.717, Mean=0.692\n",
      "Prediction stats: Min=0.640, Max=0.722, Mean=0.692\n",
      "Prediction stats: Min=0.614, Max=0.717, Mean=0.694\n",
      "Prediction stats: Min=0.633, Max=0.719, Mean=0.697\n",
      "Prediction stats: Min=0.667, Max=0.717, Mean=0.694\n",
      "Prediction stats: Min=0.649, Max=0.715, Mean=0.693\n",
      "Epoch 12/300, Train Loss: 6.55991364, Train Acc: 0.57142857, Val Loss: 2.01915610, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.659, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.653, Max=0.714, Mean=0.693\n",
      "Prediction stats: Min=0.638, Max=0.714, Mean=0.693\n",
      "Prediction stats: Min=0.645, Max=0.718, Mean=0.690\n",
      "Prediction stats: Min=0.630, Max=0.713, Mean=0.682\n",
      "Prediction stats: Min=0.618, Max=0.720, Mean=0.686\n",
      "Prediction stats: Min=0.654, Max=0.716, Mean=0.690\n",
      "Epoch 13/300, Train Loss: 6.66961718, Train Acc: 0.46428571, Val Loss: 2.01040804, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.648, Max=0.718, Mean=0.687\n",
      "Prediction stats: Min=0.654, Max=0.718, Mean=0.686\n",
      "Prediction stats: Min=0.656, Max=0.714, Mean=0.689\n",
      "Prediction stats: Min=0.647, Max=0.720, Mean=0.691\n",
      "Prediction stats: Min=0.650, Max=0.721, Mean=0.688\n",
      "Prediction stats: Min=0.623, Max=0.717, Mean=0.688\n",
      "Prediction stats: Min=0.611, Max=0.721, Mean=0.693\n",
      "Epoch 14/300, Train Loss: 6.60178256, Train Acc: 0.49107143, Val Loss: 2.01439553, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.640, Max=0.719, Mean=0.687\n",
      "Prediction stats: Min=0.654, Max=0.717, Mean=0.689\n",
      "Prediction stats: Min=0.625, Max=0.722, Mean=0.694\n",
      "Prediction stats: Min=0.628, Max=0.717, Mean=0.693\n",
      "Prediction stats: Min=0.606, Max=0.717, Mean=0.692\n",
      "Prediction stats: Min=0.616, Max=0.723, Mean=0.691\n",
      "Prediction stats: Min=0.663, Max=0.713, Mean=0.694\n",
      "Epoch 15/300, Train Loss: 6.64257395, Train Acc: 0.49107143, Val Loss: 2.01825273, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.593, Max=0.719, Mean=0.694\n",
      "Prediction stats: Min=0.647, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.646, Max=0.719, Mean=0.687\n",
      "Prediction stats: Min=0.665, Max=0.722, Mean=0.695\n",
      "Prediction stats: Min=0.642, Max=0.718, Mean=0.687\n",
      "Prediction stats: Min=0.634, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.650, Max=0.717, Mean=0.696\n",
      "Epoch 16/300, Train Loss: 6.62656182, Train Acc: 0.47767857, Val Loss: 2.01610035, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.661, Max=0.717, Mean=0.692\n",
      "Prediction stats: Min=0.640, Max=0.722, Mean=0.686\n",
      "Prediction stats: Min=0.648, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.642, Max=0.721, Mean=0.693\n",
      "Prediction stats: Min=0.632, Max=0.722, Mean=0.686\n",
      "Prediction stats: Min=0.651, Max=0.722, Mean=0.690\n",
      "Prediction stats: Min=0.640, Max=0.718, Mean=0.695\n",
      "Epoch 17/300, Train Loss: 6.64563054, Train Acc: 0.50000000, Val Loss: 2.01805711, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.660, Max=0.717, Mean=0.694\n",
      "Prediction stats: Min=0.664, Max=0.721, Mean=0.697\n",
      "Prediction stats: Min=0.631, Max=0.718, Mean=0.690\n",
      "Prediction stats: Min=0.644, Max=0.719, Mean=0.688\n",
      "Prediction stats: Min=0.646, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.607, Max=0.722, Mean=0.686\n",
      "Prediction stats: Min=0.672, Max=0.722, Mean=0.700\n",
      "Epoch 18/300, Train Loss: 6.60547882, Train Acc: 0.55803571, Val Loss: 2.01761639, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.645, Max=0.718, Mean=0.687\n",
      "Prediction stats: Min=0.640, Max=0.720, Mean=0.688\n",
      "Prediction stats: Min=0.647, Max=0.722, Mean=0.693\n",
      "Prediction stats: Min=0.647, Max=0.722, Mean=0.689\n",
      "Prediction stats: Min=0.666, Max=0.718, Mean=0.699\n",
      "Prediction stats: Min=0.627, Max=0.721, Mean=0.697\n",
      "Prediction stats: Min=0.567, Max=0.720, Mean=0.694\n",
      "Epoch 19/300, Train Loss: 6.64778113, Train Acc: 0.49553571, Val Loss: 2.02443850, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.640, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.632, Max=0.720, Mean=0.696\n",
      "Prediction stats: Min=0.641, Max=0.724, Mean=0.694\n",
      "Prediction stats: Min=0.633, Max=0.721, Mean=0.692\n",
      "Prediction stats: Min=0.658, Max=0.724, Mean=0.702\n",
      "Prediction stats: Min=0.645, Max=0.719, Mean=0.694\n",
      "Prediction stats: Min=0.638, Max=0.715, Mean=0.690\n",
      "Epoch 20/300, Train Loss: 6.64508444, Train Acc: 0.47767857, Val Loss: 2.02086127, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.635, Max=0.722, Mean=0.687\n",
      "Prediction stats: Min=0.630, Max=0.724, Mean=0.690\n",
      "Prediction stats: Min=0.635, Max=0.721, Mean=0.695\n",
      "Prediction stats: Min=0.648, Max=0.718, Mean=0.693\n",
      "Prediction stats: Min=0.637, Max=0.722, Mean=0.686\n",
      "Prediction stats: Min=0.642, Max=0.719, Mean=0.695\n",
      "Prediction stats: Min=0.670, Max=0.719, Mean=0.700\n",
      "Epoch 21/300, Train Loss: 6.60746408, Train Acc: 0.54017857, Val Loss: 2.01764154, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.664, Max=0.721, Mean=0.695\n",
      "Prediction stats: Min=0.578, Max=0.717, Mean=0.686\n",
      "Prediction stats: Min=0.640, Max=0.722, Mean=0.692\n",
      "Prediction stats: Min=0.650, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.666, Max=0.722, Mean=0.697\n",
      "Prediction stats: Min=0.621, Max=0.714, Mean=0.686\n",
      "Prediction stats: Min=0.659, Max=0.719, Mean=0.697\n",
      "Epoch 22/300, Train Loss: 6.58085424, Train Acc: 0.52232143, Val Loss: 2.02236247, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.633, Max=0.721, Mean=0.690\n",
      "Prediction stats: Min=0.644, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.619, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.640, Max=0.720, Mean=0.689\n",
      "Prediction stats: Min=0.646, Max=0.717, Mean=0.687\n",
      "Prediction stats: Min=0.633, Max=0.720, Mean=0.692\n",
      "Prediction stats: Min=0.609, Max=0.719, Mean=0.681\n",
      "Epoch 23/300, Train Loss: 6.56830746, Train Acc: 0.53125000, Val Loss: 2.01486719, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.643, Max=0.717, Mean=0.692\n",
      "Prediction stats: Min=0.659, Max=0.723, Mean=0.692\n",
      "Prediction stats: Min=0.614, Max=0.723, Mean=0.687\n",
      "Prediction stats: Min=0.622, Max=0.722, Mean=0.681\n",
      "Prediction stats: Min=0.612, Max=0.721, Mean=0.688\n",
      "Prediction stats: Min=0.595, Max=0.715, Mean=0.684\n",
      "Prediction stats: Min=0.634, Max=0.721, Mean=0.691\n",
      "Epoch 24/300, Train Loss: 6.64592403, Train Acc: 0.50000000, Val Loss: 2.01671040, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.540, Max=0.723, Mean=0.685\n",
      "Prediction stats: Min=0.625, Max=0.719, Mean=0.688\n",
      "Prediction stats: Min=0.625, Max=0.723, Mean=0.691\n",
      "Prediction stats: Min=0.615, Max=0.721, Mean=0.687\n",
      "Prediction stats: Min=0.639, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.624, Max=0.721, Mean=0.690\n",
      "Prediction stats: Min=0.637, Max=0.720, Mean=0.695\n",
      "Epoch 25/300, Train Loss: 6.67337674, Train Acc: 0.45982143, Val Loss: 2.01800317, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.639, Max=0.722, Mean=0.693\n",
      "Prediction stats: Min=0.624, Max=0.723, Mean=0.690\n",
      "Prediction stats: Min=0.648, Max=0.722, Mean=0.698\n",
      "Prediction stats: Min=0.557, Max=0.725, Mean=0.689\n",
      "Prediction stats: Min=0.653, Max=0.724, Mean=0.697\n",
      "Prediction stats: Min=0.635, Max=0.720, Mean=0.697\n",
      "Prediction stats: Min=0.629, Max=0.722, Mean=0.691\n",
      "Epoch 26/300, Train Loss: 6.61799055, Train Acc: 0.49553571, Val Loss: 2.02404672, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.651, Max=0.721, Mean=0.704\n",
      "Prediction stats: Min=0.550, Max=0.721, Mean=0.693\n",
      "Prediction stats: Min=0.664, Max=0.721, Mean=0.693\n",
      "Prediction stats: Min=0.620, Max=0.719, Mean=0.689\n",
      "Prediction stats: Min=0.647, Max=0.722, Mean=0.691\n",
      "Prediction stats: Min=0.628, Max=0.714, Mean=0.694\n",
      "Prediction stats: Min=0.663, Max=0.719, Mean=0.697\n",
      "Epoch 27/300, Train Loss: 6.63428777, Train Acc: 0.45982143, Val Loss: 2.02418852, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.521, Max=0.715, Mean=0.692\n",
      "Prediction stats: Min=0.661, Max=0.722, Mean=0.695\n",
      "Prediction stats: Min=0.652, Max=0.723, Mean=0.695\n",
      "Prediction stats: Min=0.635, Max=0.721, Mean=0.699\n",
      "Prediction stats: Min=0.623, Max=0.716, Mean=0.688\n",
      "Prediction stats: Min=0.632, Max=0.721, Mean=0.698\n",
      "Prediction stats: Min=0.658, Max=0.722, Mean=0.700\n",
      "Epoch 28/300, Train Loss: 6.65212113, Train Acc: 0.44642857, Val Loss: 2.02613819, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.654, Max=0.721, Mean=0.693\n",
      "Prediction stats: Min=0.669, Max=0.724, Mean=0.698\n",
      "Prediction stats: Min=0.657, Max=0.719, Mean=0.693\n",
      "Prediction stats: Min=0.607, Max=0.721, Mean=0.696\n",
      "Prediction stats: Min=0.668, Max=0.724, Mean=0.703\n",
      "Prediction stats: Min=0.661, Max=0.723, Mean=0.697\n",
      "Prediction stats: Min=0.660, Max=0.722, Mean=0.698\n",
      "Epoch 29/300, Train Loss: 6.60385579, Train Acc: 0.53571429, Val Loss: 2.02277821, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.658, Max=0.721, Mean=0.700\n",
      "Prediction stats: Min=0.681, Max=0.719, Mean=0.701\n",
      "Prediction stats: Min=0.645, Max=0.715, Mean=0.691\n",
      "Prediction stats: Min=0.652, Max=0.722, Mean=0.697\n",
      "Prediction stats: Min=0.536, Max=0.721, Mean=0.693\n",
      "Prediction stats: Min=0.637, Max=0.720, Mean=0.694\n",
      "Prediction stats: Min=0.647, Max=0.720, Mean=0.695\n",
      "Epoch 30/300, Train Loss: 6.59029478, Train Acc: 0.54910714, Val Loss: 2.02066886, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.651, Max=0.722, Mean=0.694\n",
      "Prediction stats: Min=0.628, Max=0.718, Mean=0.692\n",
      "Prediction stats: Min=0.640, Max=0.724, Mean=0.694\n",
      "Prediction stats: Min=0.640, Max=0.718, Mean=0.687\n",
      "Prediction stats: Min=0.644, Max=0.721, Mean=0.690\n",
      "Prediction stats: Min=0.562, Max=0.718, Mean=0.689\n",
      "Prediction stats: Min=0.641, Max=0.722, Mean=0.691\n",
      "Epoch 31/300, Train Loss: 6.66476780, Train Acc: 0.45535714, Val Loss: 2.01487660, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.643, Max=0.721, Mean=0.693\n",
      "Prediction stats: Min=0.649, Max=0.717, Mean=0.691\n",
      "Prediction stats: Min=0.647, Max=0.721, Mean=0.695\n",
      "Prediction stats: Min=0.644, Max=0.713, Mean=0.687\n",
      "Prediction stats: Min=0.638, Max=0.719, Mean=0.692\n",
      "Prediction stats: Min=0.579, Max=0.711, Mean=0.690\n",
      "Prediction stats: Min=0.630, Max=0.713, Mean=0.690\n",
      "Epoch 32/300, Train Loss: 6.60460520, Train Acc: 0.50892857, Val Loss: 2.01388872, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.642, Max=0.718, Mean=0.685\n",
      "Prediction stats: Min=0.662, Max=0.715, Mean=0.692\n",
      "Prediction stats: Min=0.549, Max=0.715, Mean=0.688\n",
      "Prediction stats: Min=0.645, Max=0.716, Mean=0.693\n",
      "Prediction stats: Min=0.649, Max=0.712, Mean=0.695\n",
      "Prediction stats: Min=0.648, Max=0.707, Mean=0.689\n",
      "Prediction stats: Min=0.643, Max=0.715, Mean=0.691\n",
      "Epoch 33/300, Train Loss: 6.58459473, Train Acc: 0.53125000, Val Loss: 2.01228410, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.635, Max=0.718, Mean=0.692\n",
      "Prediction stats: Min=0.520, Max=0.712, Mean=0.686\n",
      "Prediction stats: Min=0.622, Max=0.716, Mean=0.693\n",
      "Prediction stats: Min=0.654, Max=0.718, Mean=0.692\n",
      "Prediction stats: Min=0.626, Max=0.718, Mean=0.693\n",
      "Prediction stats: Min=0.638, Max=0.722, Mean=0.690\n",
      "Prediction stats: Min=0.652, Max=0.721, Mean=0.695\n",
      "Epoch 34/300, Train Loss: 6.63403356, Train Acc: 0.45089286, Val Loss: 2.01801598, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.640, Max=0.716, Mean=0.691\n",
      "Prediction stats: Min=0.629, Max=0.716, Mean=0.696\n",
      "Prediction stats: Min=0.669, Max=0.718, Mean=0.695\n",
      "Prediction stats: Min=0.652, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.663, Max=0.717, Mean=0.697\n",
      "Prediction stats: Min=0.545, Max=0.716, Mean=0.690\n",
      "Prediction stats: Min=0.651, Max=0.721, Mean=0.692\n",
      "Epoch 35/300, Train Loss: 6.59290391, Train Acc: 0.50446429, Val Loss: 2.01905727, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.637, Max=0.721, Mean=0.692\n",
      "Prediction stats: Min=0.636, Max=0.722, Mean=0.694\n",
      "Prediction stats: Min=0.673, Max=0.716, Mean=0.697\n",
      "Prediction stats: Min=0.658, Max=0.718, Mean=0.698\n",
      "Prediction stats: Min=0.544, Max=0.720, Mean=0.687\n",
      "Prediction stats: Min=0.647, Max=0.720, Mean=0.695\n",
      "Prediction stats: Min=0.668, Max=0.717, Mean=0.698\n",
      "Epoch 36/300, Train Loss: 6.55123061, Train Acc: 0.58035714, Val Loss: 2.01850331, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.658, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.540, Max=0.719, Mean=0.690\n",
      "Prediction stats: Min=0.649, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.659, Max=0.716, Mean=0.698\n",
      "Prediction stats: Min=0.675, Max=0.718, Mean=0.702\n",
      "Prediction stats: Min=0.649, Max=0.719, Mean=0.695\n",
      "Prediction stats: Min=0.658, Max=0.721, Mean=0.700\n",
      "Epoch 37/300, Train Loss: 6.61435330, Train Acc: 0.50000000, Val Loss: 2.01999533, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.621, Max=0.721, Mean=0.692\n",
      "Prediction stats: Min=0.673, Max=0.722, Mean=0.696\n",
      "Prediction stats: Min=0.522, Max=0.721, Mean=0.689\n",
      "Prediction stats: Min=0.640, Max=0.722, Mean=0.697\n",
      "Prediction stats: Min=0.623, Max=0.716, Mean=0.696\n",
      "Prediction stats: Min=0.660, Max=0.719, Mean=0.697\n",
      "Prediction stats: Min=0.669, Max=0.720, Mean=0.697\n",
      "Epoch 38/300, Train Loss: 6.62562436, Train Acc: 0.49553571, Val Loss: 2.02236652, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.655, Max=0.719, Mean=0.696\n",
      "Prediction stats: Min=0.652, Max=0.721, Mean=0.693\n",
      "Prediction stats: Min=0.655, Max=0.718, Mean=0.696\n",
      "Prediction stats: Min=0.654, Max=0.723, Mean=0.698\n",
      "Prediction stats: Min=0.646, Max=0.720, Mean=0.694\n",
      "Prediction stats: Min=0.515, Max=0.718, Mean=0.688\n",
      "Prediction stats: Min=0.671, Max=0.724, Mean=0.700\n",
      "Epoch 39/300, Train Loss: 6.61504894, Train Acc: 0.48214286, Val Loss: 2.02135181, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.655, Max=0.722, Mean=0.697\n",
      "Prediction stats: Min=0.627, Max=0.717, Mean=0.693\n",
      "Prediction stats: Min=0.622, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.661, Max=0.718, Mean=0.702\n",
      "Prediction stats: Min=0.512, Max=0.720, Mean=0.689\n",
      "Prediction stats: Min=0.658, Max=0.723, Mean=0.690\n",
      "Prediction stats: Min=0.672, Max=0.719, Mean=0.697\n",
      "Epoch 40/300, Train Loss: 6.58701015, Train Acc: 0.50000000, Val Loss: 2.02091604, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.665, Max=0.715, Mean=0.695\n",
      "Prediction stats: Min=0.632, Max=0.718, Mean=0.698\n",
      "Prediction stats: Min=0.651, Max=0.717, Mean=0.693\n",
      "Prediction stats: Min=0.653, Max=0.716, Mean=0.694\n",
      "Prediction stats: Min=0.644, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.643, Max=0.717, Mean=0.691\n",
      "Prediction stats: Min=0.545, Max=0.718, Mean=0.689\n",
      "Epoch 41/300, Train Loss: 6.60943770, Train Acc: 0.51339286, Val Loss: 2.01755500, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.663, Max=0.716, Mean=0.694\n",
      "Prediction stats: Min=0.521, Max=0.718, Mean=0.688\n",
      "Prediction stats: Min=0.661, Max=0.719, Mean=0.696\n",
      "Prediction stats: Min=0.660, Max=0.716, Mean=0.692\n",
      "Prediction stats: Min=0.635, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.645, Max=0.722, Mean=0.699\n",
      "Prediction stats: Min=0.670, Max=0.723, Mean=0.697\n",
      "Epoch 42/300, Train Loss: 6.64511764, Train Acc: 0.45089286, Val Loss: 2.02239209, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.661, Max=0.723, Mean=0.699\n",
      "Prediction stats: Min=0.653, Max=0.720, Mean=0.699\n",
      "Prediction stats: Min=0.538, Max=0.719, Mean=0.692\n",
      "Prediction stats: Min=0.637, Max=0.722, Mean=0.696\n",
      "Prediction stats: Min=0.646, Max=0.721, Mean=0.691\n",
      "Prediction stats: Min=0.639, Max=0.717, Mean=0.687\n",
      "Prediction stats: Min=0.647, Max=0.717, Mean=0.697\n",
      "Epoch 43/300, Train Loss: 6.66152823, Train Acc: 0.47321429, Val Loss: 2.02073371, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.656, Max=0.717, Mean=0.696\n",
      "Prediction stats: Min=0.643, Max=0.720, Mean=0.695\n",
      "Prediction stats: Min=0.633, Max=0.723, Mean=0.693\n",
      "Prediction stats: Min=0.507, Max=0.720, Mean=0.689\n",
      "Prediction stats: Min=0.642, Max=0.716, Mean=0.695\n",
      "Prediction stats: Min=0.654, Max=0.719, Mean=0.694\n",
      "Prediction stats: Min=0.648, Max=0.720, Mean=0.696\n",
      "Epoch 44/300, Train Loss: 6.60357386, Train Acc: 0.52678571, Val Loss: 2.01656389, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.644, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.645, Max=0.717, Mean=0.688\n",
      "Prediction stats: Min=0.629, Max=0.719, Mean=0.689\n",
      "Prediction stats: Min=0.524, Max=0.710, Mean=0.682\n",
      "Prediction stats: Min=0.642, Max=0.719, Mean=0.692\n",
      "Prediction stats: Min=0.624, Max=0.715, Mean=0.689\n",
      "Prediction stats: Min=0.662, Max=0.717, Mean=0.692\n",
      "Epoch 45/300, Train Loss: 6.64343601, Train Acc: 0.48214286, Val Loss: 2.01342136, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.665, Max=0.715, Mean=0.694\n",
      "Prediction stats: Min=0.543, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.656, Max=0.714, Mean=0.688\n",
      "Prediction stats: Min=0.639, Max=0.713, Mean=0.688\n",
      "Prediction stats: Min=0.654, Max=0.712, Mean=0.689\n",
      "Prediction stats: Min=0.642, Max=0.715, Mean=0.688\n",
      "Prediction stats: Min=0.672, Max=0.716, Mean=0.696\n",
      "Epoch 46/300, Train Loss: 6.62096995, Train Acc: 0.48660714, Val Loss: 2.01340508, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.640, Max=0.718, Mean=0.689\n",
      "Prediction stats: Min=0.635, Max=0.713, Mean=0.688\n",
      "Prediction stats: Min=0.636, Max=0.710, Mean=0.686\n",
      "Prediction stats: Min=0.515, Max=0.707, Mean=0.681\n",
      "Prediction stats: Min=0.648, Max=0.710, Mean=0.685\n",
      "Prediction stats: Min=0.632, Max=0.716, Mean=0.687\n",
      "Prediction stats: Min=0.639, Max=0.715, Mean=0.690\n",
      "Epoch 47/300, Train Loss: 6.58885825, Train Acc: 0.51339286, Val Loss: 2.01021999, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.654, Max=0.716, Mean=0.690\n",
      "Prediction stats: Min=0.647, Max=0.712, Mean=0.690\n",
      "Prediction stats: Min=0.639, Max=0.714, Mean=0.688\n",
      "Prediction stats: Min=0.643, Max=0.716, Mean=0.689\n",
      "Prediction stats: Min=0.655, Max=0.718, Mean=0.689\n",
      "Prediction stats: Min=0.543, Max=0.717, Mean=0.686\n",
      "Prediction stats: Min=0.642, Max=0.712, Mean=0.690\n",
      "Epoch 48/300, Train Loss: 6.57440919, Train Acc: 0.54910714, Val Loss: 2.01470840, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.523, Max=0.716, Mean=0.685\n",
      "Prediction stats: Min=0.648, Max=0.715, Mean=0.693\n",
      "Prediction stats: Min=0.637, Max=0.713, Mean=0.687\n",
      "Prediction stats: Min=0.633, Max=0.713, Mean=0.684\n",
      "Prediction stats: Min=0.661, Max=0.719, Mean=0.696\n",
      "Prediction stats: Min=0.661, Max=0.716, Mean=0.694\n",
      "Prediction stats: Min=0.631, Max=0.718, Mean=0.687\n",
      "Epoch 49/300, Train Loss: 6.62776512, Train Acc: 0.48214286, Val Loss: 2.01690412, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.643, Max=0.722, Mean=0.689\n",
      "Prediction stats: Min=0.644, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.628, Max=0.716, Mean=0.686\n",
      "Prediction stats: Min=0.516, Max=0.717, Mean=0.684\n",
      "Prediction stats: Min=0.660, Max=0.710, Mean=0.686\n",
      "Prediction stats: Min=0.658, Max=0.717, Mean=0.689\n",
      "Prediction stats: Min=0.653, Max=0.714, Mean=0.688\n",
      "Epoch 50/300, Train Loss: 6.58031911, Train Acc: 0.51785714, Val Loss: 2.01490951, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.641, Max=0.718, Mean=0.690\n",
      "Prediction stats: Min=0.646, Max=0.717, Mean=0.687\n",
      "Prediction stats: Min=0.508, Max=0.713, Mean=0.684\n",
      "Prediction stats: Min=0.646, Max=0.713, Mean=0.693\n",
      "Prediction stats: Min=0.654, Max=0.717, Mean=0.693\n",
      "Prediction stats: Min=0.656, Max=0.718, Mean=0.693\n",
      "Prediction stats: Min=0.647, Max=0.717, Mean=0.687\n",
      "Epoch 51/300, Train Loss: 6.64707857, Train Acc: 0.44642857, Val Loss: 2.01581466, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.637, Max=0.719, Mean=0.690\n",
      "Prediction stats: Min=0.624, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.666, Max=0.717, Mean=0.698\n",
      "Prediction stats: Min=0.590, Max=0.718, Mean=0.689\n",
      "Prediction stats: Min=0.651, Max=0.719, Mean=0.696\n",
      "Prediction stats: Min=0.650, Max=0.718, Mean=0.692\n",
      "Prediction stats: Min=0.658, Max=0.711, Mean=0.693\n",
      "Epoch 52/300, Train Loss: 6.60227001, Train Acc: 0.49107143, Val Loss: 2.01835132, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.661, Max=0.716, Mean=0.695\n",
      "Prediction stats: Min=0.635, Max=0.719, Mean=0.692\n",
      "Prediction stats: Min=0.511, Max=0.723, Mean=0.692\n",
      "Prediction stats: Min=0.668, Max=0.717, Mean=0.697\n",
      "Prediction stats: Min=0.651, Max=0.717, Mean=0.693\n",
      "Prediction stats: Min=0.641, Max=0.718, Mean=0.697\n",
      "Prediction stats: Min=0.652, Max=0.717, Mean=0.690\n",
      "Epoch 53/300, Train Loss: 6.57387626, Train Acc: 0.53125000, Val Loss: 2.01836950, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.664, Max=0.713, Mean=0.696\n",
      "Prediction stats: Min=0.639, Max=0.719, Mean=0.695\n",
      "Prediction stats: Min=0.640, Max=0.718, Mean=0.696\n",
      "Prediction stats: Min=0.660, Max=0.717, Mean=0.691\n",
      "Prediction stats: Min=0.647, Max=0.719, Mean=0.695\n",
      "Prediction stats: Min=0.506, Max=0.714, Mean=0.689\n",
      "Prediction stats: Min=0.651, Max=0.719, Mean=0.693\n",
      "Epoch 54/300, Train Loss: 6.57781291, Train Acc: 0.55357143, Val Loss: 2.01594210, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.641, Max=0.719, Mean=0.692\n",
      "Prediction stats: Min=0.642, Max=0.721, Mean=0.690\n",
      "Prediction stats: Min=0.519, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.640, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.639, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.630, Max=0.713, Mean=0.681\n",
      "Prediction stats: Min=0.642, Max=0.714, Mean=0.689\n",
      "Epoch 55/300, Train Loss: 6.59690505, Train Acc: 0.51339286, Val Loss: 2.01436257, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.519, Max=0.715, Mean=0.688\n",
      "Prediction stats: Min=0.660, Max=0.714, Mean=0.695\n",
      "Prediction stats: Min=0.649, Max=0.718, Mean=0.695\n",
      "Prediction stats: Min=0.660, Max=0.719, Mean=0.694\n",
      "Prediction stats: Min=0.646, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.648, Max=0.716, Mean=0.695\n",
      "Prediction stats: Min=0.628, Max=0.721, Mean=0.688\n",
      "Epoch 56/300, Train Loss: 6.63455337, Train Acc: 0.46875000, Val Loss: 2.01889443, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.650, Max=0.720, Mean=0.696\n",
      "Prediction stats: Min=0.665, Max=0.715, Mean=0.694\n",
      "Prediction stats: Min=0.646, Max=0.715, Mean=0.693\n",
      "Prediction stats: Min=0.663, Max=0.721, Mean=0.698\n",
      "Prediction stats: Min=0.639, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.652, Max=0.720, Mean=0.690\n",
      "Prediction stats: Min=0.504, Max=0.719, Mean=0.689\n",
      "Epoch 57/300, Train Loss: 6.59672350, Train Acc: 0.52232143, Val Loss: 2.01846820, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.657, Max=0.715, Mean=0.693\n",
      "Prediction stats: Min=0.636, Max=0.718, Mean=0.687\n",
      "Prediction stats: Min=0.652, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.649, Max=0.721, Mean=0.689\n",
      "Prediction stats: Min=0.656, Max=0.718, Mean=0.702\n",
      "Prediction stats: Min=0.512, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.668, Max=0.721, Mean=0.701\n",
      "Epoch 58/300, Train Loss: 6.61345500, Train Acc: 0.46875000, Val Loss: 2.02083564, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.647, Max=0.722, Mean=0.700\n",
      "Prediction stats: Min=0.658, Max=0.722, Mean=0.694\n",
      "Prediction stats: Min=0.507, Max=0.721, Mean=0.691\n",
      "Prediction stats: Min=0.651, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.636, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.651, Max=0.719, Mean=0.698\n",
      "Prediction stats: Min=0.666, Max=0.722, Mean=0.695\n",
      "Epoch 59/300, Train Loss: 6.60059744, Train Acc: 0.50000000, Val Loss: 2.02147603, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.629, Max=0.722, Mean=0.690\n",
      "Prediction stats: Min=0.652, Max=0.716, Mean=0.694\n",
      "Prediction stats: Min=0.647, Max=0.721, Mean=0.697\n",
      "Prediction stats: Min=0.560, Max=0.721, Mean=0.686\n",
      "Prediction stats: Min=0.646, Max=0.718, Mean=0.696\n",
      "Prediction stats: Min=0.660, Max=0.718, Mean=0.698\n",
      "Prediction stats: Min=0.643, Max=0.719, Mean=0.699\n",
      "Epoch 60/300, Train Loss: 6.63167465, Train Acc: 0.47321429, Val Loss: 2.02153611, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.650, Max=0.723, Mean=0.697\n",
      "Prediction stats: Min=0.503, Max=0.719, Mean=0.694\n",
      "Prediction stats: Min=0.651, Max=0.721, Mean=0.692\n",
      "Prediction stats: Min=0.637, Max=0.722, Mean=0.697\n",
      "Prediction stats: Min=0.664, Max=0.721, Mean=0.693\n",
      "Prediction stats: Min=0.655, Max=0.719, Mean=0.700\n",
      "Prediction stats: Min=0.646, Max=0.719, Mean=0.695\n",
      "Epoch 61/300, Train Loss: 6.58827776, Train Acc: 0.53571429, Val Loss: 2.01839197, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.509, Max=0.721, Mean=0.687\n",
      "Prediction stats: Min=0.631, Max=0.716, Mean=0.693\n",
      "Prediction stats: Min=0.655, Max=0.717, Mean=0.694\n",
      "Prediction stats: Min=0.649, Max=0.721, Mean=0.695\n",
      "Prediction stats: Min=0.651, Max=0.719, Mean=0.688\n",
      "Prediction stats: Min=0.635, Max=0.718, Mean=0.688\n",
      "Prediction stats: Min=0.650, Max=0.717, Mean=0.691\n",
      "Epoch 62/300, Train Loss: 6.60779703, Train Acc: 0.48214286, Val Loss: 2.01630634, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.647, Max=0.721, Mean=0.700\n",
      "Prediction stats: Min=0.645, Max=0.719, Mean=0.688\n",
      "Prediction stats: Min=0.647, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.503, Max=0.718, Mean=0.684\n",
      "Prediction stats: Min=0.633, Max=0.714, Mean=0.687\n",
      "Prediction stats: Min=0.643, Max=0.719, Mean=0.690\n",
      "Prediction stats: Min=0.649, Max=0.714, Mean=0.688\n",
      "Epoch 63/300, Train Loss: 6.60526901, Train Acc: 0.53125000, Val Loss: 2.01264930, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.648, Max=0.720, Mean=0.691\n",
      "Prediction stats: Min=0.624, Max=0.718, Mean=0.688\n",
      "Prediction stats: Min=0.645, Max=0.719, Mean=0.692\n",
      "Prediction stats: Min=0.654, Max=0.718, Mean=0.692\n",
      "Prediction stats: Min=0.643, Max=0.718, Mean=0.692\n",
      "Prediction stats: Min=0.636, Max=0.717, Mean=0.688\n",
      "Prediction stats: Min=0.511, Max=0.713, Mean=0.685\n",
      "Epoch 64/300, Train Loss: 6.63507599, Train Acc: 0.46428571, Val Loss: 2.01303226, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.644, Max=0.715, Mean=0.688\n",
      "Prediction stats: Min=0.640, Max=0.717, Mean=0.686\n",
      "Prediction stats: Min=0.506, Max=0.717, Mean=0.688\n",
      "Prediction stats: Min=0.634, Max=0.712, Mean=0.688\n",
      "Prediction stats: Min=0.637, Max=0.716, Mean=0.688\n",
      "Prediction stats: Min=0.658, Max=0.716, Mean=0.691\n",
      "Prediction stats: Min=0.661, Max=0.715, Mean=0.691\n",
      "Epoch 65/300, Train Loss: 6.60130960, Train Acc: 0.52232143, Val Loss: 2.01028973, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.639, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.659, Max=0.720, Mean=0.690\n",
      "Prediction stats: Min=0.644, Max=0.712, Mean=0.684\n",
      "Prediction stats: Min=0.502, Max=0.715, Mean=0.683\n",
      "Prediction stats: Min=0.634, Max=0.716, Mean=0.689\n",
      "Prediction stats: Min=0.635, Max=0.716, Mean=0.687\n",
      "Prediction stats: Min=0.647, Max=0.717, Mean=0.688\n",
      "Epoch 66/300, Train Loss: 6.65720624, Train Acc: 0.46428571, Val Loss: 2.01039219, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.646, Max=0.721, Mean=0.691\n",
      "Prediction stats: Min=0.642, Max=0.719, Mean=0.688\n",
      "Prediction stats: Min=0.637, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.639, Max=0.716, Mean=0.684\n",
      "Prediction stats: Min=0.511, Max=0.716, Mean=0.678\n",
      "Prediction stats: Min=0.625, Max=0.709, Mean=0.684\n",
      "Prediction stats: Min=0.653, Max=0.714, Mean=0.691\n",
      "Epoch 67/300, Train Loss: 6.60496765, Train Acc: 0.46428571, Val Loss: 2.00714016, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.635, Max=0.715, Mean=0.688\n",
      "Prediction stats: Min=0.515, Max=0.715, Mean=0.677\n",
      "Prediction stats: Min=0.651, Max=0.716, Mean=0.688\n",
      "Prediction stats: Min=0.651, Max=0.719, Mean=0.688\n",
      "Prediction stats: Min=0.642, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.656, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.646, Max=0.721, Mean=0.692\n",
      "Epoch 68/300, Train Loss: 6.62673002, Train Acc: 0.43750000, Val Loss: 2.01481742, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.646, Max=0.718, Mean=0.688\n",
      "Prediction stats: Min=0.627, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.650, Max=0.720, Mean=0.691\n",
      "Prediction stats: Min=0.645, Max=0.718, Mean=0.692\n",
      "Prediction stats: Min=0.501, Max=0.718, Mean=0.685\n",
      "Prediction stats: Min=0.648, Max=0.716, Mean=0.690\n",
      "Prediction stats: Min=0.642, Max=0.720, Mean=0.691\n",
      "Epoch 69/300, Train Loss: 6.62248826, Train Acc: 0.47767857, Val Loss: 2.01549292, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.650, Max=0.721, Mean=0.696\n",
      "Prediction stats: Min=0.508, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.642, Max=0.718, Mean=0.693\n",
      "Prediction stats: Min=0.661, Max=0.715, Mean=0.690\n",
      "Prediction stats: Min=0.644, Max=0.712, Mean=0.689\n",
      "Prediction stats: Min=0.643, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.639, Max=0.713, Mean=0.691\n",
      "Epoch 70/300, Train Loss: 6.62304175, Train Acc: 0.44642857, Val Loss: 2.01151979, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.671, Max=0.713, Mean=0.695\n",
      "Prediction stats: Min=0.627, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.646, Max=0.710, Mean=0.684\n",
      "Prediction stats: Min=0.637, Max=0.715, Mean=0.686\n",
      "Prediction stats: Min=0.656, Max=0.715, Mean=0.689\n",
      "Prediction stats: Min=0.504, Max=0.718, Mean=0.682\n",
      "Prediction stats: Min=0.633, Max=0.717, Mean=0.684\n",
      "Epoch 71/300, Train Loss: 6.61620790, Train Acc: 0.48660714, Val Loss: 2.00955522, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.644, Max=0.709, Mean=0.684\n",
      "Prediction stats: Min=0.642, Max=0.704, Mean=0.680\n",
      "Prediction stats: Min=0.635, Max=0.715, Mean=0.685\n",
      "Prediction stats: Min=0.626, Max=0.715, Mean=0.696\n",
      "Prediction stats: Min=0.645, Max=0.711, Mean=0.688\n",
      "Prediction stats: Min=0.506, Max=0.709, Mean=0.679\n",
      "Prediction stats: Min=0.637, Max=0.716, Mean=0.691\n",
      "Epoch 72/300, Train Loss: 6.60680032, Train Acc: 0.49553571, Val Loss: 2.00967461, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.628, Max=0.714, Mean=0.688\n",
      "Prediction stats: Min=0.642, Max=0.718, Mean=0.688\n",
      "Prediction stats: Min=0.635, Max=0.717, Mean=0.686\n",
      "Prediction stats: Min=0.545, Max=0.716, Mean=0.681\n",
      "Prediction stats: Min=0.628, Max=0.710, Mean=0.690\n",
      "Prediction stats: Min=0.633, Max=0.713, Mean=0.686\n",
      "Prediction stats: Min=0.642, Max=0.717, Mean=0.685\n",
      "Epoch 73/300, Train Loss: 6.58364254, Train Acc: 0.51339286, Val Loss: 2.00859135, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.641, Max=0.718, Mean=0.683\n",
      "Prediction stats: Min=0.657, Max=0.716, Mean=0.689\n",
      "Prediction stats: Min=0.650, Max=0.718, Mean=0.687\n",
      "Prediction stats: Min=0.645, Max=0.720, Mean=0.685\n",
      "Prediction stats: Min=0.510, Max=0.710, Mean=0.680\n",
      "Prediction stats: Min=0.652, Max=0.713, Mean=0.689\n",
      "Prediction stats: Min=0.664, Max=0.717, Mean=0.695\n",
      "Epoch 74/300, Train Loss: 6.56018656, Train Acc: 0.54910714, Val Loss: 2.00944519, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.640, Max=0.720, Mean=0.688\n",
      "Prediction stats: Min=0.649, Max=0.716, Mean=0.692\n",
      "Prediction stats: Min=0.631, Max=0.721, Mean=0.688\n",
      "Prediction stats: Min=0.658, Max=0.717, Mean=0.694\n",
      "Prediction stats: Min=0.503, Max=0.717, Mean=0.687\n",
      "Prediction stats: Min=0.632, Max=0.715, Mean=0.686\n",
      "Prediction stats: Min=0.635, Max=0.720, Mean=0.688\n",
      "Epoch 75/300, Train Loss: 6.59944195, Train Acc: 0.51785714, Val Loss: 2.00807905, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.632, Max=0.719, Mean=0.683\n",
      "Prediction stats: Min=0.519, Max=0.710, Mean=0.679\n",
      "Prediction stats: Min=0.648, Max=0.712, Mean=0.684\n",
      "Prediction stats: Min=0.636, Max=0.718, Mean=0.687\n",
      "Prediction stats: Min=0.615, Max=0.713, Mean=0.683\n",
      "Prediction stats: Min=0.643, Max=0.711, Mean=0.682\n",
      "Prediction stats: Min=0.620, Max=0.715, Mean=0.690\n",
      "Epoch 76/300, Train Loss: 6.58940643, Train Acc: 0.53571429, Val Loss: 2.00560516, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.632, Max=0.716, Mean=0.686\n",
      "Prediction stats: Min=0.653, Max=0.719, Mean=0.688\n",
      "Prediction stats: Min=0.641, Max=0.714, Mean=0.684\n",
      "Prediction stats: Min=0.626, Max=0.714, Mean=0.678\n",
      "Prediction stats: Min=0.634, Max=0.720, Mean=0.681\n",
      "Prediction stats: Min=0.506, Max=0.717, Mean=0.682\n",
      "Prediction stats: Min=0.632, Max=0.712, Mean=0.681\n",
      "Epoch 77/300, Train Loss: 6.64082378, Train Acc: 0.41964286, Val Loss: 2.00432050, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.635, Max=0.716, Mean=0.688\n",
      "Prediction stats: Min=0.641, Max=0.714, Mean=0.681\n",
      "Prediction stats: Min=0.502, Max=0.713, Mean=0.676\n",
      "Prediction stats: Min=0.630, Max=0.717, Mean=0.677\n",
      "Prediction stats: Min=0.642, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.638, Max=0.720, Mean=0.690\n",
      "Prediction stats: Min=0.639, Max=0.720, Mean=0.684\n",
      "Epoch 78/300, Train Loss: 6.59513223, Train Acc: 0.54017857, Val Loss: 2.01186293, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.637, Max=0.714, Mean=0.681\n",
      "Prediction stats: Min=0.647, Max=0.719, Mean=0.695\n",
      "Prediction stats: Min=0.634, Max=0.719, Mean=0.689\n",
      "Prediction stats: Min=0.629, Max=0.718, Mean=0.688\n",
      "Prediction stats: Min=0.627, Max=0.719, Mean=0.687\n",
      "Prediction stats: Min=0.629, Max=0.717, Mean=0.685\n",
      "Prediction stats: Min=0.511, Max=0.721, Mean=0.689\n",
      "Epoch 79/300, Train Loss: 6.62551087, Train Acc: 0.50446429, Val Loss: 2.01555723, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.500, Max=0.717, Mean=0.682\n",
      "Prediction stats: Min=0.640, Max=0.721, Mean=0.699\n",
      "Prediction stats: Min=0.652, Max=0.720, Mean=0.692\n",
      "Prediction stats: Min=0.622, Max=0.719, Mean=0.692\n",
      "Prediction stats: Min=0.644, Max=0.721, Mean=0.690\n",
      "Prediction stats: Min=0.636, Max=0.720, Mean=0.689\n",
      "Prediction stats: Min=0.646, Max=0.719, Mean=0.693\n",
      "Epoch 80/300, Train Loss: 6.55323899, Train Acc: 0.51339286, Val Loss: 2.02141666, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.625, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.648, Max=0.719, Mean=0.696\n",
      "Prediction stats: Min=0.641, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.626, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.633, Max=0.719, Mean=0.694\n",
      "Prediction stats: Min=0.506, Max=0.719, Mean=0.682\n",
      "Prediction stats: Min=0.643, Max=0.718, Mean=0.696\n",
      "Epoch 81/300, Train Loss: 6.63965827, Train Acc: 0.50000000, Val Loss: 2.02122211, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.509, Max=0.723, Mean=0.684\n",
      "Prediction stats: Min=0.640, Max=0.724, Mean=0.696\n",
      "Prediction stats: Min=0.663, Max=0.723, Mean=0.703\n",
      "Prediction stats: Min=0.642, Max=0.719, Mean=0.696\n",
      "Prediction stats: Min=0.648, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.642, Max=0.721, Mean=0.696\n",
      "Prediction stats: Min=0.676, Max=0.722, Mean=0.701\n",
      "Epoch 82/300, Train Loss: 6.58706629, Train Acc: 0.53571429, Val Loss: 2.02592593, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.633, Max=0.723, Mean=0.694\n",
      "Prediction stats: Min=0.646, Max=0.720, Mean=0.692\n",
      "Prediction stats: Min=0.653, Max=0.724, Mean=0.695\n",
      "Prediction stats: Min=0.655, Max=0.722, Mean=0.698\n",
      "Prediction stats: Min=0.657, Max=0.721, Mean=0.699\n",
      "Prediction stats: Min=0.513, Max=0.723, Mean=0.684\n",
      "Prediction stats: Min=0.661, Max=0.722, Mean=0.702\n",
      "Epoch 83/300, Train Loss: 6.64422613, Train Acc: 0.49107143, Val Loss: 2.02633363, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.657, Max=0.722, Mean=0.699\n",
      "Prediction stats: Min=0.635, Max=0.720, Mean=0.694\n",
      "Prediction stats: Min=0.649, Max=0.722, Mean=0.697\n",
      "Prediction stats: Min=0.501, Max=0.720, Mean=0.685\n",
      "Prediction stats: Min=0.665, Max=0.717, Mean=0.695\n",
      "Prediction stats: Min=0.645, Max=0.717, Mean=0.691\n",
      "Prediction stats: Min=0.648, Max=0.721, Mean=0.696\n",
      "Epoch 84/300, Train Loss: 6.61958963, Train Acc: 0.45535714, Val Loss: 2.02383286, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.649, Max=0.717, Mean=0.691\n",
      "Prediction stats: Min=0.663, Max=0.718, Mean=0.697\n",
      "Prediction stats: Min=0.645, Max=0.723, Mean=0.695\n",
      "Prediction stats: Min=0.627, Max=0.720, Mean=0.691\n",
      "Prediction stats: Min=0.648, Max=0.721, Mean=0.687\n",
      "Prediction stats: Min=0.504, Max=0.718, Mean=0.688\n",
      "Prediction stats: Min=0.659, Max=0.722, Mean=0.699\n",
      "Epoch 85/300, Train Loss: 6.64044607, Train Acc: 0.44642857, Val Loss: 2.02341753, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.629, Max=0.721, Mean=0.691\n",
      "Prediction stats: Min=0.645, Max=0.720, Mean=0.699\n",
      "Prediction stats: Min=0.502, Max=0.721, Mean=0.691\n",
      "Prediction stats: Min=0.671, Max=0.722, Mean=0.701\n",
      "Prediction stats: Min=0.636, Max=0.720, Mean=0.697\n",
      "Prediction stats: Min=0.627, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.657, Max=0.722, Mean=0.698\n",
      "Epoch 86/300, Train Loss: 6.60924011, Train Acc: 0.52232143, Val Loss: 2.02414465, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.655, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.650, Max=0.719, Mean=0.699\n",
      "Prediction stats: Min=0.643, Max=0.722, Mean=0.693\n",
      "Prediction stats: Min=0.629, Max=0.724, Mean=0.695\n",
      "Prediction stats: Min=0.652, Max=0.723, Mean=0.698\n",
      "Prediction stats: Min=0.504, Max=0.723, Mean=0.693\n",
      "Prediction stats: Min=0.654, Max=0.721, Mean=0.696\n",
      "Epoch 87/300, Train Loss: 6.63723350, Train Acc: 0.50000000, Val Loss: 2.02681136, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.643, Max=0.720, Mean=0.696\n",
      "Prediction stats: Min=0.501, Max=0.720, Mean=0.688\n",
      "Prediction stats: Min=0.643, Max=0.720, Mean=0.697\n",
      "Prediction stats: Min=0.650, Max=0.720, Mean=0.697\n",
      "Prediction stats: Min=0.653, Max=0.720, Mean=0.698\n",
      "Prediction stats: Min=0.629, Max=0.722, Mean=0.693\n",
      "Prediction stats: Min=0.675, Max=0.723, Mean=0.698\n",
      "Epoch 88/300, Train Loss: 6.59669119, Train Acc: 0.52678571, Val Loss: 2.02542579, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.641, Max=0.722, Mean=0.695\n",
      "Prediction stats: Min=0.646, Max=0.721, Mean=0.694\n",
      "Prediction stats: Min=0.642, Max=0.720, Mean=0.694\n",
      "Prediction stats: Min=0.502, Max=0.722, Mean=0.684\n",
      "Prediction stats: Min=0.663, Max=0.721, Mean=0.699\n",
      "Prediction stats: Min=0.672, Max=0.720, Mean=0.705\n",
      "Prediction stats: Min=0.658, Max=0.721, Mean=0.693\n",
      "Epoch 89/300, Train Loss: 6.67402411, Train Acc: 0.44642857, Val Loss: 2.02357960, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.506, Max=0.719, Mean=0.686\n",
      "Prediction stats: Min=0.662, Max=0.722, Mean=0.700\n",
      "Prediction stats: Min=0.657, Max=0.721, Mean=0.698\n",
      "Prediction stats: Min=0.631, Max=0.718, Mean=0.690\n",
      "Prediction stats: Min=0.670, Max=0.721, Mean=0.704\n",
      "Prediction stats: Min=0.645, Max=0.721, Mean=0.701\n",
      "Prediction stats: Min=0.640, Max=0.722, Mean=0.695\n",
      "Epoch 90/300, Train Loss: 6.59542948, Train Acc: 0.53125000, Val Loss: 2.02235997, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.630, Max=0.721, Mean=0.692\n",
      "Prediction stats: Min=0.661, Max=0.721, Mean=0.699\n",
      "Prediction stats: Min=0.648, Max=0.722, Mean=0.699\n",
      "Prediction stats: Min=0.659, Max=0.716, Mean=0.697\n",
      "Prediction stats: Min=0.502, Max=0.722, Mean=0.690\n",
      "Prediction stats: Min=0.635, Max=0.719, Mean=0.694\n",
      "Prediction stats: Min=0.659, Max=0.719, Mean=0.700\n",
      "Epoch 91/300, Train Loss: 6.64929497, Train Acc: 0.45535714, Val Loss: 2.02104235, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.670, Max=0.717, Mean=0.700\n",
      "Prediction stats: Min=0.539, Max=0.721, Mean=0.687\n",
      "Prediction stats: Min=0.649, Max=0.721, Mean=0.696\n",
      "Prediction stats: Min=0.642, Max=0.718, Mean=0.689\n",
      "Prediction stats: Min=0.642, Max=0.719, Mean=0.693\n",
      "Prediction stats: Min=0.669, Max=0.720, Mean=0.695\n",
      "Prediction stats: Min=0.647, Max=0.717, Mean=0.694\n",
      "Epoch 92/300, Train Loss: 6.55745327, Train Acc: 0.54464286, Val Loss: 2.01993442, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.639, Max=0.717, Mean=0.689\n",
      "Prediction stats: Min=0.501, Max=0.716, Mean=0.689\n",
      "Prediction stats: Min=0.641, Max=0.716, Mean=0.687\n",
      "Prediction stats: Min=0.653, Max=0.721, Mean=0.692\n",
      "Prediction stats: Min=0.654, Max=0.714, Mean=0.696\n",
      "Prediction stats: Min=0.650, Max=0.720, Mean=0.699\n",
      "Prediction stats: Min=0.662, Max=0.715, Mean=0.694\n",
      "Epoch 93/300, Train Loss: 6.58625531, Train Acc: 0.53125000, Val Loss: 2.01998580, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.651, Max=0.717, Mean=0.697\n",
      "Prediction stats: Min=0.639, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.636, Max=0.719, Mean=0.693\n",
      "Prediction stats: Min=0.656, Max=0.720, Mean=0.694\n",
      "Prediction stats: Min=0.507, Max=0.717, Mean=0.687\n",
      "Prediction stats: Min=0.656, Max=0.720, Mean=0.696\n",
      "Prediction stats: Min=0.667, Max=0.715, Mean=0.694\n",
      "Epoch 94/300, Train Loss: 6.64876312, Train Acc: 0.48214286, Val Loss: 2.01790595, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.500, Max=0.720, Mean=0.687\n",
      "Prediction stats: Min=0.652, Max=0.718, Mean=0.689\n",
      "Prediction stats: Min=0.642, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.632, Max=0.715, Mean=0.692\n",
      "Prediction stats: Min=0.657, Max=0.720, Mean=0.695\n",
      "Prediction stats: Min=0.638, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.644, Max=0.715, Mean=0.695\n",
      "Epoch 95/300, Train Loss: 6.55745924, Train Acc: 0.56250000, Val Loss: 2.01827705, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.646, Max=0.716, Mean=0.691\n",
      "Prediction stats: Min=0.653, Max=0.720, Mean=0.698\n",
      "Prediction stats: Min=0.647, Max=0.715, Mean=0.695\n",
      "Prediction stats: Min=0.659, Max=0.718, Mean=0.695\n",
      "Prediction stats: Min=0.650, Max=0.717, Mean=0.697\n",
      "Prediction stats: Min=0.500, Max=0.717, Mean=0.680\n",
      "Prediction stats: Min=0.649, Max=0.721, Mean=0.693\n",
      "Epoch 96/300, Train Loss: 6.60864645, Train Acc: 0.44196429, Val Loss: 2.01732719, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.654, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.677, Max=0.716, Mean=0.699\n",
      "Prediction stats: Min=0.665, Max=0.714, Mean=0.692\n",
      "Prediction stats: Min=0.516, Max=0.719, Mean=0.689\n",
      "Prediction stats: Min=0.638, Max=0.716, Mean=0.693\n",
      "Prediction stats: Min=0.644, Max=0.717, Mean=0.694\n",
      "Prediction stats: Min=0.637, Max=0.722, Mean=0.692\n",
      "Epoch 97/300, Train Loss: 6.57293874, Train Acc: 0.51339286, Val Loss: 2.01862150, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.670, Max=0.718, Mean=0.697\n",
      "Prediction stats: Min=0.654, Max=0.714, Mean=0.697\n",
      "Prediction stats: Min=0.504, Max=0.719, Mean=0.689\n",
      "Prediction stats: Min=0.650, Max=0.715, Mean=0.694\n",
      "Prediction stats: Min=0.636, Max=0.716, Mean=0.691\n",
      "Prediction stats: Min=0.645, Max=0.710, Mean=0.691\n",
      "Prediction stats: Min=0.639, Max=0.718, Mean=0.693\n",
      "Epoch 98/300, Train Loss: 6.61794537, Train Acc: 0.44642857, Val Loss: 2.02017164, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.657, Max=0.722, Mean=0.690\n",
      "Prediction stats: Min=0.636, Max=0.718, Mean=0.692\n",
      "Prediction stats: Min=0.636, Max=0.723, Mean=0.689\n",
      "Prediction stats: Min=0.649, Max=0.719, Mean=0.693\n",
      "Prediction stats: Min=0.639, Max=0.720, Mean=0.685\n",
      "Prediction stats: Min=0.639, Max=0.714, Mean=0.694\n",
      "Prediction stats: Min=0.508, Max=0.722, Mean=0.691\n",
      "Epoch 99/300, Train Loss: 6.59223735, Train Acc: 0.50892857, Val Loss: 2.02010107, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.655, Max=0.711, Mean=0.694\n",
      "Prediction stats: Min=0.640, Max=0.718, Mean=0.694\n",
      "Prediction stats: Min=0.637, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.537, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.661, Max=0.718, Mean=0.692\n",
      "Prediction stats: Min=0.668, Max=0.720, Mean=0.700\n",
      "Prediction stats: Min=0.643, Max=0.715, Mean=0.690\n",
      "Epoch 100/300, Train Loss: 6.66064841, Train Acc: 0.43303571, Val Loss: 2.01848072, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.661, Max=0.717, Mean=0.696\n",
      "Prediction stats: Min=0.651, Max=0.714, Mean=0.693\n",
      "Prediction stats: Min=0.643, Max=0.714, Mean=0.689\n",
      "Prediction stats: Min=0.647, Max=0.717, Mean=0.690\n",
      "Prediction stats: Min=0.522, Max=0.719, Mean=0.686\n",
      "Prediction stats: Min=0.648, Max=0.717, Mean=0.689\n",
      "Prediction stats: Min=0.666, Max=0.716, Mean=0.693\n",
      "Epoch 101/300, Train Loss: 6.58525348, Train Acc: 0.49107143, Val Loss: 2.01622427, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.651, Max=0.717, Mean=0.686\n",
      "Prediction stats: Min=0.665, Max=0.719, Mean=0.695\n",
      "Prediction stats: Min=0.643, Max=0.714, Mean=0.689\n",
      "Prediction stats: Min=0.662, Max=0.718, Mean=0.691\n",
      "Prediction stats: Min=0.640, Max=0.718, Mean=0.690\n",
      "Prediction stats: Min=0.652, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.502, Max=0.718, Mean=0.685\n",
      "Epoch 102/300, Train Loss: 6.60241449, Train Acc: 0.52678571, Val Loss: 2.01351029, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.503, Max=0.716, Mean=0.690\n",
      "Prediction stats: Min=0.637, Max=0.717, Mean=0.687\n",
      "Prediction stats: Min=0.641, Max=0.716, Mean=0.690\n",
      "Prediction stats: Min=0.645, Max=0.710, Mean=0.693\n",
      "Prediction stats: Min=0.662, Max=0.717, Mean=0.694\n",
      "Prediction stats: Min=0.640, Max=0.716, Mean=0.692\n",
      "Prediction stats: Min=0.662, Max=0.711, Mean=0.693\n",
      "Epoch 103/300, Train Loss: 6.60839111, Train Acc: 0.50446429, Val Loss: 2.01483226, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.642, Max=0.714, Mean=0.688\n",
      "Prediction stats: Min=0.656, Max=0.716, Mean=0.694\n",
      "Prediction stats: Min=0.663, Max=0.717, Mean=0.693\n",
      "Prediction stats: Min=0.664, Max=0.713, Mean=0.690\n",
      "Prediction stats: Min=0.652, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.505, Max=0.720, Mean=0.693\n",
      "Prediction stats: Min=0.665, Max=0.715, Mean=0.692\n",
      "Epoch 104/300, Train Loss: 6.59628308, Train Acc: 0.51339286, Val Loss: 2.01795340, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.657, Max=0.714, Mean=0.693\n",
      "Prediction stats: Min=0.645, Max=0.717, Mean=0.695\n",
      "Prediction stats: Min=0.649, Max=0.716, Mean=0.698\n",
      "Prediction stats: Min=0.647, Max=0.711, Mean=0.691\n",
      "Prediction stats: Min=0.642, Max=0.718, Mean=0.690\n",
      "Prediction stats: Min=0.501, Max=0.720, Mean=0.692\n",
      "Prediction stats: Min=0.642, Max=0.713, Mean=0.693\n",
      "Epoch 105/300, Train Loss: 6.60242081, Train Acc: 0.47767857, Val Loss: 2.01727128, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.654, Max=0.716, Mean=0.693\n",
      "Prediction stats: Min=0.662, Max=0.721, Mean=0.695\n",
      "Prediction stats: Min=0.652, Max=0.715, Mean=0.698\n",
      "Prediction stats: Min=0.667, Max=0.714, Mean=0.696\n",
      "Prediction stats: Min=0.658, Max=0.714, Mean=0.696\n",
      "Prediction stats: Min=0.632, Max=0.715, Mean=0.686\n",
      "Prediction stats: Min=0.501, Max=0.721, Mean=0.689\n",
      "Epoch 106/300, Train Loss: 6.62106800, Train Acc: 0.48660714, Val Loss: 2.01521009, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.635, Max=0.717, Mean=0.683\n",
      "Prediction stats: Min=0.502, Max=0.720, Mean=0.687\n",
      "Prediction stats: Min=0.666, Max=0.715, Mean=0.689\n",
      "Prediction stats: Min=0.655, Max=0.719, Mean=0.693\n",
      "Prediction stats: Min=0.649, Max=0.715, Mean=0.693\n",
      "Prediction stats: Min=0.637, Max=0.711, Mean=0.684\n",
      "Prediction stats: Min=0.670, Max=0.719, Mean=0.692\n",
      "Epoch 107/300, Train Loss: 6.59943533, Train Acc: 0.49107143, Val Loss: 2.01365364, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.650, Max=0.714, Mean=0.693\n",
      "Prediction stats: Min=0.653, Max=0.716, Mean=0.691\n",
      "Prediction stats: Min=0.660, Max=0.715, Mean=0.695\n",
      "Prediction stats: Min=0.650, Max=0.716, Mean=0.692\n",
      "Prediction stats: Min=0.642, Max=0.716, Mean=0.690\n",
      "Prediction stats: Min=0.509, Max=0.719, Mean=0.691\n",
      "Prediction stats: Min=0.636, Max=0.716, Mean=0.691\n",
      "Epoch 108/300, Train Loss: 6.58383703, Train Acc: 0.52678571, Val Loss: 2.01441801, Val Acc: 0.35416667\n",
      "Prediction stats: Min=0.665, Max=0.719, Mean=0.692\n",
      "Prediction stats: Min=0.663, Max=0.714, Mean=0.692\n",
      "Prediction stats: Min=0.654, Max=0.716, Mean=0.695\n",
      "Prediction stats: Min=0.501, Max=0.713, Mean=0.683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_torch_model_binary\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_torch_model_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_treshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bartosz\\Desktop\\automatic-lie-detection\\models\\utils\\model_functions\\train_model.py:102\u001b[0m, in \u001b[0;36mtrain_torch_model_binary\u001b[1;34m(model, criterion, optimizer, X_train, y_train, X_val, y_val, writer, batch_size, epochs, prediction_treshold)\u001b[0m\n\u001b[0;32m    100\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    101\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(outputs)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction stats: Min=\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.3f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m, Max=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobs\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Mean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobs\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    103\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:1052\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m-> 1052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.model_functions import train_torch_model_binary\n",
    "\n",
    "train_torch_model_binary(model, criterion, optimizer, X_train, y_train, X_val, y_val, writer=writer, prediction_treshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0390, Test Accuracy: 0.5208\n"
     ]
    }
   ],
   "source": [
    "from utils.model_functions import eval_torch_model_binary\n",
    "\n",
    "eval_torch_model_binary(model, criterion, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Seglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from seglearn.pipe import Pype\n",
    "from seglearn.transform import FeatureRep, Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.numpy()\n",
    "X_val_np = X_val.numpy()\n",
    "X_test_np = X_test.numpy()\n",
    "y_train_np = y_train.numpy()\n",
    "y_val_np = y_val.numpy()\n",
    "y_test_np = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 1679, 154)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budowa modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pype([\n",
    "    (\"segment\", Segment(width=20, step=10)),  # Segmentacja sekwencji\n",
    "    (\"features\", FeatureRep()),              # Ekstrakcja cech\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=200\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pype(steps=[(&#x27;segment&#x27;, Segment(overlap=None, step=10, width=20)),\n",
       "            (&#x27;features&#x27;,\n",
       "             FeatureRep(features={&#x27;abs_energy&#x27;: &lt;function abs_energy at 0x0000011B25FDCA40&gt;,\n",
       "                                  &#x27;kurt&#x27;: &lt;function kurt at 0x0000011B25FDCF40&gt;,\n",
       "                                  &#x27;max&#x27;: &lt;function maximum at 0x0000011B25FDCE00&gt;,\n",
       "                                  &#x27;mean&#x27;: &lt;function mean at 0x0000011B25FDC680&gt;,\n",
       "                                  &#x27;median&#x27;: &lt;function median at 0x0000011B25FDC720&gt;,\n",
       "                                  &#x27;min&#x27;: &lt;function minimum at 0x...\n",
       "                           gamma=None, grow_policy=None, importance_type=None,\n",
       "                           interaction_constraints=None, learning_rate=None,\n",
       "                           max_bin=None, max_cat_threshold=None,\n",
       "                           max_cat_to_onehot=None, max_delta_step=None,\n",
       "                           max_depth=None, max_leaves=None,\n",
       "                           min_child_weight=None, missing=nan,\n",
       "                           monotone_constraints=None, multi_strategy=None,\n",
       "                           n_estimators=200, n_jobs=None,\n",
       "                           num_parallel_tree=None, random_state=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Pype<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pype(steps=[(&#x27;segment&#x27;, Segment(overlap=None, step=10, width=20)),\n",
       "            (&#x27;features&#x27;,\n",
       "             FeatureRep(features={&#x27;abs_energy&#x27;: &lt;function abs_energy at 0x0000011B25FDCA40&gt;,\n",
       "                                  &#x27;kurt&#x27;: &lt;function kurt at 0x0000011B25FDCF40&gt;,\n",
       "                                  &#x27;max&#x27;: &lt;function maximum at 0x0000011B25FDCE00&gt;,\n",
       "                                  &#x27;mean&#x27;: &lt;function mean at 0x0000011B25FDC680&gt;,\n",
       "                                  &#x27;median&#x27;: &lt;function median at 0x0000011B25FDC720&gt;,\n",
       "                                  &#x27;min&#x27;: &lt;function minimum at 0x...\n",
       "                           gamma=None, grow_policy=None, importance_type=None,\n",
       "                           interaction_constraints=None, learning_rate=None,\n",
       "                           max_bin=None, max_cat_threshold=None,\n",
       "                           max_cat_to_onehot=None, max_delta_step=None,\n",
       "                           max_depth=None, max_leaves=None,\n",
       "                           min_child_weight=None, missing=nan,\n",
       "                           monotone_constraints=None, multi_strategy=None,\n",
       "                           n_estimators=200, n_jobs=None,\n",
       "                           num_parallel_tree=None, random_state=None, ...))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Segment</label><div class=\"sk-toggleable__content fitted\"><pre>Segment(overlap=None, step=10, width=20)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">FeatureRep</label><div class=\"sk-toggleable__content fitted\"><pre>FeatureRep(features={&#x27;abs_energy&#x27;: &lt;function abs_energy at 0x0000011B25FDCA40&gt;,\n",
       "                     &#x27;kurt&#x27;: &lt;function kurt at 0x0000011B25FDCF40&gt;,\n",
       "                     &#x27;max&#x27;: &lt;function maximum at 0x0000011B25FDCE00&gt;,\n",
       "                     &#x27;mean&#x27;: &lt;function mean at 0x0000011B25FDC680&gt;,\n",
       "                     &#x27;median&#x27;: &lt;function median at 0x0000011B25FDC720&gt;,\n",
       "                     &#x27;min&#x27;: &lt;function minimum at 0x0000011B25FDCD60&gt;,\n",
       "                     &#x27;mnx&#x27;: &lt;function mean_crossings at 0x0000011B25FDD1C0&gt;,\n",
       "                     &#x27;mse&#x27;: &lt;function mse at 0x0000011B25FDD120&gt;,\n",
       "                     &#x27;skew&#x27;: &lt;function skew at 0x0000011B25FDCEA0&gt;,\n",
       "                     &#x27;std&#x27;: &lt;function std at 0x0000011B25FDCAE0&gt;,\n",
       "                     &#x27;var&#x27;: &lt;function var at 0x0000011B25FDCB80&gt;})</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">XGBClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pype(steps=[('segment', Segment(overlap=None, step=10, width=20)),\n",
       "            ('features',\n",
       "             FeatureRep(features={'abs_energy': <function abs_energy at 0x0000011B25FDCA40>,\n",
       "                                  'kurt': <function kurt at 0x0000011B25FDCF40>,\n",
       "                                  'max': <function maximum at 0x0000011B25FDCE00>,\n",
       "                                  'mean': <function mean at 0x0000011B25FDC680>,\n",
       "                                  'median': <function median at 0x0000011B25FDC720>,\n",
       "                                  'min': <function minimum at 0x...\n",
       "                           gamma=None, grow_policy=None, importance_type=None,\n",
       "                           interaction_constraints=None, learning_rate=None,\n",
       "                           max_bin=None, max_cat_threshold=None,\n",
       "                           max_cat_to_onehot=None, max_delta_step=None,\n",
       "                           max_depth=None, max_leaves=None,\n",
       "                           min_child_weight=None, missing=nan,\n",
       "                           monotone_constraints=None, multi_strategy=None,\n",
       "                           n_estimators=200, n_jobs=None,\n",
       "                           num_parallel_tree=None, random_state=None, ...))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train_np, y_train_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ewaluacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność na zbiorze walidacyjnym: 0.43\n",
      "Dokładność na zbiorze testowym: 0.50\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = pipe.score(X_val_np, y_val_np)\n",
    "test_accuracy = pipe.score(X_test_np, y_test_np)\n",
    "\n",
    "print(f\"Dokładność na zbiorze walidacyjnym: {val_accuracy:.2f}\")\n",
    "print(f\"Dokładność na zbiorze testowym: {test_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
