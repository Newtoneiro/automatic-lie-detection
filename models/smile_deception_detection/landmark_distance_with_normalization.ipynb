{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "**********\n",
      "_CUDA version: \n",
      "CUDA version:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Oct_30_01:18:48_Pacific_Daylight_Time_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n",
      "\n",
      "**********\n",
      "CUDNN version: 90100\n",
      "Available GPU devices: 1\n",
      "Device Name: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "from utils import get_cuda_info\n",
    "\n",
    "get_cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybór optymalnych punktów charakterystycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "LANDMARK_INDEXES = np.load(os.path.join('..', '..', 'data', 'landmarks', 'combined_selected_points.npy'))\n",
    "REFERENCE_POINT_IDX = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja wybranych punktów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import visualize_landmarks\n",
    "\n",
    "visualize_landmarks(LANDMARK_INDEXES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zdobycie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, convert_landmarks_to_distances\n",
    "\n",
    "all_data, all_labels = load_data('nemo_smile')\n",
    "all_data = convert_landmarks_to_distances(all_data, LANDMARK_INDEXES, REFERENCE_POINT_IDX, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess_data\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(all_data, all_labels, binarize_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([868, 609, 154])\n",
      "torch.Size([868])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Class distribution <===\n",
      "0: 597\n",
      "1: 643\n",
      "=============><=============\n"
     ]
    }
   ],
   "source": [
    "from utils import get_class_distribution\n",
    "\n",
    "get_class_distribution(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W podejściu wykorzystane zostaną 2 modele - pierwszy z nich będzie siecią konwolucyjną 2d, która będzie miała za zadanie nauczyć się rozpoznawać cechy charakterystyczne dla wybranej klatki (zbioru współrzędnych pkt charakterystycznych). Do klasyfikacji szeregu czasowego zostanie wykorzystana sekwencyjna sieć neuronowa LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbudowanie modelu ekstrakcji cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LieClassifier(nn.Module):\n",
    "    def __init__(self, input_distances):\n",
    "        super(LieClassifier, self).__init__()\n",
    "        \n",
    "        # Spatial feature extraction (changed in_channels to 1)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Calculate LSTM input dimension\n",
    "        self.conv_output_size = 64 * (input_distances // 4)  # After two poolings\n",
    "        \n",
    "        # Temporal feature extraction\n",
    "        self.lstm = nn.LSTM(input_size=self.conv_output_size, hidden_size=128,\n",
    "                          batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Classification head\n",
    "        self.attn = nn.Linear(256, 1)\n",
    "        self.fc1 = nn.Linear(256, 64)  # 128*2 for bidirectional\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, frames, distances)\n",
    "        batch_size, frames, distances = x.shape\n",
    "        \n",
    "        # Reshape for Conv1D: (batch*frames, 1, distances)\n",
    "        x = x.view(-1, 1, distances)\n",
    "        \n",
    "        # Spatial features\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        x = x.view(batch_size, frames, -1)\n",
    "        \n",
    "        # Temporal features\n",
    "        x, _ = self.lstm(x)\n",
    "        attn_weights = torch.softmax(self.attn(x), dim=1)\n",
    "        x = (x * attn_weights).sum(dim=1)\n",
    "        \n",
    "        # Classification head (remove sigmoid!)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)  # Raw logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = LieClassifier(input_distances=len(LANDMARK_INDEXES))\n",
    "pos_weight = torch.tensor([(len(y_train) - y_train.sum()) / y_train.sum()]).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostyka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Debug Mode ===\n",
      "Input shape: torch.Size([32, 609, 154])\n",
      "Label distribution: 0.66 (1s)\n",
      "\n",
      "Step 0:\n",
      "Loss: 0.6271\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.4063 0.5719 0.6382 0.5509 0.5415]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Param conv1.weight: shape (32, 1, 3) | grad norm: 0.027005\n",
      "Param conv1.bias: shape (32,) | grad norm: 0.096207\n",
      "Param conv2.weight: shape (64, 32, 3) | grad norm: 0.136927\n",
      "Param conv2.bias: shape (64,) | grad norm: 0.124403\n",
      "Param lstm.weight_ih_l0: shape (512, 2432) | grad norm: 0.392871\n",
      "Param lstm.weight_hh_l0: shape (512, 128) | grad norm: 0.109606\n",
      "Param lstm.bias_ih_l0: shape (512,) | grad norm: 0.056534\n",
      "Param lstm.bias_hh_l0: shape (512,) | grad norm: 0.056534\n",
      "Param lstm.weight_ih_l0_reverse: shape (512, 2432) | grad norm: 0.457260\n",
      "Param lstm.weight_hh_l0_reverse: shape (512, 128) | grad norm: 0.135324\n",
      "Param lstm.bias_ih_l0_reverse: shape (512,) | grad norm: 0.066103\n",
      "Param lstm.bias_hh_l0_reverse: shape (512,) | grad norm: 0.066103\n",
      "Param attn.weight: shape (1, 256) | grad norm: 0.001243\n",
      "Param attn.bias: shape (1,) | grad norm: 0.000000\n",
      "Param fc1.weight: shape (64, 256) | grad norm: 0.305711\n",
      "Param fc1.bias: shape (64,) | grad norm: 0.108021\n",
      "Param fc2.weight: shape (1, 64) | grad norm: 0.151671\n",
      "Param fc2.bias: shape (1,) | grad norm: 0.068831\n",
      "Total gradient norm: 2.3604\n",
      "\n",
      "Step 2:\n",
      "Loss: 0.6160\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.5857 0.5397 0.6062 0.5896 0.5785]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 5.6353\n",
      "\n",
      "Step 4:\n",
      "Loss: 0.6386\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.4379 0.5928 0.4922 0.6475 0.5177]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 8.4143\n",
      "\n",
      "Step 6:\n",
      "Loss: 0.6246\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.4888 0.4653 0.6009 0.5302 0.5204]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 10.9816\n",
      "\n",
      "Step 8:\n",
      "Loss: 0.6414\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.5024 0.5331 0.5308 0.6064 0.5668]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 14.6034\n",
      "\n",
      "Step 10:\n",
      "Loss: 0.6500\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.4996 0.506  0.5117 0.5802 0.4317]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 18.7112\n",
      "\n",
      "Step 12:\n",
      "Loss: 0.6447\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.4349 0.4974 0.4183 0.4922 0.421 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 22.5468\n",
      "\n",
      "Step 14:\n",
      "Loss: 0.6420\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5387 0.6817 0.503  0.4975 0.5505]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 25.2296\n",
      "\n",
      "Step 16:\n",
      "Loss: 0.5872\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.5153 0.5486 0.5547 0.5597 0.5462]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 28.8524\n",
      "\n",
      "Step 18:\n",
      "Loss: 0.6350\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.5215 0.4653 0.5201 0.4178 0.4647]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 32.4125\n",
      "\n",
      "Step 20:\n",
      "Loss: 0.6447\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.4812 0.5408 0.488  0.6169 0.6233]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 36.1093\n",
      "\n",
      "Step 22:\n",
      "Loss: 0.6298\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.4418 0.6105 0.5232 0.6085 0.5417]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 39.5690\n",
      "\n",
      "Step 24:\n",
      "Loss: 0.5900\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.6464 0.5589 0.5277 0.4631 0.5643]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 42.0880\n",
      "\n",
      "Step 26:\n",
      "Loss: 0.6338\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.4568 0.5297 0.5127 0.5828 0.6417]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 44.6953\n",
      "\n",
      "Step 28:\n",
      "Loss: 0.5921\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.5423 0.5601 0.4381 0.6302 0.6859]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 47.4422\n",
      "\n",
      "Step 30:\n",
      "Loss: 0.6495\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.5499 0.5759 0.4737 0.5935 0.4623]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 50.4227\n",
      "\n",
      "Step 32:\n",
      "Loss: 0.6363\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.5096 0.6308 0.5273 0.663  0.5596]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 53.8756\n",
      "\n",
      "Step 34:\n",
      "Loss: 0.6048\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.6048 0.5496 0.5136 0.5341 0.6363]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 57.1839\n",
      "\n",
      "Step 36:\n",
      "Loss: 0.5710\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.5248 0.5182 0.6396 0.4766 0.6058]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 59.8448\n",
      "\n",
      "Step 38:\n",
      "Loss: 0.6640\n",
      "Accuracy: 46.88%\n",
      "Predictions (5 samples): [0.5439 0.6885 0.6277 0.5604 0.6012]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 63.2392\n",
      "\n",
      "Step 40:\n",
      "Loss: 0.6576\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.5639 0.5917 0.5098 0.6385 0.4389]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 65.9976\n",
      "\n",
      "Step 42:\n",
      "Loss: 0.6489\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.4897 0.3756 0.6311 0.6081 0.4411]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 70.0306\n",
      "\n",
      "Step 44:\n",
      "Loss: 0.6315\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.5652 0.4892 0.5534 0.6189 0.5368]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 73.2991\n",
      "\n",
      "Step 46:\n",
      "Loss: 0.6389\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.5521 0.5611 0.601  0.4363 0.408 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 76.7620\n",
      "\n",
      "Step 48:\n",
      "Loss: 0.6003\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.6348 0.6235 0.4882 0.5621 0.6739]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 78.7626\n",
      "\n",
      "Step 50:\n",
      "Loss: 0.6204\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5556 0.5517 0.5998 0.4791 0.613 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 82.2571\n",
      "\n",
      "Step 52:\n",
      "Loss: 0.6177\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.5668 0.552  0.5252 0.5579 0.5222]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 84.7921\n",
      "\n",
      "Step 54:\n",
      "Loss: 0.6220\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5444 0.5244 0.5632 0.5863 0.5455]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 88.0868\n",
      "\n",
      "Step 56:\n",
      "Loss: 0.6383\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.5135 0.5461 0.6064 0.4877 0.5972]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 91.6711\n",
      "\n",
      "Step 58:\n",
      "Loss: 0.6057\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.6722 0.6397 0.5183 0.5035 0.4365]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 94.7912\n",
      "\n",
      "Step 60:\n",
      "Loss: 0.6157\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.6637 0.5326 0.5524 0.6161 0.4858]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 97.7659\n",
      "\n",
      "Step 62:\n",
      "Loss: 0.6404\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.3877 0.6318 0.5296 0.4362 0.5939]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 101.0852\n",
      "\n",
      "Step 64:\n",
      "Loss: 0.6063\n",
      "Accuracy: 68.75%\n",
      "Predictions (5 samples): [0.4207 0.4279 0.4059 0.4603 0.55  ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 103.9771\n",
      "\n",
      "Step 66:\n",
      "Loss: 0.6133\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.5124 0.4835 0.5632 0.6181 0.6776]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 107.3826\n",
      "\n",
      "Step 68:\n",
      "Loss: 0.6052\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.51   0.5365 0.6535 0.4714 0.4768]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 110.9223\n",
      "\n",
      "Step 70:\n",
      "Loss: 0.6585\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.5237 0.5622 0.5455 0.5411 0.4042]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 113.8898\n",
      "\n",
      "Step 72:\n",
      "Loss: 0.6163\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.6261 0.5018 0.4479 0.5039 0.4351]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 117.7331\n",
      "\n",
      "Step 74:\n",
      "Loss: 0.6694\n",
      "Accuracy: 34.38%\n",
      "Predictions (5 samples): [0.5931 0.5145 0.5262 0.5177 0.5584]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 122.3508\n",
      "\n",
      "Step 76:\n",
      "Loss: 0.6027\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5465 0.5886 0.5082 0.5444 0.542 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 125.2607\n",
      "\n",
      "Step 78:\n",
      "Loss: 0.6070\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.5747 0.5185 0.5378 0.5616 0.6822]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 128.6904\n",
      "\n",
      "Step 80:\n",
      "Loss: 0.6530\n",
      "Accuracy: 43.75%\n",
      "Predictions (5 samples): [0.5561 0.5964 0.605  0.5933 0.464 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 133.2789\n",
      "\n",
      "Step 82:\n",
      "Loss: 0.6106\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.4501 0.4975 0.4638 0.6372 0.5345]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 136.2391\n",
      "\n",
      "Step 84:\n",
      "Loss: 0.5893\n",
      "Accuracy: 75.00%\n",
      "Predictions (5 samples): [0.4925 0.4615 0.6591 0.5226 0.5787]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 139.5192\n",
      "\n",
      "Step 86:\n",
      "Loss: 0.6036\n",
      "Accuracy: 68.75%\n",
      "Predictions (5 samples): [0.5979 0.4946 0.5994 0.4743 0.5392]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 142.7587\n",
      "\n",
      "Step 88:\n",
      "Loss: 0.6256\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.4464 0.5081 0.4677 0.5794 0.4954]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 146.2696\n",
      "\n",
      "Step 90:\n",
      "Loss: 0.6064\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.6562 0.5676 0.5505 0.5683 0.6025]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 150.1086\n",
      "\n",
      "Step 92:\n",
      "Loss: 0.6081\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.6432 0.5454 0.4353 0.5176 0.5654]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 153.0121\n",
      "\n",
      "Step 94:\n",
      "Loss: 0.6084\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.7112 0.5429 0.6255 0.5762 0.4696]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 156.4336\n",
      "\n",
      "Step 96:\n",
      "Loss: 0.6193\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.6239 0.495  0.5335 0.5668 0.569 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 159.6326\n",
      "\n",
      "Step 98:\n",
      "Loss: 0.6336\n",
      "Accuracy: 46.88%\n",
      "Predictions (5 samples): [0.4904 0.5839 0.5216 0.5137 0.6211]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 164.1660\n",
      "\n",
      "Step 100:\n",
      "Loss: 0.6284\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5354 0.4882 0.5326 0.5033 0.6391]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 167.2106\n",
      "\n",
      "Step 102:\n",
      "Loss: 0.6157\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.5336 0.6048 0.5983 0.552  0.4203]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 170.0697\n",
      "\n",
      "Step 104:\n",
      "Loss: 0.6349\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5806 0.61   0.5256 0.597  0.59  ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 173.2010\n",
      "\n",
      "Step 106:\n",
      "Loss: 0.6025\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.4775 0.4959 0.574  0.4989 0.5199]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 176.0585\n",
      "\n",
      "Step 108:\n",
      "Loss: 0.6448\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.5073 0.6027 0.6271 0.4627 0.4818]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 180.2529\n",
      "\n",
      "Step 110:\n",
      "Loss: 0.6024\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5778 0.525  0.5496 0.5904 0.6101]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 183.6491\n",
      "\n",
      "Step 112:\n",
      "Loss: 0.5994\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.4852 0.5082 0.4907 0.569  0.5701]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 186.9321\n",
      "\n",
      "Step 114:\n",
      "Loss: 0.6119\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.5805 0.459  0.627  0.5769 0.4831]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 190.4340\n",
      "\n",
      "Step 116:\n",
      "Loss: 0.6012\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.6675 0.5531 0.5744 0.5727 0.5771]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 193.0731\n",
      "\n",
      "Step 118:\n",
      "Loss: 0.6066\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.6189 0.6076 0.5844 0.4739 0.449 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 196.2693\n",
      "\n",
      "Step 120:\n",
      "Loss: 0.6486\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.4693 0.6114 0.436  0.4572 0.6423]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 199.5277\n",
      "\n",
      "Step 122:\n",
      "Loss: 0.6035\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.5099 0.5316 0.5982 0.4897 0.6054]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 203.4087\n",
      "\n",
      "Step 124:\n",
      "Loss: 0.6681\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.4074 0.4932 0.4277 0.5441 0.5836]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 206.6635\n",
      "\n",
      "Step 126:\n",
      "Loss: 0.5926\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.5454 0.5666 0.531  0.4901 0.4004]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 209.0146\n",
      "\n",
      "Step 128:\n",
      "Loss: 0.6133\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.6074 0.6055 0.5712 0.5915 0.5937]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 212.7831\n",
      "\n",
      "Step 130:\n",
      "Loss: 0.6450\n",
      "Accuracy: 46.88%\n",
      "Predictions (5 samples): [0.5815 0.5275 0.4748 0.5208 0.431 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 217.6504\n",
      "\n",
      "Step 132:\n",
      "Loss: 0.5991\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.4824 0.5269 0.4864 0.5738 0.5276]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 220.5655\n",
      "\n",
      "Step 134:\n",
      "Loss: 0.5781\n",
      "Accuracy: 68.75%\n",
      "Predictions (5 samples): [0.5963 0.5196 0.5953 0.4632 0.5635]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 223.5144\n",
      "\n",
      "Step 136:\n",
      "Loss: 0.6184\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5398 0.499  0.6242 0.4651 0.5819]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 226.8154\n",
      "\n",
      "Step 138:\n",
      "Loss: 0.6394\n",
      "Accuracy: 40.62%\n",
      "Predictions (5 samples): [0.472  0.5172 0.4784 0.5065 0.496 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 230.7283\n",
      "\n",
      "Step 140:\n",
      "Loss: 0.5747\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.4948 0.521  0.5395 0.4377 0.6086]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 234.3666\n",
      "\n",
      "Step 142:\n",
      "Loss: 0.6216\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.3645 0.4559 0.6254 0.6175 0.4302]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 238.0317\n",
      "\n",
      "Step 144:\n",
      "Loss: 0.6537\n",
      "Accuracy: 46.88%\n",
      "Predictions (5 samples): [0.4682 0.5887 0.5788 0.5431 0.4642]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 240.7975\n",
      "\n",
      "Step 146:\n",
      "Loss: 0.6188\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.4862 0.4865 0.5532 0.477  0.4728]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 244.8567\n",
      "\n",
      "Step 148:\n",
      "Loss: 0.6495\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.5053 0.5219 0.5504 0.469  0.435 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 247.7781\n",
      "\n",
      "Step 150:\n",
      "Loss: 0.6176\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.4497 0.5922 0.6445 0.5413 0.6155]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 251.6888\n",
      "\n",
      "Step 152:\n",
      "Loss: 0.5979\n",
      "Accuracy: 68.75%\n",
      "Predictions (5 samples): [0.567  0.4731 0.5896 0.5199 0.4662]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 255.0497\n",
      "\n",
      "Step 154:\n",
      "Loss: 0.6455\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.4419 0.4672 0.5138 0.6183 0.4289]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 257.9750\n",
      "\n",
      "Step 156:\n",
      "Loss: 0.6223\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.558  0.5068 0.6018 0.4954 0.5115]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 261.7389\n",
      "\n",
      "Step 158:\n",
      "Loss: 0.6335\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.4604 0.477  0.5283 0.4852 0.635 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 265.5281\n",
      "\n",
      "Step 160:\n",
      "Loss: 0.5937\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.6357 0.5338 0.5642 0.5202 0.5106]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 267.0982\n",
      "\n",
      "Step 162:\n",
      "Loss: 0.6477\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.4836 0.5686 0.5667 0.477  0.3943]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 270.3172\n",
      "\n",
      "Step 164:\n",
      "Loss: 0.5939\n",
      "Accuracy: 75.00%\n",
      "Predictions (5 samples): [0.6185 0.5487 0.4823 0.4667 0.5245]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 272.2035\n",
      "\n",
      "Step 166:\n",
      "Loss: 0.5870\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.5861 0.5726 0.6181 0.4156 0.666 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 275.4311\n",
      "\n",
      "Step 168:\n",
      "Loss: 0.6224\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.5909 0.5294 0.5313 0.5691 0.608 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 279.3405\n",
      "\n",
      "Step 170:\n",
      "Loss: 0.5833\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.544  0.5311 0.5467 0.5412 0.5872]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 282.3281\n",
      "\n",
      "Step 172:\n",
      "Loss: 0.6180\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5036 0.6445 0.4847 0.5286 0.5429]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 285.0331\n",
      "\n",
      "Step 174:\n",
      "Loss: 0.5962\n",
      "Accuracy: 75.00%\n",
      "Predictions (5 samples): [0.6663 0.4908 0.4873 0.6129 0.5199]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 288.3669\n",
      "\n",
      "Step 176:\n",
      "Loss: 0.6027\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.5594 0.5629 0.4697 0.4301 0.6504]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 291.8077\n",
      "\n",
      "Step 178:\n",
      "Loss: 0.6120\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5879 0.5717 0.5207 0.434  0.6041]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 295.0707\n",
      "\n",
      "Step 180:\n",
      "Loss: 0.6426\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.3818 0.5321 0.5704 0.5379 0.5693]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 297.9205\n",
      "\n",
      "Step 182:\n",
      "Loss: 0.5929\n",
      "Accuracy: 78.12%\n",
      "Predictions (5 samples): [0.5536 0.5792 0.5976 0.4243 0.537 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 300.5539\n",
      "\n",
      "Step 184:\n",
      "Loss: 0.6336\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.5701 0.5121 0.4571 0.5211 0.4847]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 303.9882\n",
      "\n",
      "Step 186:\n",
      "Loss: 0.6384\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5325 0.5622 0.4698 0.4483 0.4842]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 306.9000\n",
      "\n",
      "Step 188:\n",
      "Loss: 0.6099\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.4831 0.5298 0.5863 0.6353 0.5695]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 310.0671\n",
      "\n",
      "Step 190:\n",
      "Loss: 0.6465\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.6064 0.5641 0.5449 0.5077 0.4903]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 314.4549\n",
      "\n",
      "Step 192:\n",
      "Loss: 0.6099\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.4914 0.6111 0.5403 0.4179 0.5091]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 317.8676\n",
      "\n",
      "Step 194:\n",
      "Loss: 0.6247\n",
      "Accuracy: 46.88%\n",
      "Predictions (5 samples): [0.4991 0.58   0.5169 0.5275 0.4667]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 321.0659\n",
      "\n",
      "Step 196:\n",
      "Loss: 0.6402\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5916 0.5427 0.5385 0.6292 0.5374]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 324.9098\n",
      "\n",
      "Step 198:\n",
      "Loss: 0.6032\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.4345 0.5856 0.5378 0.4197 0.5425]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 328.1427\n",
      "\n",
      "Step 200:\n",
      "Loss: 0.6391\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.4289 0.5453 0.6122 0.4929 0.5067]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 331.6875\n",
      "\n",
      "Step 202:\n",
      "Loss: 0.5791\n",
      "Accuracy: 75.00%\n",
      "Predictions (5 samples): [0.6566 0.4607 0.6361 0.4647 0.5454]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 335.5286\n",
      "\n",
      "Step 204:\n",
      "Loss: 0.6119\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5875 0.6494 0.5694 0.4081 0.541 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 338.7308\n",
      "\n",
      "Step 206:\n",
      "Loss: 0.6206\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5952 0.5034 0.5116 0.5185 0.4455]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 342.6459\n",
      "\n",
      "Step 208:\n",
      "Loss: 0.6273\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.4451 0.5422 0.638  0.5948 0.4513]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 345.5690\n",
      "\n",
      "Step 210:\n",
      "Loss: 0.6099\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5724 0.5754 0.5459 0.4833 0.5177]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 349.0429\n",
      "\n",
      "Step 212:\n",
      "Loss: 0.5953\n",
      "Accuracy: 68.75%\n",
      "Predictions (5 samples): [0.5781 0.6697 0.6086 0.5299 0.5123]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 352.5171\n",
      "\n",
      "Step 214:\n",
      "Loss: 0.6126\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.4206 0.5419 0.4965 0.4978 0.6154]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 355.8037\n",
      "\n",
      "Step 216:\n",
      "Loss: 0.6256\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.5676 0.5778 0.537  0.5743 0.5859]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 359.3564\n",
      "\n",
      "Step 218:\n",
      "Loss: 0.6257\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5872 0.5309 0.5697 0.6012 0.5502]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 363.0937\n",
      "\n",
      "Step 220:\n",
      "Loss: 0.6464\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.5055 0.4607 0.4839 0.551  0.5737]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 365.9446\n",
      "\n",
      "Step 222:\n",
      "Loss: 0.5984\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.5229 0.5175 0.5224 0.4288 0.5219]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 369.8521\n",
      "\n",
      "Step 224:\n",
      "Loss: 0.6189\n",
      "Accuracy: 68.75%\n",
      "Predictions (5 samples): [0.5804 0.4923 0.5552 0.691  0.5403]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 372.6193\n",
      "\n",
      "Step 226:\n",
      "Loss: 0.6348\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5893 0.5813 0.5375 0.5762 0.5648]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 376.3383\n",
      "\n",
      "Step 228:\n",
      "Loss: 0.6233\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.558  0.5681 0.6575 0.5183 0.4625]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 379.8227\n",
      "\n",
      "Step 230:\n",
      "Loss: 0.5954\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.5827 0.4774 0.5149 0.4539 0.5342]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 382.5526\n",
      "\n",
      "Step 232:\n",
      "Loss: 0.6068\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.6104 0.61   0.4426 0.5234 0.4231]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 385.2628\n",
      "\n",
      "Step 234:\n",
      "Loss: 0.6359\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5094 0.6239 0.5404 0.4567 0.5376]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 388.2576\n",
      "\n",
      "Step 236:\n",
      "Loss: 0.6155\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.4933 0.5079 0.5271 0.5405 0.4876]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 391.9810\n",
      "\n",
      "Step 238:\n",
      "Loss: 0.5923\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.624  0.4792 0.4322 0.539  0.4387]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 395.0712\n",
      "\n",
      "Step 240:\n",
      "Loss: 0.6570\n",
      "Accuracy: 40.62%\n",
      "Predictions (5 samples): [0.5483 0.5752 0.4596 0.4462 0.5185]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 399.5915\n",
      "\n",
      "Step 242:\n",
      "Loss: 0.6418\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.5647 0.61   0.6576 0.5121 0.5853]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 403.9609\n",
      "\n",
      "Step 244:\n",
      "Loss: 0.6039\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.6348 0.5674 0.5754 0.6378 0.5087]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 407.5927\n",
      "\n",
      "Step 246:\n",
      "Loss: 0.5994\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.5238 0.4483 0.4916 0.5852 0.5871]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 410.3787\n",
      "\n",
      "Step 248:\n",
      "Loss: 0.6216\n",
      "Accuracy: 46.88%\n",
      "Predictions (5 samples): [0.5276 0.573  0.4857 0.5108 0.4812]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 414.1854\n",
      "\n",
      "Step 250:\n",
      "Loss: 0.6231\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.6321 0.5229 0.4453 0.462  0.6425]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 417.2776\n",
      "\n",
      "Step 252:\n",
      "Loss: 0.6037\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.6237 0.6513 0.6761 0.4995 0.5023]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 421.7206\n",
      "\n",
      "Step 254:\n",
      "Loss: 0.6188\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.4845 0.6319 0.5442 0.6181 0.6308]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 424.4825\n",
      "\n",
      "Step 256:\n",
      "Loss: 0.6247\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5263 0.5402 0.4679 0.632  0.5256]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 428.7531\n",
      "\n",
      "Step 258:\n",
      "Loss: 0.6002\n",
      "Accuracy: 65.62%\n",
      "Predictions (5 samples): [0.5662 0.5129 0.5859 0.4945 0.5712]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 431.7965\n",
      "\n",
      "Step 260:\n",
      "Loss: 0.6415\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5074 0.6007 0.6287 0.6183 0.5412]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 434.9910\n",
      "\n",
      "Step 262:\n",
      "Loss: 0.6381\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.5249 0.4823 0.5029 0.5711 0.5951]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 438.3473\n",
      "\n",
      "Step 264:\n",
      "Loss: 0.6217\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5439 0.4309 0.5832 0.5832 0.5328]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 441.9034\n",
      "\n",
      "Step 266:\n",
      "Loss: 0.6102\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.5524 0.5705 0.6159 0.4751 0.522 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 445.2308\n",
      "\n",
      "Step 268:\n",
      "Loss: 0.6441\n",
      "Accuracy: 50.00%\n",
      "Predictions (5 samples): [0.5244 0.6182 0.4526 0.5839 0.541 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 448.2957\n",
      "\n",
      "Step 270:\n",
      "Loss: 0.6487\n",
      "Accuracy: 46.88%\n",
      "Predictions (5 samples): [0.4692 0.6366 0.6276 0.5344 0.6593]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 452.2228\n",
      "\n",
      "Step 272:\n",
      "Loss: 0.6001\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.5253 0.61   0.62   0.5162 0.619 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 456.5167\n",
      "\n",
      "Step 274:\n",
      "Loss: 0.6097\n",
      "Accuracy: 62.50%\n",
      "Predictions (5 samples): [0.4573 0.4833 0.4736 0.5381 0.598 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 459.5874\n",
      "\n",
      "Step 276:\n",
      "Loss: 0.6172\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.5825 0.6301 0.4452 0.4898 0.5822]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 462.8421\n",
      "\n",
      "Step 278:\n",
      "Loss: 0.5802\n",
      "Accuracy: 68.75%\n",
      "Predictions (5 samples): [0.537  0.5019 0.5634 0.4957 0.5246]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 465.6557\n",
      "\n",
      "Step 280:\n",
      "Loss: 0.6148\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.6224 0.5836 0.5657 0.5652 0.5045]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 468.0619\n",
      "\n",
      "Step 282:\n",
      "Loss: 0.6169\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.6746 0.5561 0.4924 0.6286 0.548 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 470.2298\n",
      "\n",
      "Step 284:\n",
      "Loss: 0.5844\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.5394 0.4705 0.4603 0.5186 0.531 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 473.4715\n",
      "\n",
      "Step 286:\n",
      "Loss: 0.5766\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.5666 0.4828 0.461  0.4717 0.5928]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 477.0766\n",
      "\n",
      "Step 288:\n",
      "Loss: 0.6082\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.6239 0.5344 0.5211 0.5384 0.556 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 480.2519\n",
      "\n",
      "Step 290:\n",
      "Loss: 0.6095\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.6713 0.5402 0.6296 0.5029 0.5068]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 483.1225\n",
      "\n",
      "Step 292:\n",
      "Loss: 0.6194\n",
      "Accuracy: 53.12%\n",
      "Predictions (5 samples): [0.4414 0.5077 0.5492 0.5065 0.4491]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 485.2683\n",
      "\n",
      "Step 294:\n",
      "Loss: 0.6014\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.6145 0.5384 0.536  0.5151 0.6101]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 489.0879\n",
      "\n",
      "Step 296:\n",
      "Loss: 0.6161\n",
      "Accuracy: 68.75%\n",
      "Predictions (5 samples): [0.5209 0.4792 0.4892 0.4872 0.534 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 492.2576\n",
      "\n",
      "Step 298:\n",
      "Loss: 0.6444\n",
      "Accuracy: 46.88%\n",
      "Predictions (5 samples): [0.5953 0.5129 0.5146 0.5481 0.501 ]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 494.6225\n",
      "\n",
      "Step 300:\n",
      "Loss: 0.6557\n",
      "Accuracy: 59.38%\n",
      "Predictions (5 samples): [0.4802 0.6862 0.5489 0.5415 0.6752]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 497.6234\n",
      "\n",
      "Step 302:\n",
      "Loss: 0.6159\n",
      "Accuracy: 71.88%\n",
      "Predictions (5 samples): [0.5501 0.6349 0.5206 0.5378 0.5217]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 500.6235\n",
      "\n",
      "Step 304:\n",
      "Loss: 0.6386\n",
      "Accuracy: 56.25%\n",
      "Predictions (5 samples): [0.6051 0.5327 0.5593 0.5962 0.5948]\n",
      "Labels (5 samples): [1. 0. 1. 0. 1.]\n",
      "Total gradient norm: 503.8563\n",
      "\n",
      "Step 306:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m criterion_diag \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(pos_weight\u001b[38;5;241m=\u001b[39mpos_weight)\n\u001b[0;32m      5\u001b[0m optimizer_diag \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43moverfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_diag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_diag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_diag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_treshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bartosz\\Desktop\\automatic-lie-detection\\models\\utils\\model_functions\\train_model.py:204\u001b[0m, in \u001b[0;36moverfit_model\u001b[1;34m(model, criterion, optimizer, X_train, y_train, batch_size, prediction_treshold)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m999\u001b[39m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions (5 samples): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobs[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.model_functions import overfit_model\n",
    "\n",
    "model_diag = LieClassifier(input_distances=len(LANDMARK_INDEXES))\n",
    "criterion_diag = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer_diag = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "overfit_model(model_diag, criterion_diag, optimizer_diag, X_train, y_train, prediction_treshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "RUNS_FOLDER_PATH = os.path.abspath('runs')\n",
    "writer_path = os.path.join('runs', 'torch_lstm', 'lie_classifier_landmark_distance_normalized')\n",
    "writer = SummaryWriter(writer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Train Loss: 17.67683303, Train Acc: 0.55299539, Val Loss: 3.50655770, Val Acc: 0.67204301\n",
      "Epoch 2/300, Train Loss: 15.55383250, Train Acc: 0.69239631, Val Loss: 3.28689194, Val Acc: 0.73118280\n",
      "Epoch 3/300, Train Loss: 15.42053059, Train Acc: 0.67396313, Val Loss: 3.48408949, Val Acc: 0.67741935\n",
      "Epoch 4/300, Train Loss: 15.06688705, Train Acc: 0.71082949, Val Loss: 3.35401917, Val Acc: 0.70430108\n",
      "Epoch 5/300, Train Loss: 14.98503661, Train Acc: 0.73041475, Val Loss: 3.17962787, Val Acc: 0.73118280\n",
      "Epoch 6/300, Train Loss: 14.78154027, Train Acc: 0.73502304, Val Loss: 3.17293599, Val Acc: 0.66666667\n",
      "Epoch 7/300, Train Loss: 14.53898847, Train Acc: 0.73387097, Val Loss: 3.18807545, Val Acc: 0.65591398\n",
      "Epoch 8/300, Train Loss: 13.87881291, Train Acc: 0.76267281, Val Loss: 3.10582122, Val Acc: 0.72580645\n",
      "Epoch 9/300, Train Loss: 13.42370999, Train Acc: 0.75806452, Val Loss: 3.28144169, Val Acc: 0.62903226\n",
      "Epoch 10/300, Train Loss: 14.20917526, Train Acc: 0.74539171, Val Loss: 3.09425366, Val Acc: 0.72580645\n",
      "Epoch 11/300, Train Loss: 13.99501199, Train Acc: 0.73847926, Val Loss: 3.26116604, Val Acc: 0.72580645\n",
      "Epoch 12/300, Train Loss: 13.32460344, Train Acc: 0.76152074, Val Loss: 3.08557987, Val Acc: 0.72043011\n",
      "Epoch 13/300, Train Loss: 13.39423431, Train Acc: 0.76267281, Val Loss: 3.08029163, Val Acc: 0.73118280\n",
      "Epoch 14/300, Train Loss: 13.84097081, Train Acc: 0.73502304, Val Loss: 3.27291000, Val Acc: 0.63440860\n",
      "Epoch 15/300, Train Loss: 14.46362552, Train Acc: 0.72695853, Val Loss: 3.12673080, Val Acc: 0.72043011\n",
      "Epoch 16/300, Train Loss: 12.90321887, Train Acc: 0.77995392, Val Loss: 3.13695309, Val Acc: 0.72043011\n",
      "Epoch 17/300, Train Loss: 13.18580189, Train Acc: 0.76728111, Val Loss: 3.15435886, Val Acc: 0.69354839\n",
      "Epoch 18/300, Train Loss: 13.76186493, Train Acc: 0.76267281, Val Loss: 3.10927072, Val Acc: 0.69892473\n",
      "Epoch 19/300, Train Loss: 13.63769075, Train Acc: 0.76036866, Val Loss: 3.09999061, Val Acc: 0.71505376\n",
      "Epoch 20/300, Train Loss: 14.07836840, Train Acc: 0.75806452, Val Loss: 3.12435901, Val Acc: 0.72043011\n",
      "Epoch 21/300, Train Loss: 14.45019573, Train Acc: 0.73041475, Val Loss: 3.07324150, Val Acc: 0.72043011\n",
      "Epoch 22/300, Train Loss: 14.09319723, Train Acc: 0.74539171, Val Loss: 3.05328929, Val Acc: 0.73655914\n",
      "Epoch 23/300, Train Loss: 14.25762588, Train Acc: 0.72235023, Val Loss: 3.05333138, Val Acc: 0.73655914\n",
      "Epoch 24/300, Train Loss: 13.12820745, Train Acc: 0.78686636, Val Loss: 3.06316227, Val Acc: 0.72043011\n",
      "Epoch 25/300, Train Loss: 13.55028903, Train Acc: 0.73847926, Val Loss: 3.10671121, Val Acc: 0.72580645\n",
      "Epoch 26/300, Train Loss: 13.64914948, Train Acc: 0.75460829, Val Loss: 3.04085958, Val Acc: 0.71505376\n",
      "Epoch 27/300, Train Loss: 13.40648934, Train Acc: 0.76267281, Val Loss: 3.05700296, Val Acc: 0.71505376\n",
      "Epoch 28/300, Train Loss: 13.61984214, Train Acc: 0.76497696, Val Loss: 3.04449520, Val Acc: 0.72043011\n",
      "Epoch 29/300, Train Loss: 14.94280440, Train Acc: 0.71082949, Val Loss: 3.18402010, Val Acc: 0.71505376\n",
      "Epoch 30/300, Train Loss: 12.97421724, Train Acc: 0.77188940, Val Loss: 3.12140867, Val Acc: 0.69354839\n",
      "Epoch 31/300, Train Loss: 13.22968519, Train Acc: 0.76843318, Val Loss: 3.07018644, Val Acc: 0.71505376\n",
      "Epoch 32/300, Train Loss: 12.76088238, Train Acc: 0.77419355, Val Loss: 3.04458675, Val Acc: 0.70430108\n",
      "Epoch 33/300, Train Loss: 13.57399771, Train Acc: 0.76497696, Val Loss: 3.09736508, Val Acc: 0.70967742\n",
      "Epoch 34/300, Train Loss: 13.45850366, Train Acc: 0.75230415, Val Loss: 3.02699149, Val Acc: 0.71505376\n",
      "Epoch 35/300, Train Loss: 14.29109916, Train Acc: 0.74539171, Val Loss: 3.02723521, Val Acc: 0.72043011\n",
      "Epoch 36/300, Train Loss: 13.57390428, Train Acc: 0.74884793, Val Loss: 3.02534637, Val Acc: 0.71505376\n",
      "Epoch 37/300, Train Loss: 13.68034554, Train Acc: 0.75806452, Val Loss: 3.05580944, Val Acc: 0.70967742\n",
      "Epoch 38/300, Train Loss: 14.99107397, Train Acc: 0.72465438, Val Loss: 3.03257701, Val Acc: 0.70967742\n",
      "Epoch 39/300, Train Loss: 13.88673982, Train Acc: 0.75345622, Val Loss: 3.06718695, Val Acc: 0.70967742\n",
      "Epoch 40/300, Train Loss: 13.95963055, Train Acc: 0.75115207, Val Loss: 3.03142890, Val Acc: 0.71505376\n",
      "Epoch 41/300, Train Loss: 14.29507020, Train Acc: 0.73617512, Val Loss: 3.06403989, Val Acc: 0.71505376\n",
      "Epoch 42/300, Train Loss: 13.45211947, Train Acc: 0.76036866, Val Loss: 3.02732202, Val Acc: 0.70967742\n",
      "Epoch 43/300, Train Loss: 12.96077889, Train Acc: 0.77419355, Val Loss: 3.11111772, Val Acc: 0.73655914\n",
      "Epoch 44/300, Train Loss: 14.60036060, Train Acc: 0.72695853, Val Loss: 3.07230887, Val Acc: 0.72043011\n",
      "Epoch 45/300, Train Loss: 14.03256759, Train Acc: 0.74078341, Val Loss: 3.06320471, Val Acc: 0.72580645\n",
      "Epoch 46/300, Train Loss: 13.26102796, Train Acc: 0.77649770, Val Loss: 3.02225298, Val Acc: 0.72043011\n",
      "Epoch 47/300, Train Loss: 13.32590090, Train Acc: 0.75921659, Val Loss: 3.02829403, Val Acc: 0.72043011\n",
      "Epoch 48/300, Train Loss: 13.22196004, Train Acc: 0.78341014, Val Loss: 3.05239713, Val Acc: 0.71505376\n",
      "Epoch 49/300, Train Loss: 14.01644361, Train Acc: 0.75691244, Val Loss: 3.05063796, Val Acc: 0.72043011\n",
      "Epoch 50/300, Train Loss: 13.97073492, Train Acc: 0.74769585, Val Loss: 3.01506609, Val Acc: 0.70967742\n",
      "Epoch 51/300, Train Loss: 13.04582086, Train Acc: 0.76267281, Val Loss: 3.05941942, Val Acc: 0.71505376\n",
      "Epoch 52/300, Train Loss: 13.88736650, Train Acc: 0.75230415, Val Loss: 3.02140212, Val Acc: 0.70967742\n",
      "Epoch 53/300, Train Loss: 13.22365612, Train Acc: 0.76612903, Val Loss: 3.22582969, Val Acc: 0.73118280\n",
      "Epoch 54/300, Train Loss: 14.54264444, Train Acc: 0.73847926, Val Loss: 3.03557000, Val Acc: 0.70430108\n",
      "Epoch 55/300, Train Loss: 13.89256516, Train Acc: 0.74769585, Val Loss: 3.04219663, Val Acc: 0.71505376\n",
      "Epoch 56/300, Train Loss: 13.38845164, Train Acc: 0.76267281, Val Loss: 3.03034621, Val Acc: 0.70967742\n",
      "Epoch 57/300, Train Loss: 13.95815131, Train Acc: 0.74769585, Val Loss: 3.02602664, Val Acc: 0.69354839\n",
      "Epoch 58/300, Train Loss: 13.28258494, Train Acc: 0.77419355, Val Loss: 3.02358946, Val Acc: 0.70967742\n",
      "Epoch 59/300, Train Loss: 13.17034599, Train Acc: 0.78341014, Val Loss: 3.07457027, Val Acc: 0.69892473\n",
      "Epoch 60/300, Train Loss: 13.11690369, Train Acc: 0.76497696, Val Loss: 3.03557360, Val Acc: 0.72580645\n",
      "Epoch 61/300, Train Loss: 13.12463585, Train Acc: 0.77534562, Val Loss: 3.04398519, Val Acc: 0.70430108\n",
      "Epoch 62/300, Train Loss: 12.99897021, Train Acc: 0.76267281, Val Loss: 3.03138322, Val Acc: 0.70967742\n",
      "Epoch 63/300, Train Loss: 13.85185456, Train Acc: 0.75806452, Val Loss: 3.07615224, Val Acc: 0.69892473\n",
      "Epoch 64/300, Train Loss: 13.62841117, Train Acc: 0.75691244, Val Loss: 3.02170095, Val Acc: 0.72580645\n",
      "Epoch 65/300, Train Loss: 13.01523024, Train Acc: 0.76728111, Val Loss: 3.03302217, Val Acc: 0.70967742\n",
      "Epoch 66/300, Train Loss: 13.28292072, Train Acc: 0.77995392, Val Loss: 3.02956539, Val Acc: 0.70430108\n",
      "Epoch 67/300, Train Loss: 13.51525578, Train Acc: 0.76382488, Val Loss: 3.01140529, Val Acc: 0.72043011\n",
      "Epoch 68/300, Train Loss: 12.74292389, Train Acc: 0.77534562, Val Loss: 3.02840269, Val Acc: 0.71505376\n",
      "Epoch 69/300, Train Loss: 13.46467337, Train Acc: 0.79723502, Val Loss: 3.04150054, Val Acc: 0.72043011\n",
      "Epoch 70/300, Train Loss: 14.22175193, Train Acc: 0.72119816, Val Loss: 3.06416160, Val Acc: 0.70967742\n",
      "Epoch 71/300, Train Loss: 13.68061656, Train Acc: 0.74654378, Val Loss: 3.03334281, Val Acc: 0.72043011\n",
      "Epoch 72/300, Train Loss: 13.83252040, Train Acc: 0.74884793, Val Loss: 3.13829276, Val Acc: 0.67741935\n",
      "Epoch 73/300, Train Loss: 13.59288877, Train Acc: 0.75691244, Val Loss: 3.23828411, Val Acc: 0.67741935\n",
      "Epoch 74/300, Train Loss: 13.61410549, Train Acc: 0.74769585, Val Loss: 3.03245509, Val Acc: 0.70430108\n",
      "Epoch 75/300, Train Loss: 13.20021886, Train Acc: 0.76036866, Val Loss: 3.01882184, Val Acc: 0.72043011\n",
      "Epoch 76/300, Train Loss: 13.21034786, Train Acc: 0.76267281, Val Loss: 3.02699387, Val Acc: 0.71505376\n",
      "Epoch 77/300, Train Loss: 12.90516990, Train Acc: 0.77419355, Val Loss: 3.04138970, Val Acc: 0.71505376\n",
      "Epoch 78/300, Train Loss: 13.29201299, Train Acc: 0.75460829, Val Loss: 3.02523288, Val Acc: 0.72580645\n",
      "Epoch 79/300, Train Loss: 13.36573529, Train Acc: 0.76958525, Val Loss: 3.02605537, Val Acc: 0.72043011\n",
      "Epoch 80/300, Train Loss: 13.16357762, Train Acc: 0.76843318, Val Loss: 3.01932952, Val Acc: 0.72580645\n",
      "Epoch 81/300, Train Loss: 13.19892144, Train Acc: 0.77764977, Val Loss: 3.03879088, Val Acc: 0.71505376\n",
      "Epoch 82/300, Train Loss: 11.81145421, Train Acc: 0.79377880, Val Loss: 3.08715591, Val Acc: 0.70967742\n",
      "Epoch 83/300, Train Loss: 13.38936403, Train Acc: 0.75921659, Val Loss: 3.14500910, Val Acc: 0.70430108\n",
      "Epoch 84/300, Train Loss: 13.02559304, Train Acc: 0.76497696, Val Loss: 3.09303564, Val Acc: 0.70967742\n",
      "Epoch 85/300, Train Loss: 13.79486820, Train Acc: 0.76497696, Val Loss: 3.02141052, Val Acc: 0.70967742\n",
      "Epoch 86/300, Train Loss: 13.63648680, Train Acc: 0.74308756, Val Loss: 3.03802010, Val Acc: 0.70967742\n",
      "Epoch 87/300, Train Loss: 13.66134983, Train Acc: 0.75806452, Val Loss: 3.01781452, Val Acc: 0.71505376\n",
      "Epoch 88/300, Train Loss: 13.02700460, Train Acc: 0.77995392, Val Loss: 3.02680239, Val Acc: 0.72043011\n",
      "Epoch 89/300, Train Loss: 13.56922665, Train Acc: 0.74884793, Val Loss: 3.10980412, Val Acc: 0.73118280\n",
      "Epoch 90/300, Train Loss: 13.38000964, Train Acc: 0.76382488, Val Loss: 3.02805123, Val Acc: 0.70967742\n",
      "Epoch 91/300, Train Loss: 12.93719828, Train Acc: 0.79262673, Val Loss: 3.02731669, Val Acc: 0.71505376\n",
      "Epoch 92/300, Train Loss: 13.60310438, Train Acc: 0.75345622, Val Loss: 3.02335399, Val Acc: 0.72043011\n",
      "Epoch 93/300, Train Loss: 12.92783460, Train Acc: 0.78571429, Val Loss: 3.10134014, Val Acc: 0.71505376\n",
      "Epoch 94/300, Train Loss: 13.26490289, Train Acc: 0.76497696, Val Loss: 3.02623922, Val Acc: 0.72043011\n",
      "Epoch 95/300, Train Loss: 12.74264863, Train Acc: 0.78341014, Val Loss: 3.03789890, Val Acc: 0.70967742\n",
      "Epoch 96/300, Train Loss: 13.47084287, Train Acc: 0.75345622, Val Loss: 3.04842415, Val Acc: 0.70430108\n",
      "Epoch 97/300, Train Loss: 13.44202709, Train Acc: 0.77188940, Val Loss: 3.05824241, Val Acc: 0.72580645\n",
      "Epoch 98/300, Train Loss: 13.10576347, Train Acc: 0.78456221, Val Loss: 3.04353929, Val Acc: 0.70967742\n",
      "Epoch 99/300, Train Loss: 13.21445712, Train Acc: 0.77534562, Val Loss: 2.99343866, Val Acc: 0.72043011\n",
      "Epoch 100/300, Train Loss: 13.39101684, Train Acc: 0.78686636, Val Loss: 3.07257769, Val Acc: 0.72043011\n",
      "Epoch 101/300, Train Loss: 13.40746433, Train Acc: 0.76728111, Val Loss: 2.99573743, Val Acc: 0.71505376\n",
      "Epoch 102/300, Train Loss: 13.48425514, Train Acc: 0.74769585, Val Loss: 2.97719502, Val Acc: 0.71505376\n",
      "Epoch 103/300, Train Loss: 12.87051353, Train Acc: 0.75806452, Val Loss: 2.98139533, Val Acc: 0.72043011\n",
      "Epoch 104/300, Train Loss: 12.98725110, Train Acc: 0.77534562, Val Loss: 3.01194066, Val Acc: 0.72043011\n",
      "Epoch 105/300, Train Loss: 13.15211704, Train Acc: 0.76267281, Val Loss: 2.99205655, Val Acc: 0.72580645\n",
      "Epoch 106/300, Train Loss: 13.02267680, Train Acc: 0.77073733, Val Loss: 2.99798226, Val Acc: 0.73118280\n",
      "Epoch 107/300, Train Loss: 12.66311219, Train Acc: 0.78801843, Val Loss: 3.06019223, Val Acc: 0.69892473\n",
      "Epoch 108/300, Train Loss: 12.83659983, Train Acc: 0.77304147, Val Loss: 3.03987643, Val Acc: 0.72580645\n",
      "Epoch 109/300, Train Loss: 12.05262944, Train Acc: 0.78341014, Val Loss: 3.01580438, Val Acc: 0.72043011\n",
      "Epoch 110/300, Train Loss: 12.53094518, Train Acc: 0.76612903, Val Loss: 3.01426584, Val Acc: 0.72580645\n",
      "Epoch 111/300, Train Loss: 12.28311180, Train Acc: 0.79147465, Val Loss: 2.99142379, Val Acc: 0.71505376\n",
      "Epoch 112/300, Train Loss: 12.25704014, Train Acc: 0.78686636, Val Loss: 3.12035972, Val Acc: 0.71505376\n",
      "Epoch 113/300, Train Loss: 13.71599215, Train Acc: 0.73617512, Val Loss: 3.08939680, Val Acc: 0.70967742\n",
      "Epoch 114/300, Train Loss: 12.42957473, Train Acc: 0.78686636, Val Loss: 3.01411480, Val Acc: 0.73118280\n",
      "Epoch 115/300, Train Loss: 12.73069790, Train Acc: 0.75576037, Val Loss: 3.02699590, Val Acc: 0.72580645\n",
      "Epoch 116/300, Train Loss: 12.63877481, Train Acc: 0.77649770, Val Loss: 3.01842788, Val Acc: 0.71505376\n",
      "Epoch 117/300, Train Loss: 12.73222841, Train Acc: 0.77649770, Val Loss: 2.99143425, Val Acc: 0.72043011\n",
      "Epoch 118/300, Train Loss: 13.05092630, Train Acc: 0.76382488, Val Loss: 3.00427204, Val Acc: 0.73655914\n",
      "Epoch 119/300, Train Loss: 12.09369096, Train Acc: 0.79493088, Val Loss: 3.00980210, Val Acc: 0.72043011\n",
      "Epoch 120/300, Train Loss: 12.98276958, Train Acc: 0.78686636, Val Loss: 2.99419272, Val Acc: 0.71505376\n",
      "Epoch 121/300, Train Loss: 12.58851469, Train Acc: 0.77419355, Val Loss: 2.99187887, Val Acc: 0.72043011\n",
      "Epoch 122/300, Train Loss: 13.03200099, Train Acc: 0.76612903, Val Loss: 2.97446772, Val Acc: 0.72043011\n",
      "Epoch 123/300, Train Loss: 13.14813730, Train Acc: 0.76612903, Val Loss: 2.98093310, Val Acc: 0.72043011\n",
      "Epoch 124/300, Train Loss: 12.89514220, Train Acc: 0.75345622, Val Loss: 3.08580419, Val Acc: 0.71505376\n",
      "Epoch 125/300, Train Loss: 13.15323707, Train Acc: 0.76152074, Val Loss: 2.98461157, Val Acc: 0.72580645\n",
      "Epoch 126/300, Train Loss: 13.06792966, Train Acc: 0.78341014, Val Loss: 3.07531905, Val Acc: 0.71505376\n",
      "Epoch 127/300, Train Loss: 13.13959843, Train Acc: 0.77304147, Val Loss: 2.98386550, Val Acc: 0.73118280\n",
      "Epoch 128/300, Train Loss: 12.66268530, Train Acc: 0.77995392, Val Loss: 3.01441190, Val Acc: 0.72043011\n",
      "Epoch 129/300, Train Loss: 12.46464661, Train Acc: 0.79262673, Val Loss: 3.00490722, Val Acc: 0.73655914\n",
      "Epoch 130/300, Train Loss: 12.94467312, Train Acc: 0.76267281, Val Loss: 2.96648124, Val Acc: 0.72043011\n",
      "Epoch 131/300, Train Loss: 12.38313422, Train Acc: 0.77649770, Val Loss: 3.02589667, Val Acc: 0.73118280\n",
      "Epoch 132/300, Train Loss: 12.70725790, Train Acc: 0.79262673, Val Loss: 3.02718851, Val Acc: 0.73118280\n",
      "Epoch 133/300, Train Loss: 12.74449903, Train Acc: 0.77073733, Val Loss: 2.97861096, Val Acc: 0.73118280\n",
      "Epoch 134/300, Train Loss: 12.90072981, Train Acc: 0.77073733, Val Loss: 3.01556376, Val Acc: 0.72580645\n",
      "Epoch 135/300, Train Loss: 13.12524518, Train Acc: 0.76958525, Val Loss: 2.99561685, Val Acc: 0.73655914\n",
      "Epoch 136/300, Train Loss: 13.00344381, Train Acc: 0.76958525, Val Loss: 3.07931444, Val Acc: 0.72580645\n",
      "Epoch 137/300, Train Loss: 12.90092710, Train Acc: 0.77649770, Val Loss: 2.99309847, Val Acc: 0.73655914\n",
      "Epoch 138/300, Train Loss: 13.04344645, Train Acc: 0.78917051, Val Loss: 2.93539941, Val Acc: 0.72580645\n",
      "Epoch 139/300, Train Loss: 12.87453845, Train Acc: 0.77188940, Val Loss: 2.95795646, Val Acc: 0.72580645\n",
      "Epoch 140/300, Train Loss: 12.98685801, Train Acc: 0.76843318, Val Loss: 2.97253034, Val Acc: 0.72580645\n",
      "Epoch 141/300, Train Loss: 12.66055077, Train Acc: 0.76497696, Val Loss: 2.99716046, Val Acc: 0.73655914\n",
      "Epoch 142/300, Train Loss: 13.30915904, Train Acc: 0.75230415, Val Loss: 3.05608505, Val Acc: 0.70967742\n",
      "Epoch 143/300, Train Loss: 12.63215625, Train Acc: 0.76267281, Val Loss: 2.94583240, Val Acc: 0.73655914\n",
      "Epoch 144/300, Train Loss: 12.39395273, Train Acc: 0.79608295, Val Loss: 2.97138306, Val Acc: 0.73655914\n",
      "Epoch 145/300, Train Loss: 11.92969267, Train Acc: 0.78110599, Val Loss: 2.93419337, Val Acc: 0.72580645\n",
      "Epoch 146/300, Train Loss: 12.63126373, Train Acc: 0.78686636, Val Loss: 2.98239040, Val Acc: 0.70967742\n",
      "Epoch 147/300, Train Loss: 12.47280630, Train Acc: 0.79493088, Val Loss: 3.02787668, Val Acc: 0.70430108\n",
      "Epoch 148/300, Train Loss: 13.48255509, Train Acc: 0.76728111, Val Loss: 2.97876912, Val Acc: 0.71505376\n",
      "Epoch 149/300, Train Loss: 12.17412001, Train Acc: 0.79147465, Val Loss: 2.93048367, Val Acc: 0.73655914\n",
      "Epoch 150/300, Train Loss: 12.94951269, Train Acc: 0.76497696, Val Loss: 2.98462108, Val Acc: 0.73118280\n",
      "Epoch 151/300, Train Loss: 12.57070729, Train Acc: 0.77534562, Val Loss: 2.94407168, Val Acc: 0.72043011\n",
      "Epoch 152/300, Train Loss: 12.64666086, Train Acc: 0.77073733, Val Loss: 2.90105125, Val Acc: 0.74731183\n",
      "Epoch 153/300, Train Loss: 11.84088588, Train Acc: 0.79608295, Val Loss: 2.90491503, Val Acc: 0.73118280\n",
      "Epoch 154/300, Train Loss: 12.01450956, Train Acc: 0.77534562, Val Loss: 2.92906806, Val Acc: 0.74193548\n",
      "Epoch 155/300, Train Loss: 12.57267514, Train Acc: 0.77995392, Val Loss: 2.90479529, Val Acc: 0.74193548\n",
      "Epoch 156/300, Train Loss: 11.57860722, Train Acc: 0.79032258, Val Loss: 2.95392245, Val Acc: 0.71505376\n",
      "Epoch 157/300, Train Loss: 12.92814627, Train Acc: 0.77188940, Val Loss: 2.98510060, Val Acc: 0.73118280\n",
      "Epoch 158/300, Train Loss: 13.72843498, Train Acc: 0.74308756, Val Loss: 3.08274916, Val Acc: 0.72043011\n",
      "Epoch 159/300, Train Loss: 12.78014043, Train Acc: 0.77534562, Val Loss: 2.99075934, Val Acc: 0.71505376\n",
      "Epoch 160/300, Train Loss: 12.83634454, Train Acc: 0.77073733, Val Loss: 3.06701332, Val Acc: 0.72580645\n",
      "Epoch 161/300, Train Loss: 13.77516815, Train Acc: 0.75691244, Val Loss: 3.03310961, Val Acc: 0.71505376\n",
      "Epoch 162/300, Train Loss: 13.29020134, Train Acc: 0.73617512, Val Loss: 2.97379673, Val Acc: 0.72043011\n",
      "Epoch 163/300, Train Loss: 13.05569842, Train Acc: 0.76036866, Val Loss: 2.95404011, Val Acc: 0.72043011\n",
      "Epoch 164/300, Train Loss: 12.86231166, Train Acc: 0.77534562, Val Loss: 3.01668233, Val Acc: 0.72580645\n",
      "Epoch 165/300, Train Loss: 13.63296011, Train Acc: 0.78110599, Val Loss: 3.11140519, Val Acc: 0.72043011\n",
      "Epoch 166/300, Train Loss: 13.30376405, Train Acc: 0.75806452, Val Loss: 3.01068985, Val Acc: 0.71505376\n",
      "Epoch 167/300, Train Loss: 13.09416708, Train Acc: 0.76382488, Val Loss: 2.98646730, Val Acc: 0.72580645\n",
      "Epoch 168/300, Train Loss: 13.22658542, Train Acc: 0.76497696, Val Loss: 2.99054974, Val Acc: 0.71505376\n",
      "Epoch 169/300, Train Loss: 12.72310966, Train Acc: 0.77534562, Val Loss: 2.98398596, Val Acc: 0.71505376\n",
      "Epoch 170/300, Train Loss: 12.11963604, Train Acc: 0.79723502, Val Loss: 3.00697470, Val Acc: 0.73118280\n",
      "Epoch 171/300, Train Loss: 11.54907843, Train Acc: 0.80875576, Val Loss: 3.02261457, Val Acc: 0.73118280\n",
      "Epoch 172/300, Train Loss: 12.87850988, Train Acc: 0.76036866, Val Loss: 2.98885196, Val Acc: 0.72580645\n",
      "Epoch 173/300, Train Loss: 11.98176247, Train Acc: 0.79147465, Val Loss: 3.00560844, Val Acc: 0.71505376\n",
      "Epoch 174/300, Train Loss: 13.40313125, Train Acc: 0.77073733, Val Loss: 2.93860400, Val Acc: 0.73118280\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_torch_model_binary\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_torch_model_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munbalanced\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_treshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bartosz\\Desktop\\automatic-lie-detection\\models\\utils\\model_functions\\train_model.py:117\u001b[0m, in \u001b[0;36mtrain_torch_model_binary\u001b[1;34m(model, criterion, optimizer, X_train, y_train, X_val, y_val, unbalanced, writer, batch_size, epochs, prediction_treshold, show_prediction_stats)\u001b[0m\n\u001b[0;32m    114\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m--> 117\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[43mX_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y_batch\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.model_functions import train_torch_model_binary\n",
    "\n",
    "train_torch_model_binary(model, criterion, optimizer, X_train, y_train, X_val, y_val, unbalanced=True, writer=writer, prediction_treshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0185, Test Accuracy: 0.6882\n"
     ]
    }
   ],
   "source": [
    "from utils.model_functions import eval_torch_model_binary\n",
    "\n",
    "eval_torch_model_binary(model, criterion, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Seglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from seglearn.pipe import Pype\n",
    "from seglearn.transform import FeatureRep, Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.numpy()\n",
    "X_val_np = X_val.numpy()\n",
    "X_test_np = X_test.numpy()\n",
    "y_train_np = y_train.numpy()\n",
    "y_val_np = y_val.numpy()\n",
    "y_test_np = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(868, 609, 154)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(868,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budowa modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pype([\n",
    "    (\"segment\", Segment(width=20, step=10)),  # Segmentacja sekwencji\n",
    "    (\"features\", FeatureRep()),              # Ekstrakcja cech\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=200\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pype(steps=[(&#x27;segment&#x27;, Segment(overlap=None, step=10, width=20)),\n",
       "            (&#x27;features&#x27;,\n",
       "             FeatureRep(features={&#x27;abs_energy&#x27;: &lt;function abs_energy at 0x000002761D1DAFC0&gt;,\n",
       "                                  &#x27;kurt&#x27;: &lt;function kurt at 0x000002761D1DB4C0&gt;,\n",
       "                                  &#x27;max&#x27;: &lt;function maximum at 0x000002761D1DB380&gt;,\n",
       "                                  &#x27;mean&#x27;: &lt;function mean at 0x000002761D1DAC00&gt;,\n",
       "                                  &#x27;median&#x27;: &lt;function median at 0x000002761D1DACA0&gt;,\n",
       "                                  &#x27;min&#x27;: &lt;function minimum at 0x...\n",
       "                           gamma=None, grow_policy=None, importance_type=None,\n",
       "                           interaction_constraints=None, learning_rate=None,\n",
       "                           max_bin=None, max_cat_threshold=None,\n",
       "                           max_cat_to_onehot=None, max_delta_step=None,\n",
       "                           max_depth=None, max_leaves=None,\n",
       "                           min_child_weight=None, missing=nan,\n",
       "                           monotone_constraints=None, multi_strategy=None,\n",
       "                           n_estimators=200, n_jobs=None,\n",
       "                           num_parallel_tree=None, random_state=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Pype<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pype(steps=[(&#x27;segment&#x27;, Segment(overlap=None, step=10, width=20)),\n",
       "            (&#x27;features&#x27;,\n",
       "             FeatureRep(features={&#x27;abs_energy&#x27;: &lt;function abs_energy at 0x000002761D1DAFC0&gt;,\n",
       "                                  &#x27;kurt&#x27;: &lt;function kurt at 0x000002761D1DB4C0&gt;,\n",
       "                                  &#x27;max&#x27;: &lt;function maximum at 0x000002761D1DB380&gt;,\n",
       "                                  &#x27;mean&#x27;: &lt;function mean at 0x000002761D1DAC00&gt;,\n",
       "                                  &#x27;median&#x27;: &lt;function median at 0x000002761D1DACA0&gt;,\n",
       "                                  &#x27;min&#x27;: &lt;function minimum at 0x...\n",
       "                           gamma=None, grow_policy=None, importance_type=None,\n",
       "                           interaction_constraints=None, learning_rate=None,\n",
       "                           max_bin=None, max_cat_threshold=None,\n",
       "                           max_cat_to_onehot=None, max_delta_step=None,\n",
       "                           max_depth=None, max_leaves=None,\n",
       "                           min_child_weight=None, missing=nan,\n",
       "                           monotone_constraints=None, multi_strategy=None,\n",
       "                           n_estimators=200, n_jobs=None,\n",
       "                           num_parallel_tree=None, random_state=None, ...))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Segment</label><div class=\"sk-toggleable__content fitted\"><pre>Segment(overlap=None, step=10, width=20)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">FeatureRep</label><div class=\"sk-toggleable__content fitted\"><pre>FeatureRep(features={&#x27;abs_energy&#x27;: &lt;function abs_energy at 0x000002761D1DAFC0&gt;,\n",
       "                     &#x27;kurt&#x27;: &lt;function kurt at 0x000002761D1DB4C0&gt;,\n",
       "                     &#x27;max&#x27;: &lt;function maximum at 0x000002761D1DB380&gt;,\n",
       "                     &#x27;mean&#x27;: &lt;function mean at 0x000002761D1DAC00&gt;,\n",
       "                     &#x27;median&#x27;: &lt;function median at 0x000002761D1DACA0&gt;,\n",
       "                     &#x27;min&#x27;: &lt;function minimum at 0x000002761D1DB2E0&gt;,\n",
       "                     &#x27;mnx&#x27;: &lt;function mean_crossings at 0x000002761D1DB740&gt;,\n",
       "                     &#x27;mse&#x27;: &lt;function mse at 0x000002761D1DB6A0&gt;,\n",
       "                     &#x27;skew&#x27;: &lt;function skew at 0x000002761D1DB420&gt;,\n",
       "                     &#x27;std&#x27;: &lt;function std at 0x000002761D1DB060&gt;,\n",
       "                     &#x27;var&#x27;: &lt;function var at 0x000002761D1DB100&gt;})</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">XGBClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pype(steps=[('segment', Segment(overlap=None, step=10, width=20)),\n",
       "            ('features',\n",
       "             FeatureRep(features={'abs_energy': <function abs_energy at 0x000002761D1DAFC0>,\n",
       "                                  'kurt': <function kurt at 0x000002761D1DB4C0>,\n",
       "                                  'max': <function maximum at 0x000002761D1DB380>,\n",
       "                                  'mean': <function mean at 0x000002761D1DAC00>,\n",
       "                                  'median': <function median at 0x000002761D1DACA0>,\n",
       "                                  'min': <function minimum at 0x...\n",
       "                           gamma=None, grow_policy=None, importance_type=None,\n",
       "                           interaction_constraints=None, learning_rate=None,\n",
       "                           max_bin=None, max_cat_threshold=None,\n",
       "                           max_cat_to_onehot=None, max_delta_step=None,\n",
       "                           max_depth=None, max_leaves=None,\n",
       "                           min_child_weight=None, missing=nan,\n",
       "                           monotone_constraints=None, multi_strategy=None,\n",
       "                           n_estimators=200, n_jobs=None,\n",
       "                           num_parallel_tree=None, random_state=None, ...))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train_np, y_train_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ewaluacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność na zbiorze walidacyjnym: 0.56\n",
      "Dokładność na zbiorze testowym: 0.55\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = pipe.score(X_val_np, y_val_np)\n",
    "test_accuracy = pipe.score(X_test_np, y_test_np)\n",
    "\n",
    "print(f\"Dokładność na zbiorze walidacyjnym: {val_accuracy:.2f}\")\n",
    "print(f\"Dokładność na zbiorze testowym: {test_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
