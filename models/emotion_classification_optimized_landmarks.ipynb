{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "**********\n",
      "_CUDA version: \n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Oct_30_01:18:48_Pacific_Daylight_Time_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n",
      "**********\n",
      "CUDNN version: 90100\n",
      "Available GPU devices: 1\n",
      "Device Name: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print('*'*10)\n",
    "print(f'_CUDA version: ')\n",
    "!nvcc --version\n",
    "print('*'*10)\n",
    "print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
    "print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
    "print(f'Device Name: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zdobycie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.abspath(os.path.join('..', 'data', 'processed', 'ravdess'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_LANDMARKS = 100\n",
    "LANDMARK_INDEXES = np.load(os.path.join('..', 'data', 'landmarks', f'top_{NUMBER_LANDMARKS}_important_landmarks.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja wybranych punkt√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_key_points = np.load(os.path.join('..', 'data', 'reference_points', 'key_points_xyz.npy'))\n",
    "selected_key_points = loaded_key_points[:, LANDMARK_INDEXES, :]\n",
    "\n",
    "# Plot the key points\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(selected_key_points[:, :, 0], selected_key_points[:, :, 1], c='r', marker='o', label='Selected Landmarks')\n",
    "\n",
    "# Flip Y-axis if needed (often in image coordinates)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(f\"Selected Top {NUMBER_LANDMARKS} Landmarks\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(1, 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".npy\"):\n",
    "            data = np.load(os.path.join(data_path, file), allow_pickle=True)\n",
    "            data = np.array(data, dtype=np.float32)\n",
    "            selected_data = data[:, LANDMARK_INDEXES, :]\n",
    "\n",
    "            all_data.append(selected_data)\n",
    "\n",
    "            label = int(file.split(\"-\")[2])\n",
    "            all_labels.append(label)\n",
    "\n",
    "    return np.array(all_data, dtype=object), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, all_labels = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, labels):\n",
    "    tensor_data = [torch.tensor(d, dtype=torch.float32) for d in data]\n",
    "    padded_data = pad_sequence(tensor_data, batch_first=True)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    encoded_labels = torch.tensor(encoded_labels, dtype=torch.float32)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        padded_data, encoded_labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(all_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2012, 157, 100, 2])\n",
      "torch.Size([2012, 8])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W podej≈õciu wykorzystane zostanƒÖ 2 modele - pierwszy z nich bƒôdzie sieciƒÖ konwolucyjnƒÖ 2d, kt√≥ra bƒôdzie mia≈Ça za zadanie nauczyƒá siƒô rozpoznawaƒá cechy charakterystyczne dla wybranej klatki (zbioru wsp√≥≈Çrzƒôdnych pkt charakterystycznych). Do klasyfikacji szeregu czasowego zostanie wykorzystana sekwencyjna sieƒá neuronowa LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbudowanie modelu ekstrakcji cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, number_landmarks=NUMBER_LANDMARKS):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.number_landmarks = number_landmarks\n",
    "        \n",
    "        # Spatial feature extraction using Conv1D\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Calculate the number of features after Conv1D and pooling\n",
    "        # Conv1D output: (batch_size * frames, 32, number_landmarks)\n",
    "        # Pool1D output: (batch_size * frames, 32, number_landmarks // 2)\n",
    "        self.flattened_features = 32 * (number_landmarks // 2)\n",
    "        \n",
    "        # LSTM layers for temporal feature extraction\n",
    "        self.lstm1 = nn.LSTM(input_size=self.flattened_features, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128 * 2, hidden_size=64, batch_first=True)\n",
    "        \n",
    "        # Fully connected classification layer\n",
    "        self.fc = nn.Linear(64, 8)  # 8 emotion classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, frames, number_landmarks, 2)\n",
    "        batch_size, frames, landmarks, coordinates = x.shape\n",
    "        \n",
    "        # Reshape for Conv1D: (batch_size * frames, coordinates, landmarks)\n",
    "        x = x.view(-1, landmarks, coordinates).permute(0, 2, 1)  # Shape: (batch_size * frames, 2, number_landmarks)\n",
    "        \n",
    "        # Spatial feature extraction\n",
    "        x = F.relu(self.conv1(x))  # Shape: (batch_size * frames, 32, number_landmarks)\n",
    "        x = self.pool1(x)          # Shape: (batch_size * frames, 32, number_landmarks // 2)\n",
    "        \n",
    "        # Flatten spatial features\n",
    "        x = x.view(batch_size, frames, -1)  # Shape: (batch_size, frames, 32 * (number_landmarks // 2))\n",
    "        \n",
    "        # Temporal feature extraction\n",
    "        x, _ = self.lstm1(x)  # Shape: (batch_size, frames, 128 * 2)\n",
    "        x, _ = self.lstm2(x)  # Shape: (batch_size, frames, 64)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.fc(x[:, -1, :])  # Take the last timestep's output; Shape: (batch_size, 8)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EmotionClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Train Loss: 130.4423, Train Acc: 0.1238, Val Loss: 28.8964, Val Acc: 0.1276\n",
      "Epoch 2/300, Train Loss: 130.1587, Train Acc: 0.1272, Val Loss: 28.8733, Val Acc: 0.1276\n",
      "Epoch 3/300, Train Loss: 130.0699, Train Acc: 0.1302, Val Loss: 28.8711, Val Acc: 0.1323\n",
      "Epoch 4/300, Train Loss: 130.1428, Train Acc: 0.1218, Val Loss: 28.8703, Val Acc: 0.1299\n",
      "Epoch 5/300, Train Loss: 130.0132, Train Acc: 0.1317, Val Loss: 28.8641, Val Acc: 0.1276\n",
      "Epoch 6/300, Train Loss: 130.0402, Train Acc: 0.1307, Val Loss: 28.8609, Val Acc: 0.1323\n",
      "Epoch 7/300, Train Loss: 130.0745, Train Acc: 0.1252, Val Loss: 28.8705, Val Acc: 0.1276\n",
      "Epoch 8/300, Train Loss: 130.0503, Train Acc: 0.1312, Val Loss: 28.8609, Val Acc: 0.1415\n",
      "Epoch 9/300, Train Loss: 130.0555, Train Acc: 0.1282, Val Loss: 28.8563, Val Acc: 0.1439\n",
      "Epoch 10/300, Train Loss: 130.0160, Train Acc: 0.1352, Val Loss: 28.8547, Val Acc: 0.1531\n",
      "Epoch 11/300, Train Loss: 130.0345, Train Acc: 0.1337, Val Loss: 28.8538, Val Acc: 0.1555\n",
      "Epoch 12/300, Train Loss: 129.9881, Train Acc: 0.1287, Val Loss: 28.8315, Val Acc: 0.1276\n",
      "Epoch 13/300, Train Loss: 129.4248, Train Acc: 0.1516, Val Loss: 28.5019, Val Acc: 0.1624\n",
      "Epoch 14/300, Train Loss: 126.4134, Train Acc: 0.1948, Val Loss: 27.6008, Val Acc: 0.1787\n",
      "Epoch 15/300, Train Loss: 123.9772, Train Acc: 0.2202, Val Loss: 27.3829, Val Acc: 0.1810\n",
      "Epoch 16/300, Train Loss: 124.0072, Train Acc: 0.2058, Val Loss: 27.4063, Val Acc: 0.1949\n",
      "Epoch 17/300, Train Loss: 123.0284, Train Acc: 0.2147, Val Loss: 27.4279, Val Acc: 0.1694\n",
      "Epoch 18/300, Train Loss: 122.7537, Train Acc: 0.2162, Val Loss: 27.3320, Val Acc: 0.2065\n",
      "Epoch 19/300, Train Loss: 122.7133, Train Acc: 0.2152, Val Loss: 27.5095, Val Acc: 0.1787\n",
      "Epoch 20/300, Train Loss: 122.4929, Train Acc: 0.2187, Val Loss: 27.2393, Val Acc: 0.1903\n",
      "Epoch 21/300, Train Loss: 122.7214, Train Acc: 0.1998, Val Loss: 27.2580, Val Acc: 0.1856\n",
      "Epoch 22/300, Train Loss: 122.5785, Train Acc: 0.2167, Val Loss: 27.2991, Val Acc: 0.1810\n",
      "Epoch 23/300, Train Loss: 122.3057, Train Acc: 0.2222, Val Loss: 27.2958, Val Acc: 0.1972\n",
      "Epoch 24/300, Train Loss: 122.3277, Train Acc: 0.2147, Val Loss: 27.1976, Val Acc: 0.1926\n",
      "Epoch 25/300, Train Loss: 122.4211, Train Acc: 0.2222, Val Loss: 27.1665, Val Acc: 0.1949\n",
      "Epoch 26/300, Train Loss: 122.5866, Train Acc: 0.2167, Val Loss: 27.2542, Val Acc: 0.1856\n",
      "Epoch 27/300, Train Loss: 122.2714, Train Acc: 0.2162, Val Loss: 27.2209, Val Acc: 0.1972\n",
      "Epoch 28/300, Train Loss: 122.2941, Train Acc: 0.2227, Val Loss: 27.2598, Val Acc: 0.1833\n",
      "Epoch 29/300, Train Loss: 122.4150, Train Acc: 0.2097, Val Loss: 27.1866, Val Acc: 0.1903\n",
      "Epoch 30/300, Train Loss: 122.2793, Train Acc: 0.2172, Val Loss: 27.1808, Val Acc: 0.1949\n",
      "Epoch 31/300, Train Loss: 122.0971, Train Acc: 0.2177, Val Loss: 27.1800, Val Acc: 0.1856\n",
      "Epoch 32/300, Train Loss: 122.1382, Train Acc: 0.2222, Val Loss: 27.2433, Val Acc: 0.1949\n",
      "Epoch 33/300, Train Loss: 122.1417, Train Acc: 0.2167, Val Loss: 27.1608, Val Acc: 0.2042\n",
      "Epoch 34/300, Train Loss: 122.1665, Train Acc: 0.2172, Val Loss: 27.2946, Val Acc: 0.2065\n",
      "Epoch 35/300, Train Loss: 122.1709, Train Acc: 0.2276, Val Loss: 27.2405, Val Acc: 0.2111\n",
      "Epoch 36/300, Train Loss: 122.0874, Train Acc: 0.2232, Val Loss: 27.2006, Val Acc: 0.1903\n",
      "Epoch 37/300, Train Loss: 122.0808, Train Acc: 0.2162, Val Loss: 27.2315, Val Acc: 0.1903\n",
      "Epoch 38/300, Train Loss: 121.9055, Train Acc: 0.2237, Val Loss: 27.2222, Val Acc: 0.1903\n",
      "Epoch 39/300, Train Loss: 121.8760, Train Acc: 0.2172, Val Loss: 27.4849, Val Acc: 0.1810\n",
      "Epoch 40/300, Train Loss: 122.1049, Train Acc: 0.2097, Val Loss: 27.2908, Val Acc: 0.1949\n",
      "Epoch 41/300, Train Loss: 121.9562, Train Acc: 0.2107, Val Loss: 27.1663, Val Acc: 0.1903\n",
      "Epoch 42/300, Train Loss: 121.8988, Train Acc: 0.2207, Val Loss: 27.1876, Val Acc: 0.2111\n",
      "Epoch 43/300, Train Loss: 121.9703, Train Acc: 0.2212, Val Loss: 27.1862, Val Acc: 0.2065\n",
      "Epoch 44/300, Train Loss: 121.9994, Train Acc: 0.2122, Val Loss: 27.1877, Val Acc: 0.1856\n",
      "Epoch 45/300, Train Loss: 121.8345, Train Acc: 0.2217, Val Loss: 27.1535, Val Acc: 0.1926\n",
      "Epoch 46/300, Train Loss: 121.9871, Train Acc: 0.2202, Val Loss: 27.1794, Val Acc: 0.1926\n",
      "Epoch 47/300, Train Loss: 121.8446, Train Acc: 0.2227, Val Loss: 27.1927, Val Acc: 0.1949\n",
      "Epoch 48/300, Train Loss: 121.8302, Train Acc: 0.2237, Val Loss: 27.1250, Val Acc: 0.1856\n",
      "Epoch 49/300, Train Loss: 121.8145, Train Acc: 0.2212, Val Loss: 27.1119, Val Acc: 0.2042\n",
      "Epoch 50/300, Train Loss: 121.9350, Train Acc: 0.2192, Val Loss: 27.1530, Val Acc: 0.1879\n",
      "Epoch 51/300, Train Loss: 121.7561, Train Acc: 0.2321, Val Loss: 27.1137, Val Acc: 0.2065\n",
      "Epoch 52/300, Train Loss: 121.7962, Train Acc: 0.2207, Val Loss: 27.1593, Val Acc: 0.1972\n",
      "Epoch 53/300, Train Loss: 121.6093, Train Acc: 0.2217, Val Loss: 27.1060, Val Acc: 0.1972\n",
      "Epoch 54/300, Train Loss: 121.5371, Train Acc: 0.2157, Val Loss: 27.2307, Val Acc: 0.1879\n",
      "Epoch 55/300, Train Loss: 121.6194, Train Acc: 0.2197, Val Loss: 27.2128, Val Acc: 0.1810\n",
      "Epoch 56/300, Train Loss: 121.7629, Train Acc: 0.2247, Val Loss: 27.1833, Val Acc: 0.2065\n",
      "Epoch 57/300, Train Loss: 121.6683, Train Acc: 0.2256, Val Loss: 27.1137, Val Acc: 0.1949\n",
      "Epoch 58/300, Train Loss: 121.4738, Train Acc: 0.2271, Val Loss: 27.1195, Val Acc: 0.1949\n",
      "Epoch 59/300, Train Loss: 121.4099, Train Acc: 0.2197, Val Loss: 27.1375, Val Acc: 0.2135\n",
      "Epoch 60/300, Train Loss: 121.2670, Train Acc: 0.2227, Val Loss: 27.0880, Val Acc: 0.1903\n",
      "Epoch 61/300, Train Loss: 121.4447, Train Acc: 0.2197, Val Loss: 27.1079, Val Acc: 0.1972\n",
      "Epoch 62/300, Train Loss: 121.4438, Train Acc: 0.2276, Val Loss: 27.1199, Val Acc: 0.1926\n",
      "Epoch 63/300, Train Loss: 121.5481, Train Acc: 0.2182, Val Loss: 27.1910, Val Acc: 0.1833\n",
      "Epoch 64/300, Train Loss: 121.3823, Train Acc: 0.2232, Val Loss: 27.0665, Val Acc: 0.2111\n",
      "Epoch 65/300, Train Loss: 121.6833, Train Acc: 0.2177, Val Loss: 27.0766, Val Acc: 0.2042\n",
      "Epoch 66/300, Train Loss: 121.5069, Train Acc: 0.2172, Val Loss: 27.1097, Val Acc: 0.2065\n",
      "Epoch 67/300, Train Loss: 121.8446, Train Acc: 0.2172, Val Loss: 27.0668, Val Acc: 0.2019\n",
      "Epoch 68/300, Train Loss: 121.2866, Train Acc: 0.2227, Val Loss: 27.1539, Val Acc: 0.1787\n",
      "Epoch 69/300, Train Loss: 120.9021, Train Acc: 0.2232, Val Loss: 26.8938, Val Acc: 0.2019\n",
      "Epoch 70/300, Train Loss: 121.1881, Train Acc: 0.2247, Val Loss: 27.1651, Val Acc: 0.1810\n",
      "Epoch 71/300, Train Loss: 121.4563, Train Acc: 0.2261, Val Loss: 27.0819, Val Acc: 0.2065\n",
      "Epoch 72/300, Train Loss: 121.4262, Train Acc: 0.2172, Val Loss: 27.1741, Val Acc: 0.2019\n",
      "Epoch 73/300, Train Loss: 121.8860, Train Acc: 0.2167, Val Loss: 27.1342, Val Acc: 0.1903\n",
      "Epoch 74/300, Train Loss: 121.3612, Train Acc: 0.2242, Val Loss: 26.9950, Val Acc: 0.2042\n",
      "Epoch 75/300, Train Loss: 121.6483, Train Acc: 0.2192, Val Loss: 27.1219, Val Acc: 0.1856\n",
      "Epoch 76/300, Train Loss: 121.6014, Train Acc: 0.2182, Val Loss: 27.2302, Val Acc: 0.1972\n",
      "Epoch 77/300, Train Loss: 121.3345, Train Acc: 0.2227, Val Loss: 27.1353, Val Acc: 0.2065\n",
      "Epoch 78/300, Train Loss: 121.5105, Train Acc: 0.2237, Val Loss: 27.1929, Val Acc: 0.1926\n",
      "Epoch 79/300, Train Loss: 120.7964, Train Acc: 0.2271, Val Loss: 27.3959, Val Acc: 0.1763\n",
      "Epoch 80/300, Train Loss: 121.6696, Train Acc: 0.2207, Val Loss: 27.0806, Val Acc: 0.1926\n",
      "Epoch 81/300, Train Loss: 121.3972, Train Acc: 0.2187, Val Loss: 27.1141, Val Acc: 0.1995\n",
      "Epoch 82/300, Train Loss: 121.6807, Train Acc: 0.2142, Val Loss: 27.1230, Val Acc: 0.2042\n",
      "Epoch 83/300, Train Loss: 121.1046, Train Acc: 0.2142, Val Loss: 27.1002, Val Acc: 0.1995\n",
      "Epoch 84/300, Train Loss: 121.5044, Train Acc: 0.2242, Val Loss: 27.1432, Val Acc: 0.1856\n",
      "Epoch 85/300, Train Loss: 121.1370, Train Acc: 0.2197, Val Loss: 27.0274, Val Acc: 0.1926\n",
      "Epoch 86/300, Train Loss: 120.8001, Train Acc: 0.2207, Val Loss: 26.8866, Val Acc: 0.2088\n",
      "Epoch 87/300, Train Loss: 121.0165, Train Acc: 0.2192, Val Loss: 26.8152, Val Acc: 0.2088\n",
      "Epoch 88/300, Train Loss: 120.9831, Train Acc: 0.2227, Val Loss: 26.8130, Val Acc: 0.2042\n",
      "Epoch 89/300, Train Loss: 121.2762, Train Acc: 0.2256, Val Loss: 26.9085, Val Acc: 0.2019\n",
      "Epoch 90/300, Train Loss: 120.2971, Train Acc: 0.2336, Val Loss: 26.7485, Val Acc: 0.2088\n",
      "Epoch 91/300, Train Loss: 119.9080, Train Acc: 0.2296, Val Loss: 26.6035, Val Acc: 0.2227\n",
      "Epoch 92/300, Train Loss: 120.2417, Train Acc: 0.2351, Val Loss: 26.8383, Val Acc: 0.2042\n",
      "Epoch 93/300, Train Loss: 120.2339, Train Acc: 0.2301, Val Loss: 26.6763, Val Acc: 0.2320\n",
      "Epoch 94/300, Train Loss: 120.0201, Train Acc: 0.2237, Val Loss: 27.0110, Val Acc: 0.2065\n",
      "Epoch 95/300, Train Loss: 119.7960, Train Acc: 0.2217, Val Loss: 26.4743, Val Acc: 0.2158\n",
      "Epoch 96/300, Train Loss: 119.0445, Train Acc: 0.2331, Val Loss: 26.5756, Val Acc: 0.2158\n",
      "Epoch 97/300, Train Loss: 122.3910, Train Acc: 0.2207, Val Loss: 27.0883, Val Acc: 0.2042\n",
      "Epoch 98/300, Train Loss: 120.1540, Train Acc: 0.2291, Val Loss: 26.4008, Val Acc: 0.2390\n",
      "Epoch 99/300, Train Loss: 118.5932, Train Acc: 0.2435, Val Loss: 26.2576, Val Acc: 0.2204\n",
      "Epoch 100/300, Train Loss: 119.8237, Train Acc: 0.2361, Val Loss: 26.8958, Val Acc: 0.2065\n",
      "Epoch 101/300, Train Loss: 118.7948, Train Acc: 0.2406, Val Loss: 26.4069, Val Acc: 0.2552\n",
      "Epoch 102/300, Train Loss: 119.5891, Train Acc: 0.2207, Val Loss: 26.6491, Val Acc: 0.2065\n",
      "Epoch 103/300, Train Loss: 118.1302, Train Acc: 0.2445, Val Loss: 26.6733, Val Acc: 0.1972\n",
      "Epoch 104/300, Train Loss: 117.2613, Train Acc: 0.2505, Val Loss: 26.4113, Val Acc: 0.2158\n",
      "Epoch 105/300, Train Loss: 117.7263, Train Acc: 0.2460, Val Loss: 26.5905, Val Acc: 0.2343\n",
      "Epoch 106/300, Train Loss: 116.7350, Train Acc: 0.2490, Val Loss: 26.0871, Val Acc: 0.2320\n",
      "Epoch 107/300, Train Loss: 117.5967, Train Acc: 0.2540, Val Loss: 26.3513, Val Acc: 0.2111\n",
      "Epoch 108/300, Train Loss: 117.9029, Train Acc: 0.2470, Val Loss: 26.3094, Val Acc: 0.2158\n",
      "Epoch 109/300, Train Loss: 117.3804, Train Acc: 0.2515, Val Loss: 26.0591, Val Acc: 0.2459\n",
      "Epoch 110/300, Train Loss: 117.9467, Train Acc: 0.2435, Val Loss: 26.0956, Val Acc: 0.2227\n",
      "Epoch 111/300, Train Loss: 117.5948, Train Acc: 0.2515, Val Loss: 26.5477, Val Acc: 0.2088\n",
      "Epoch 112/300, Train Loss: 116.4597, Train Acc: 0.2729, Val Loss: 25.9706, Val Acc: 0.2343\n",
      "Epoch 113/300, Train Loss: 116.1868, Train Acc: 0.2634, Val Loss: 26.3101, Val Acc: 0.2297\n",
      "Epoch 114/300, Train Loss: 115.7436, Train Acc: 0.2629, Val Loss: 26.3790, Val Acc: 0.2367\n",
      "Epoch 115/300, Train Loss: 116.1015, Train Acc: 0.2485, Val Loss: 25.9624, Val Acc: 0.2297\n",
      "Epoch 116/300, Train Loss: 115.6256, Train Acc: 0.2778, Val Loss: 25.8553, Val Acc: 0.2668\n",
      "Epoch 117/300, Train Loss: 114.9729, Train Acc: 0.2689, Val Loss: 25.7575, Val Acc: 0.2807\n",
      "Epoch 118/300, Train Loss: 117.3783, Train Acc: 0.2580, Val Loss: 26.3528, Val Acc: 0.2436\n",
      "Epoch 119/300, Train Loss: 115.2459, Train Acc: 0.2878, Val Loss: 25.9554, Val Acc: 0.2506\n",
      "Epoch 120/300, Train Loss: 114.3441, Train Acc: 0.2868, Val Loss: 25.5870, Val Acc: 0.2506\n",
      "Epoch 121/300, Train Loss: 114.7458, Train Acc: 0.2828, Val Loss: 25.9918, Val Acc: 0.2320\n",
      "Epoch 122/300, Train Loss: 115.4047, Train Acc: 0.2778, Val Loss: 25.5982, Val Acc: 0.2715\n",
      "Epoch 123/300, Train Loss: 113.9747, Train Acc: 0.3022, Val Loss: 25.9352, Val Acc: 0.2413\n",
      "Epoch 124/300, Train Loss: 114.6229, Train Acc: 0.2838, Val Loss: 25.6576, Val Acc: 0.2436\n",
      "Epoch 125/300, Train Loss: 113.4756, Train Acc: 0.2893, Val Loss: 25.1471, Val Acc: 0.2715\n",
      "Epoch 126/300, Train Loss: 112.3136, Train Acc: 0.3037, Val Loss: 24.9779, Val Acc: 0.2854\n",
      "Epoch 127/300, Train Loss: 111.4042, Train Acc: 0.3101, Val Loss: 25.1698, Val Acc: 0.2831\n",
      "Epoch 128/300, Train Loss: 111.9429, Train Acc: 0.3086, Val Loss: 26.0099, Val Acc: 0.2343\n",
      "Epoch 129/300, Train Loss: 111.3006, Train Acc: 0.3166, Val Loss: 25.1586, Val Acc: 0.2552\n",
      "Epoch 130/300, Train Loss: 110.4217, Train Acc: 0.3191, Val Loss: 25.2116, Val Acc: 0.2738\n",
      "Epoch 131/300, Train Loss: 111.5070, Train Acc: 0.3096, Val Loss: 24.8607, Val Acc: 0.2784\n",
      "Epoch 132/300, Train Loss: 108.3247, Train Acc: 0.3325, Val Loss: 24.8597, Val Acc: 0.2900\n",
      "Epoch 133/300, Train Loss: 109.4771, Train Acc: 0.3320, Val Loss: 24.9232, Val Acc: 0.2831\n",
      "Epoch 134/300, Train Loss: 107.8959, Train Acc: 0.3315, Val Loss: 23.8941, Val Acc: 0.3016\n",
      "Epoch 135/300, Train Loss: 105.3267, Train Acc: 0.3539, Val Loss: 24.3221, Val Acc: 0.2970\n",
      "Epoch 136/300, Train Loss: 105.2249, Train Acc: 0.3504, Val Loss: 23.2612, Val Acc: 0.3295\n",
      "Epoch 137/300, Train Loss: 105.7907, Train Acc: 0.3320, Val Loss: 23.2500, Val Acc: 0.3295\n",
      "Epoch 138/300, Train Loss: 108.1603, Train Acc: 0.3405, Val Loss: 23.8706, Val Acc: 0.3271\n",
      "Epoch 139/300, Train Loss: 106.8338, Train Acc: 0.3400, Val Loss: 23.9395, Val Acc: 0.3295\n",
      "Epoch 140/300, Train Loss: 106.5109, Train Acc: 0.3186, Val Loss: 23.2348, Val Acc: 0.3271\n",
      "Epoch 141/300, Train Loss: 103.1579, Train Acc: 0.3569, Val Loss: 22.0247, Val Acc: 0.3643\n",
      "Epoch 142/300, Train Loss: 102.4765, Train Acc: 0.3633, Val Loss: 23.7981, Val Acc: 0.3271\n",
      "Epoch 143/300, Train Loss: 103.9864, Train Acc: 0.3494, Val Loss: 23.3156, Val Acc: 0.3295\n",
      "Epoch 144/300, Train Loss: 100.2686, Train Acc: 0.3757, Val Loss: 21.9496, Val Acc: 0.3619\n",
      "Epoch 145/300, Train Loss: 99.5653, Train Acc: 0.3777, Val Loss: 21.9879, Val Acc: 0.3828\n",
      "Epoch 146/300, Train Loss: 98.7461, Train Acc: 0.3867, Val Loss: 21.8561, Val Acc: 0.3643\n",
      "Epoch 147/300, Train Loss: 100.6961, Train Acc: 0.3767, Val Loss: 23.4391, Val Acc: 0.3411\n",
      "Epoch 148/300, Train Loss: 98.8736, Train Acc: 0.3877, Val Loss: 21.5613, Val Acc: 0.3782\n",
      "Epoch 149/300, Train Loss: 100.4178, Train Acc: 0.3658, Val Loss: 22.2017, Val Acc: 0.3271\n",
      "Epoch 150/300, Train Loss: 98.7006, Train Acc: 0.3862, Val Loss: 22.0122, Val Acc: 0.3434\n",
      "Epoch 151/300, Train Loss: 98.5741, Train Acc: 0.3802, Val Loss: 21.3121, Val Acc: 0.3759\n",
      "Epoch 152/300, Train Loss: 97.9838, Train Acc: 0.3877, Val Loss: 21.4965, Val Acc: 0.3805\n",
      "Epoch 153/300, Train Loss: 95.9904, Train Acc: 0.3931, Val Loss: 21.7304, Val Acc: 0.3480\n",
      "Epoch 154/300, Train Loss: 96.7620, Train Acc: 0.3926, Val Loss: 21.8193, Val Acc: 0.3805\n",
      "Epoch 155/300, Train Loss: 97.7203, Train Acc: 0.3792, Val Loss: 22.4060, Val Acc: 0.3666\n",
      "Epoch 156/300, Train Loss: 95.8900, Train Acc: 0.4071, Val Loss: 22.1916, Val Acc: 0.4107\n",
      "Epoch 157/300, Train Loss: 97.0273, Train Acc: 0.3941, Val Loss: 20.6142, Val Acc: 0.4037\n",
      "Epoch 158/300, Train Loss: 95.2067, Train Acc: 0.4061, Val Loss: 21.3267, Val Acc: 0.3875\n",
      "Epoch 159/300, Train Loss: 93.9054, Train Acc: 0.4140, Val Loss: 20.6710, Val Acc: 0.4014\n",
      "Epoch 160/300, Train Loss: 93.3192, Train Acc: 0.4115, Val Loss: 20.7156, Val Acc: 0.3944\n",
      "Epoch 161/300, Train Loss: 93.4449, Train Acc: 0.4110, Val Loss: 20.5149, Val Acc: 0.4060\n",
      "Epoch 162/300, Train Loss: 92.8717, Train Acc: 0.4021, Val Loss: 21.3965, Val Acc: 0.3550\n",
      "Epoch 163/300, Train Loss: 103.1034, Train Acc: 0.3469, Val Loss: 22.9393, Val Acc: 0.3782\n",
      "Epoch 164/300, Train Loss: 99.9086, Train Acc: 0.3842, Val Loss: 20.9639, Val Acc: 0.3782\n",
      "Epoch 165/300, Train Loss: 94.8177, Train Acc: 0.4061, Val Loss: 21.4115, Val Acc: 0.3318\n",
      "Epoch 166/300, Train Loss: 93.8307, Train Acc: 0.4100, Val Loss: 21.0850, Val Acc: 0.3550\n",
      "Epoch 167/300, Train Loss: 95.1674, Train Acc: 0.3966, Val Loss: 20.4710, Val Acc: 0.4014\n",
      "Epoch 168/300, Train Loss: 95.9358, Train Acc: 0.4081, Val Loss: 21.4705, Val Acc: 0.3875\n",
      "Epoch 169/300, Train Loss: 97.1083, Train Acc: 0.3877, Val Loss: 21.9834, Val Acc: 0.3364\n",
      "Epoch 170/300, Train Loss: 92.4173, Train Acc: 0.4135, Val Loss: 20.9746, Val Acc: 0.3735\n",
      "Epoch 171/300, Train Loss: 93.2678, Train Acc: 0.4145, Val Loss: 20.6133, Val Acc: 0.4014\n",
      "Epoch 172/300, Train Loss: 90.7015, Train Acc: 0.4309, Val Loss: 20.1363, Val Acc: 0.4107\n",
      "Epoch 173/300, Train Loss: 90.6173, Train Acc: 0.4354, Val Loss: 21.4021, Val Acc: 0.3712\n",
      "Epoch 174/300, Train Loss: 91.7709, Train Acc: 0.4289, Val Loss: 20.9619, Val Acc: 0.3828\n",
      "Epoch 175/300, Train Loss: 91.0695, Train Acc: 0.4185, Val Loss: 20.0580, Val Acc: 0.4339\n",
      "Epoch 176/300, Train Loss: 93.1494, Train Acc: 0.4190, Val Loss: 20.9042, Val Acc: 0.4084\n",
      "Epoch 177/300, Train Loss: 92.3661, Train Acc: 0.4314, Val Loss: 20.3132, Val Acc: 0.4060\n",
      "Epoch 178/300, Train Loss: 92.4673, Train Acc: 0.4180, Val Loss: 21.2310, Val Acc: 0.3968\n",
      "Epoch 179/300, Train Loss: 89.9499, Train Acc: 0.4299, Val Loss: 19.7768, Val Acc: 0.4432\n",
      "Epoch 180/300, Train Loss: 91.0373, Train Acc: 0.4264, Val Loss: 19.9607, Val Acc: 0.4176\n",
      "Epoch 181/300, Train Loss: 89.8383, Train Acc: 0.4284, Val Loss: 20.5751, Val Acc: 0.3968\n",
      "Epoch 182/300, Train Loss: 88.2712, Train Acc: 0.4433, Val Loss: 19.8348, Val Acc: 0.4408\n",
      "Epoch 183/300, Train Loss: 89.2328, Train Acc: 0.4468, Val Loss: 20.1551, Val Acc: 0.4223\n",
      "Epoch 184/300, Train Loss: 92.9334, Train Acc: 0.4190, Val Loss: 21.1153, Val Acc: 0.4014\n",
      "Epoch 185/300, Train Loss: 89.7586, Train Acc: 0.4448, Val Loss: 20.3238, Val Acc: 0.4130\n",
      "Epoch 186/300, Train Loss: 89.0169, Train Acc: 0.4339, Val Loss: 19.5957, Val Acc: 0.4408\n",
      "Epoch 187/300, Train Loss: 90.4189, Train Acc: 0.4299, Val Loss: 20.4222, Val Acc: 0.4153\n",
      "Epoch 188/300, Train Loss: 88.9474, Train Acc: 0.4414, Val Loss: 19.5970, Val Acc: 0.4292\n",
      "Epoch 189/300, Train Loss: 93.7804, Train Acc: 0.4130, Val Loss: 20.9070, Val Acc: 0.4060\n",
      "Epoch 190/300, Train Loss: 88.2411, Train Acc: 0.4458, Val Loss: 19.5568, Val Acc: 0.4269\n",
      "Epoch 191/300, Train Loss: 90.8721, Train Acc: 0.4438, Val Loss: 20.0110, Val Acc: 0.4223\n",
      "Epoch 192/300, Train Loss: 90.7078, Train Acc: 0.4210, Val Loss: 19.6046, Val Acc: 0.4455\n",
      "Epoch 193/300, Train Loss: 86.9753, Train Acc: 0.4359, Val Loss: 19.6804, Val Acc: 0.4339\n",
      "Epoch 194/300, Train Loss: 90.2557, Train Acc: 0.4230, Val Loss: 21.3452, Val Acc: 0.3852\n",
      "Epoch 195/300, Train Loss: 89.6413, Train Acc: 0.4314, Val Loss: 20.6992, Val Acc: 0.4385\n",
      "Epoch 196/300, Train Loss: 92.6064, Train Acc: 0.4061, Val Loss: 19.6397, Val Acc: 0.4362\n",
      "Epoch 197/300, Train Loss: 90.0605, Train Acc: 0.4364, Val Loss: 19.8986, Val Acc: 0.4362\n",
      "Epoch 198/300, Train Loss: 91.9980, Train Acc: 0.4155, Val Loss: 20.8462, Val Acc: 0.3852\n",
      "Epoch 199/300, Train Loss: 91.7343, Train Acc: 0.4095, Val Loss: 20.4170, Val Acc: 0.4107\n",
      "Epoch 200/300, Train Loss: 89.4814, Train Acc: 0.4324, Val Loss: 21.2924, Val Acc: 0.3875\n",
      "Epoch 201/300, Train Loss: 89.1965, Train Acc: 0.4418, Val Loss: 19.6778, Val Acc: 0.4223\n",
      "Epoch 202/300, Train Loss: 90.6072, Train Acc: 0.4359, Val Loss: 19.8502, Val Acc: 0.4223\n",
      "Epoch 203/300, Train Loss: 86.6039, Train Acc: 0.4483, Val Loss: 19.6446, Val Acc: 0.4501\n",
      "Epoch 204/300, Train Loss: 89.1182, Train Acc: 0.4478, Val Loss: 21.1390, Val Acc: 0.3875\n",
      "Epoch 205/300, Train Loss: 86.3997, Train Acc: 0.4568, Val Loss: 20.2145, Val Acc: 0.4037\n",
      "Epoch 206/300, Train Loss: 84.2771, Train Acc: 0.4697, Val Loss: 20.1582, Val Acc: 0.4153\n",
      "Epoch 207/300, Train Loss: 86.1365, Train Acc: 0.4652, Val Loss: 19.3753, Val Acc: 0.4501\n",
      "Epoch 208/300, Train Loss: 87.2931, Train Acc: 0.4478, Val Loss: 20.1412, Val Acc: 0.4455\n",
      "Epoch 209/300, Train Loss: 86.6680, Train Acc: 0.4548, Val Loss: 20.6056, Val Acc: 0.4130\n",
      "Epoch 210/300, Train Loss: 84.9145, Train Acc: 0.4667, Val Loss: 18.9434, Val Acc: 0.4733\n",
      "Epoch 211/300, Train Loss: 84.9171, Train Acc: 0.4776, Val Loss: 20.5150, Val Acc: 0.4200\n",
      "Epoch 212/300, Train Loss: 86.2428, Train Acc: 0.4538, Val Loss: 20.4551, Val Acc: 0.4501\n",
      "Epoch 213/300, Train Loss: 84.4157, Train Acc: 0.4712, Val Loss: 18.6947, Val Acc: 0.4501\n",
      "Epoch 214/300, Train Loss: 83.4134, Train Acc: 0.4781, Val Loss: 18.8844, Val Acc: 0.4548\n",
      "Epoch 215/300, Train Loss: 83.2272, Train Acc: 0.4737, Val Loss: 18.7431, Val Acc: 0.4501\n",
      "Epoch 216/300, Train Loss: 86.4529, Train Acc: 0.4607, Val Loss: 19.9054, Val Acc: 0.4316\n",
      "Epoch 217/300, Train Loss: 84.5291, Train Acc: 0.4597, Val Loss: 18.4303, Val Acc: 0.4640\n",
      "Epoch 218/300, Train Loss: 82.1199, Train Acc: 0.4796, Val Loss: 19.0494, Val Acc: 0.4687\n",
      "Epoch 219/300, Train Loss: 84.7920, Train Acc: 0.4627, Val Loss: 19.2714, Val Acc: 0.4617\n",
      "Epoch 220/300, Train Loss: 87.7458, Train Acc: 0.4473, Val Loss: 19.6344, Val Acc: 0.4153\n",
      "Epoch 221/300, Train Loss: 86.4470, Train Acc: 0.4592, Val Loss: 19.6863, Val Acc: 0.4292\n",
      "Epoch 222/300, Train Loss: 85.6178, Train Acc: 0.4632, Val Loss: 18.8412, Val Acc: 0.4710\n",
      "Epoch 223/300, Train Loss: 82.8180, Train Acc: 0.4751, Val Loss: 18.9765, Val Acc: 0.4780\n",
      "Epoch 224/300, Train Loss: 87.3425, Train Acc: 0.4563, Val Loss: 19.1496, Val Acc: 0.4432\n",
      "Epoch 225/300, Train Loss: 83.0540, Train Acc: 0.4881, Val Loss: 18.9025, Val Acc: 0.4617\n",
      "Epoch 226/300, Train Loss: 90.1945, Train Acc: 0.4563, Val Loss: 35.3560, Val Acc: 0.2320\n",
      "Epoch 227/300, Train Loss: 112.8115, Train Acc: 0.3241, Val Loss: 23.3707, Val Acc: 0.3202\n",
      "Epoch 228/300, Train Loss: 93.4057, Train Acc: 0.4041, Val Loss: 20.5875, Val Acc: 0.3968\n",
      "Epoch 229/300, Train Loss: 88.7419, Train Acc: 0.4334, Val Loss: 22.3216, Val Acc: 0.3550\n",
      "Epoch 230/300, Train Loss: 87.9928, Train Acc: 0.4399, Val Loss: 19.6742, Val Acc: 0.4014\n",
      "Epoch 231/300, Train Loss: 85.7361, Train Acc: 0.4647, Val Loss: 19.2631, Val Acc: 0.4223\n",
      "Epoch 232/300, Train Loss: 88.7735, Train Acc: 0.4404, Val Loss: 19.8562, Val Acc: 0.4107\n",
      "Epoch 233/300, Train Loss: 85.9536, Train Acc: 0.4622, Val Loss: 19.6395, Val Acc: 0.4594\n",
      "Epoch 234/300, Train Loss: 84.2741, Train Acc: 0.4751, Val Loss: 19.7010, Val Acc: 0.4455\n",
      "Epoch 235/300, Train Loss: 84.7533, Train Acc: 0.4667, Val Loss: 19.2535, Val Acc: 0.4455\n",
      "Epoch 236/300, Train Loss: 88.9022, Train Acc: 0.4513, Val Loss: 20.2459, Val Acc: 0.4269\n",
      "Epoch 237/300, Train Loss: 83.3648, Train Acc: 0.4881, Val Loss: 19.1953, Val Acc: 0.4594\n",
      "Epoch 238/300, Train Loss: 83.2976, Train Acc: 0.4761, Val Loss: 19.5600, Val Acc: 0.4617\n",
      "Epoch 239/300, Train Loss: 82.7153, Train Acc: 0.4846, Val Loss: 18.9474, Val Acc: 0.4548\n",
      "Epoch 240/300, Train Loss: 82.5895, Train Acc: 0.4881, Val Loss: 18.5356, Val Acc: 0.4780\n",
      "Epoch 241/300, Train Loss: 81.0765, Train Acc: 0.4945, Val Loss: 19.0016, Val Acc: 0.4826\n",
      "Epoch 242/300, Train Loss: 83.4078, Train Acc: 0.4906, Val Loss: 18.5185, Val Acc: 0.4780\n",
      "Epoch 243/300, Train Loss: 79.1798, Train Acc: 0.5139, Val Loss: 19.0543, Val Acc: 0.4246\n",
      "Epoch 244/300, Train Loss: 80.6361, Train Acc: 0.5055, Val Loss: 19.4506, Val Acc: 0.4362\n",
      "Epoch 245/300, Train Loss: 83.6590, Train Acc: 0.4672, Val Loss: 19.4081, Val Acc: 0.4455\n",
      "Epoch 246/300, Train Loss: 79.3728, Train Acc: 0.5149, Val Loss: 18.5907, Val Acc: 0.4548\n",
      "Epoch 247/300, Train Loss: 79.7092, Train Acc: 0.5050, Val Loss: 17.9696, Val Acc: 0.4710\n",
      "Epoch 248/300, Train Loss: 79.6286, Train Acc: 0.5050, Val Loss: 18.4609, Val Acc: 0.4640\n",
      "Epoch 249/300, Train Loss: 79.1971, Train Acc: 0.5060, Val Loss: 17.9073, Val Acc: 0.5035\n",
      "Epoch 250/300, Train Loss: 80.8384, Train Acc: 0.5055, Val Loss: 19.1779, Val Acc: 0.4826\n",
      "Epoch 251/300, Train Loss: 77.4167, Train Acc: 0.5263, Val Loss: 17.8630, Val Acc: 0.5058\n",
      "Epoch 252/300, Train Loss: 81.2935, Train Acc: 0.4920, Val Loss: 17.8939, Val Acc: 0.4872\n",
      "Epoch 253/300, Train Loss: 78.6468, Train Acc: 0.5164, Val Loss: 18.4435, Val Acc: 0.4896\n",
      "Epoch 254/300, Train Loss: 80.1058, Train Acc: 0.5000, Val Loss: 20.0637, Val Acc: 0.4594\n",
      "Epoch 255/300, Train Loss: 81.4177, Train Acc: 0.4906, Val Loss: 18.8267, Val Acc: 0.4548\n",
      "Epoch 256/300, Train Loss: 81.6956, Train Acc: 0.4881, Val Loss: 17.9440, Val Acc: 0.4919\n",
      "Epoch 257/300, Train Loss: 78.3673, Train Acc: 0.5119, Val Loss: 17.8626, Val Acc: 0.5012\n",
      "Epoch 258/300, Train Loss: 78.8606, Train Acc: 0.5075, Val Loss: 18.9406, Val Acc: 0.4687\n",
      "Epoch 259/300, Train Loss: 80.0001, Train Acc: 0.4925, Val Loss: 17.8385, Val Acc: 0.5174\n",
      "Epoch 260/300, Train Loss: 76.7125, Train Acc: 0.5244, Val Loss: 17.6886, Val Acc: 0.5058\n",
      "Epoch 261/300, Train Loss: 83.3495, Train Acc: 0.4727, Val Loss: 18.5966, Val Acc: 0.4571\n",
      "Epoch 262/300, Train Loss: 79.9369, Train Acc: 0.5094, Val Loss: 18.0602, Val Acc: 0.4803\n",
      "Epoch 263/300, Train Loss: 77.9401, Train Acc: 0.5214, Val Loss: 18.2836, Val Acc: 0.4988\n",
      "Epoch 264/300, Train Loss: 80.1911, Train Acc: 0.5050, Val Loss: 17.9314, Val Acc: 0.4826\n",
      "Epoch 265/300, Train Loss: 78.1211, Train Acc: 0.5244, Val Loss: 18.1432, Val Acc: 0.4803\n",
      "Epoch 266/300, Train Loss: 78.5193, Train Acc: 0.5060, Val Loss: 18.3205, Val Acc: 0.4988\n",
      "Epoch 267/300, Train Loss: 75.9879, Train Acc: 0.5229, Val Loss: 17.6047, Val Acc: 0.5058\n",
      "Epoch 268/300, Train Loss: 74.8301, Train Acc: 0.5368, Val Loss: 17.4726, Val Acc: 0.5244\n",
      "Epoch 269/300, Train Loss: 79.0691, Train Acc: 0.5099, Val Loss: 18.3001, Val Acc: 0.4826\n",
      "Epoch 270/300, Train Loss: 78.5428, Train Acc: 0.5164, Val Loss: 18.6611, Val Acc: 0.5104\n",
      "Epoch 271/300, Train Loss: 78.6013, Train Acc: 0.5089, Val Loss: 19.1523, Val Acc: 0.4548\n",
      "Epoch 272/300, Train Loss: 77.1042, Train Acc: 0.5104, Val Loss: 17.1998, Val Acc: 0.5128\n",
      "Epoch 273/300, Train Loss: 75.6816, Train Acc: 0.5298, Val Loss: 17.3592, Val Acc: 0.5220\n",
      "Epoch 274/300, Train Loss: 75.5854, Train Acc: 0.5278, Val Loss: 17.1649, Val Acc: 0.5151\n",
      "Epoch 275/300, Train Loss: 78.6414, Train Acc: 0.5184, Val Loss: 17.3696, Val Acc: 0.4942\n",
      "Epoch 276/300, Train Loss: 78.5854, Train Acc: 0.5159, Val Loss: 17.6674, Val Acc: 0.5174\n",
      "Epoch 277/300, Train Loss: 74.8060, Train Acc: 0.5368, Val Loss: 18.4800, Val Acc: 0.5012\n",
      "Epoch 278/300, Train Loss: 75.0108, Train Acc: 0.5358, Val Loss: 17.8193, Val Acc: 0.4849\n",
      "Epoch 279/300, Train Loss: 74.1596, Train Acc: 0.5447, Val Loss: 17.1253, Val Acc: 0.5406\n",
      "Epoch 280/300, Train Loss: 75.1004, Train Acc: 0.5323, Val Loss: 18.3636, Val Acc: 0.4826\n",
      "Epoch 281/300, Train Loss: 74.1628, Train Acc: 0.5467, Val Loss: 18.2714, Val Acc: 0.4965\n",
      "Epoch 282/300, Train Loss: 76.0043, Train Acc: 0.5253, Val Loss: 16.7297, Val Acc: 0.5336\n",
      "Epoch 283/300, Train Loss: 75.4428, Train Acc: 0.5189, Val Loss: 18.4012, Val Acc: 0.4780\n",
      "Epoch 284/300, Train Loss: 76.3129, Train Acc: 0.5214, Val Loss: 18.2383, Val Acc: 0.4780\n",
      "Epoch 285/300, Train Loss: 72.6905, Train Acc: 0.5497, Val Loss: 17.6392, Val Acc: 0.5174\n",
      "Epoch 286/300, Train Loss: 79.1733, Train Acc: 0.4985, Val Loss: 17.4738, Val Acc: 0.4988\n",
      "Epoch 287/300, Train Loss: 74.7334, Train Acc: 0.5437, Val Loss: 16.8508, Val Acc: 0.5220\n",
      "Epoch 288/300, Train Loss: 71.9995, Train Acc: 0.5482, Val Loss: 16.5135, Val Acc: 0.5499\n",
      "Epoch 289/300, Train Loss: 71.3237, Train Acc: 0.5596, Val Loss: 17.6142, Val Acc: 0.4965\n",
      "Epoch 290/300, Train Loss: 70.9508, Train Acc: 0.5681, Val Loss: 16.8149, Val Acc: 0.5244\n",
      "Epoch 291/300, Train Loss: 71.6517, Train Acc: 0.5586, Val Loss: 18.3769, Val Acc: 0.4640\n",
      "Epoch 292/300, Train Loss: 74.6636, Train Acc: 0.5393, Val Loss: 16.8546, Val Acc: 0.5197\n",
      "Epoch 293/300, Train Loss: 76.8797, Train Acc: 0.5184, Val Loss: 20.3321, Val Acc: 0.4130\n",
      "Epoch 294/300, Train Loss: 74.6819, Train Acc: 0.5328, Val Loss: 16.6923, Val Acc: 0.5545\n",
      "Epoch 295/300, Train Loss: 71.8386, Train Acc: 0.5547, Val Loss: 16.7744, Val Acc: 0.5267\n",
      "Epoch 296/300, Train Loss: 70.1375, Train Acc: 0.5626, Val Loss: 16.8244, Val Acc: 0.5174\n",
      "Epoch 297/300, Train Loss: 75.5263, Train Acc: 0.5358, Val Loss: 16.6100, Val Acc: 0.5452\n",
      "Epoch 298/300, Train Loss: 72.0666, Train Acc: 0.5596, Val Loss: 18.6939, Val Acc: 0.4872\n",
      "Epoch 299/300, Train Loss: 70.4596, Train Acc: 0.5681, Val Loss: 19.2987, Val Acc: 0.4780\n",
      "Epoch 300/300, Train Loss: 70.6913, Train Acc: 0.5666, Val Loss: 16.3635, Val Acc: 0.5174\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"runs/torch-lstm/emotion_classifier_optimized_landmarks\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_batch = y_batch.argmax(dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = y_batch.argmax(dim=1)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Train\", train_acc, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Validation\", val_acc, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0395, Test Accuracy: 0.5486\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        y_batch = y_batch.argmax(dim=1)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "test_loss /= len(y_test)\n",
    "test_acc = correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
